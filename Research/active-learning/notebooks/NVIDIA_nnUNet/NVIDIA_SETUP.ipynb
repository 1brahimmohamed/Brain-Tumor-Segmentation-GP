{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'DeepLearningExamples'...\n",
      "remote: Enumerating objects: 33380, done.\u001b[K\n",
      "remote: Counting objects: 100% (4940/4940), done.\u001b[K\n",
      "remote: Compressing objects: 100% (1588/1588), done.\u001b[K\n",
      "remote: Total 33380 (delta 3611), reused 3406 (delta 3345), pack-reused 28440\u001b[K\n",
      "Receiving objects: 100% (33380/33380), 108.21 MiB | 3.07 MiB/s, done.\n",
      "Resolving deltas: 100% (23782/23782), done.\n"
     ]
    }
   ],
   "source": [
    "# Clone DeepLearningExamples repository\n",
    "!git clone https://github.com/NVIDIA/DeepLearningExamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/workstation04/anaconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: monai in /home/workstation04/anaconda3/lib/python3.9/site-packages (1.1.0)\n",
      "Requirement already satisfied: torch>=1.8 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from monai) (2.1.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from monai) (1.24.3)\n",
      "Requirement already satisfied: filelock in /home/workstation04/anaconda3/lib/python3.9/site-packages (from torch>=1.8->monai) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/workstation04/anaconda3/lib/python3.9/site-packages (from torch>=1.8->monai) (4.7.1)\n",
      "Requirement already satisfied: sympy in /home/workstation04/anaconda3/lib/python3.9/site-packages (from torch>=1.8->monai) (1.11.1)\n",
      "Requirement already satisfied: networkx in /home/workstation04/anaconda3/lib/python3.9/site-packages (from torch>=1.8->monai) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from torch>=1.8->monai) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/workstation04/anaconda3/lib/python3.9/site-packages (from torch>=1.8->monai) (2023.4.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from torch>=1.8->monai) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from torch>=1.8->monai) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from torch>=1.8->monai) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from torch>=1.8->monai) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from torch>=1.8->monai) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from torch>=1.8->monai) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from torch>=1.8->monai) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from torch>=1.8->monai) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from torch>=1.8->monai) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from torch>=1.8->monai) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from torch>=1.8->monai) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from torch>=1.8->monai) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8->monai) (12.2.140)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from jinja2->torch>=1.8->monai) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from sympy->torch>=1.8->monai) (1.3.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/workstation04/anaconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install necessary dependencies for running the models\n",
    "!pip install monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/workstation04/anaconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pytorch_lightning==1.9.4 in /home/workstation04/anaconda3/lib/python3.9/site-packages (1.9.4)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from pytorch_lightning==1.9.4) (1.24.3)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from pytorch_lightning==1.9.4) (2.1.2)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from pytorch_lightning==1.9.4) (4.65.0)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from pytorch_lightning==1.9.4) (6.0.1)\n",
      "Requirement already satisfied: fsspec>2021.06.0 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from fsspec[http]>2021.06.0->pytorch_lightning==1.9.4) (2023.4.0)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from pytorch_lightning==1.9.4) (0.11.1)\n",
      "Requirement already satisfied: packaging>=17.1 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from pytorch_lightning==1.9.4) (23.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from pytorch_lightning==1.9.4) (4.7.1)\n",
      "Requirement already satisfied: lightning-utilities>=0.6.0.post0 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from pytorch_lightning==1.9.4) (0.10.0)\n",
      "Requirement already satisfied: requests in /home/workstation04/anaconda3/lib/python3.9/site-packages (from fsspec[http]>2021.06.0->pytorch_lightning==1.9.4) (2.31.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from fsspec[http]>2021.06.0->pytorch_lightning==1.9.4) (3.8.3)\n",
      "Requirement already satisfied: setuptools in /home/workstation04/anaconda3/lib/python3.9/site-packages (from lightning-utilities>=0.6.0.post0->pytorch_lightning==1.9.4) (68.0.0)\n",
      "Requirement already satisfied: filelock in /home/workstation04/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->pytorch_lightning==1.9.4) (3.13.1)\n",
      "Requirement already satisfied: sympy in /home/workstation04/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->pytorch_lightning==1.9.4) (1.11.1)\n",
      "Requirement already satisfied: networkx in /home/workstation04/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->pytorch_lightning==1.9.4) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->pytorch_lightning==1.9.4) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->pytorch_lightning==1.9.4) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->pytorch_lightning==1.9.4) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->pytorch_lightning==1.9.4) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->pytorch_lightning==1.9.4) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->pytorch_lightning==1.9.4) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->pytorch_lightning==1.9.4) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->pytorch_lightning==1.9.4) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->pytorch_lightning==1.9.4) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->pytorch_lightning==1.9.4) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->pytorch_lightning==1.9.4) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->pytorch_lightning==1.9.4) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->pytorch_lightning==1.9.4) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->pytorch_lightning==1.9.4) (12.2.140)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.9.4) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.9.4) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.9.4) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.9.4) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.9.4) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.9.4) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.9.4) (1.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from jinja2->torch>=1.10.0->pytorch_lightning==1.9.4) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning==1.9.4) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning==1.9.4) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning==1.9.4) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from sympy->torch>=1.10.0->pytorch_lightning==1.9.4) (1.3.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/workstation04/anaconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pytorch_lightning==1.9.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/workstation04/anaconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://developer.download.nvidia.com/compute/redist\n",
      "Requirement already satisfied: nvidia-dali-cuda110 in /home/workstation04/anaconda3/lib/python3.9/site-packages (1.34.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from nvidia-dali-cuda110) (1.6.3)\n",
      "Requirement already satisfied: gast>=0.3.3 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from nvidia-dali-cuda110) (0.4.0)\n",
      "Requirement already satisfied: dm-tree in /home/workstation04/anaconda3/lib/python3.9/site-packages (from nvidia-dali-cuda110) (0.1.8)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from astunparse>=1.6.0->nvidia-dali-cuda110) (0.41.2)\n",
      "Requirement already satisfied: six<2.0,>=1.6.1 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from astunparse>=1.6.0->nvidia-dali-cuda110) (1.16.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/workstation04/anaconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --extra-index-url https://developer.download.nvidia.com/compute/redist --upgrade nvidia-dali-cuda110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'apex'...\n",
      "remote: Enumerating objects: 11597, done.\u001b[K\n",
      "remote: Counting objects: 100% (3665/3665), done.\u001b[K\n",
      "remote: Compressing objects: 100% (536/536), done.\u001b[K\n",
      "remote: Total 11597 (delta 3321), reused 3240 (delta 3126), pack-reused 7932\u001b[K\n",
      "Receiving objects: 100% (11597/11597), 15.45 MiB | 8.20 MiB/s, done.\n",
      "Resolving deltas: 100% (8150/8150), done.\n"
     ]
    }
   ],
   "source": [
    "# Clone the NVIDIA Apex repository\n",
    "!git clone https://github.com/NVIDIA/apex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Navigate to the Apex directory\n",
    "import os\n",
    "os.chdir(\"/home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex\")\n",
    "os. getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\n",
      "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n"
     ]
    }
   ],
   "source": [
    "#  Install Ninja build tool to accelerate building Apex --- in terminal\n",
    "!apt-get install ninja-build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Fix for Cuda Extension Compilation Error in The Next Cell</summary>\n",
    "\n",
    "If you encounter the following error during the compilation of Cuda extensions:\n",
    "\n",
    "```plaintext\n",
    "RuntimeError: Cuda extensions are being compiled with a version of Cuda that does not match the version used to compile Pytorch binaries.  Pytorch binaries were compiled with Cuda 12.1.\n",
    "In some cases, a minor-version mismatch will not cause later errors:  https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  You can try commenting out this check (at your own risk).\n",
    "error: subprocess-exited-with-error\n",
    "\n",
    "× Building wheel for apex (pyproject.toml) did not run successfully.\n",
    "│ exit code: 1\n",
    "╰─> See above for output.\n",
    "\n",
    "note: This error originates from a subprocess, and is likely not a problem with pip.\n",
    "full command: /usr/bin/python3 /usr/local/lib/python3.10/dist-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py build_wheel /tmp/tmp_xtq3rpt\n",
    "cwd: /content/apex\n",
    "Building wheel for apex (pyproject.toml) ... error\n",
    "ERROR: Failed building wheel for apex\n",
    "Failed to build apex\n",
    "ERROR: Could not build wheels for apex, which is required to install pyproject.toml-based projects\n",
    "```\n",
    "\n",
    "You can resolve this issue by commenting out the if condition in the check_cuda_torch_binary_vs_bare_metal function in the apex/setup.py file. Here's how you can do it:\n",
    "\n",
    "- Open the apex/setup.py file.\n",
    "\n",
    "- Locate the check_cuda_torch_binary_vs_bare_metal function.\n",
    "\n",
    "- Comment out the if condition as follows:\n",
    "\n",
    "```bash\n",
    "def check_cuda_torch_binary_vs_bare_metal(cuda_dir):\n",
    "    raw_output, bare_metal_version = get_cuda_bare_metal_version(cuda_dir)\n",
    "    torch_binary_version = parse(torch.version.cuda)\n",
    "\n",
    "    print(\"\\nCompiling cuda extensions with\")\n",
    "    print(raw_output + \"from \" + cuda_dir + \"/bin\\n\")\n",
    "    #comment this condition\n",
    "    # if (bare_metal_version != torch_binary_version):\n",
    "    #     raise RuntimeError(\n",
    "    #         \"Cuda extensions are being compiled with a version of Cuda that does \"\n",
    "    #         \"not match the version used to compile Pytorch binaries.  \"\n",
    "    #         \"Pytorch binaries were compiled with Cuda {}.\\n\".format(torch.version.cuda)\n",
    "    #         + \"In some cases, a minor-version mismatch will not cause later errors:  \"\n",
    "    #         \"https://github.com/NVIDIA/apex/pull/323#discussion_r287021798.  \"\n",
    "    #         \"You can try commenting out this check (at your own risk).\"\n",
    "    #     )\n",
    "\n",
    "\n",
    "```\n",
    "</details>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make sure that The CUDA version matches the version that was used to compile PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pip 23.3 from /home/workstation04/anaconda3/lib/python3.9/site-packages/pip (python 3.9)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/workstation04/anaconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mProcessing /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l  Running command Preparing metadata (pyproject.toml)\n",
      "\n",
      "\n",
      "  torch.__version__  = 2.1.2+cu121\n",
      "\n",
      "\n",
      "  running dist_info\n",
      "  creating /tmp/pip-modern-metadata-jgrviajh/apex.egg-info\n",
      "  writing /tmp/pip-modern-metadata-jgrviajh/apex.egg-info/PKG-INFO\n",
      "  writing dependency_links to /tmp/pip-modern-metadata-jgrviajh/apex.egg-info/dependency_links.txt\n",
      "  writing requirements to /tmp/pip-modern-metadata-jgrviajh/apex.egg-info/requires.txt\n",
      "  writing top-level names to /tmp/pip-modern-metadata-jgrviajh/apex.egg-info/top_level.txt\n",
      "  writing manifest file '/tmp/pip-modern-metadata-jgrviajh/apex.egg-info/SOURCES.txt'\n",
      "  reading manifest file '/tmp/pip-modern-metadata-jgrviajh/apex.egg-info/SOURCES.txt'\n",
      "  adding license file 'LICENSE'\n",
      "  writing manifest file '/tmp/pip-modern-metadata-jgrviajh/apex.egg-info/SOURCES.txt'\n",
      "  creating '/tmp/pip-modern-metadata-jgrviajh/apex-0.1.dist-info'\n",
      "\u001b[?25hdone\n",
      "Requirement already satisfied: packaging>20.6 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from apex==0.1) (23.1)\n",
      "Building wheels for collected packages: apex\n",
      "  Building wheel for apex (pyproject.toml) ... \u001b[?25l  Running command Building wheel for apex (pyproject.toml)\n",
      "\n",
      "\n",
      "  torch.__version__  = 2.1.2+cu121\n",
      "\n",
      "\n",
      "\n",
      "  Compiling cuda extensions with\n",
      "  nvcc: NVIDIA (R) Cuda compiler driver\n",
      "  Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "  Built on Wed_Nov_22_10:17:15_PST_2023\n",
      "  Cuda compilation tools, release 12.3, V12.3.107\n",
      "  Build cuda_12.3.r12.3/compiler.33567101_0\n",
      "  from /usr/local/cuda/bin\n",
      "\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  running build_ext\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/utils/cpp_extension.py:414: UserWarning: The detected CUDA version (12.3) has a minor version mismatch with the version that was used to compile PyTorch (12.1). Most likely this shouldn't be a problem.\n",
      "    warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/utils/cpp_extension.py:424: UserWarning: There are no g++ version bounds defined for CUDA version 12.3\n",
      "    warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
      "  building 'amp_C' extension\n",
      "  creating /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39\n",
      "  creating /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc\n",
      "  Emitting ninja build file /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/build.ninja...\n",
      "  Compiling objects...\n",
      "  Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "  [1/15] /usr/local/cuda/bin/nvcc  -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/TH -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/workstation04/anaconda3/include/python3.9 -c -c /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/multi_tensor_sgd_kernel.cu -o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/multi_tensor_sgd_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_61,code=compute_61 -gencode=arch=compute_61,code=sm_61 -std=c++17\n",
      "  [2/15] /usr/local/cuda/bin/nvcc  -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/TH -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/workstation04/anaconda3/include/python3.9 -c -c /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/multi_tensor_novograd.cu -o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/multi_tensor_novograd.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_61,code=compute_61 -gencode=arch=compute_61,code=sm_61 -std=c++17\n",
      "  [3/15] /usr/local/cuda/bin/nvcc  -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/TH -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/workstation04/anaconda3/include/python3.9 -c -c /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/multi_tensor_l2norm_scale_kernel.cu -o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/multi_tensor_l2norm_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_61,code=compute_61 -gencode=arch=compute_61,code=sm_61 -std=c++17\n",
      "  [4/15] /usr/local/cuda/bin/nvcc  -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/TH -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/workstation04/anaconda3/include/python3.9 -c -c /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/multi_tensor_lamb_mp.cu -o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/multi_tensor_lamb_mp.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_61,code=compute_61 -gencode=arch=compute_61,code=sm_61 -std=c++17\n",
      "  [5/15] /usr/local/cuda/bin/nvcc  -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/TH -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/workstation04/anaconda3/include/python3.9 -c -c /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/update_scale_hysteresis.cu -o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/update_scale_hysteresis.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_61,code=compute_61 -gencode=arch=compute_61,code=sm_61 -std=c++17\n",
      "  [6/15] /usr/local/cuda/bin/nvcc  -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/TH -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/workstation04/anaconda3/include/python3.9 -c -c /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/multi_tensor_adagrad.cu -o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/multi_tensor_adagrad.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_61,code=compute_61 -gencode=arch=compute_61,code=sm_61 -std=c++17\n",
      "  [7/15] /usr/local/cuda/bin/nvcc  -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/TH -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/workstation04/anaconda3/include/python3.9 -c -c /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/multi_tensor_axpby_kernel.cu -o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/multi_tensor_axpby_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_61,code=compute_61 -gencode=arch=compute_61,code=sm_61 -std=c++17\n",
      "  [8/15] /usr/local/cuda/bin/nvcc  -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/TH -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/workstation04/anaconda3/include/python3.9 -c -c /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/multi_tensor_l2norm_kernel_mp.cu -o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/multi_tensor_l2norm_kernel_mp.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_61,code=compute_61 -gencode=arch=compute_61,code=sm_61 -std=c++17\n",
      "  [9/15] /usr/local/cuda/bin/nvcc  -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/TH -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/workstation04/anaconda3/include/python3.9 -c -c /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/multi_tensor_lamb_stage_1.cu -o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/multi_tensor_lamb_stage_1.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_61,code=compute_61 -gencode=arch=compute_61,code=sm_61 -std=c++17\n",
      "  [10/15] /usr/local/cuda/bin/nvcc  -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/TH -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/workstation04/anaconda3/include/python3.9 -c -c /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/multi_tensor_adam.cu -o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/multi_tensor_adam.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_61,code=compute_61 -gencode=arch=compute_61,code=sm_61 -std=c++17\n",
      "  [11/15] /usr/local/cuda/bin/nvcc  -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/TH -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/workstation04/anaconda3/include/python3.9 -c -c /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/multi_tensor_l2norm_kernel.cu -o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_61,code=compute_61 -gencode=arch=compute_61,code=sm_61 -std=c++17\n",
      "  [12/15] c++ -MMD -MF /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/amp_C_frontend.o.d -pthread -B /home/workstation04/anaconda3/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/workstation04/anaconda3/include -I/home/workstation04/anaconda3/include -fPIC -O2 -isystem /home/workstation04/anaconda3/include -fPIC -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/TH -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/workstation04/anaconda3/include/python3.9 -c -c /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/amp_C_frontend.cpp -o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/amp_C_frontend.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
      "  [13/15] /usr/local/cuda/bin/nvcc  -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/TH -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/workstation04/anaconda3/include/python3.9 -c -c /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/multi_tensor_lamb.cu -o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/multi_tensor_lamb.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_61,code=compute_61 -gencode=arch=compute_61,code=sm_61 -std=c++17\n",
      "  [14/15] /usr/local/cuda/bin/nvcc  -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/TH -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/workstation04/anaconda3/include/python3.9 -c -c /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/multi_tensor_scale_kernel.cu -o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_61,code=compute_61 -gencode=arch=compute_61,code=sm_61 -std=c++17\n",
      "  [15/15] /usr/local/cuda/bin/nvcc  -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/TH -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/workstation04/anaconda3/include/python3.9 -c -c /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/multi_tensor_lamb_stage_2.cu -o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/multi_tensor_lamb_stage_2.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_61,code=compute_61 -gencode=arch=compute_61,code=sm_61 -std=c++17\n",
      "  g++ -pthread -B /home/workstation04/anaconda3/compiler_compat -shared -Wl,-rpath,/home/workstation04/anaconda3/lib -Wl,-rpath-link,/home/workstation04/anaconda3/lib -L/home/workstation04/anaconda3/lib -L/home/workstation04/anaconda3/lib -Wl,-rpath,/home/workstation04/anaconda3/lib -Wl,-rpath-link,/home/workstation04/anaconda3/lib -L/home/workstation04/anaconda3/lib /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/amp_C_frontend.o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/multi_tensor_adagrad.o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/multi_tensor_adam.o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/multi_tensor_axpby_kernel.o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/multi_tensor_l2norm_kernel.o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/multi_tensor_l2norm_kernel_mp.o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/multi_tensor_l2norm_scale_kernel.o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/multi_tensor_lamb.o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/multi_tensor_lamb_mp.o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/multi_tensor_lamb_stage_1.o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/multi_tensor_lamb_stage_2.o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/multi_tensor_novograd.o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/multi_tensor_scale_kernel.o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/multi_tensor_sgd_kernel.o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/update_scale_hysteresis.o -L/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-39/amp_C.cpython-39-x86_64-linux-gnu.so\n",
      "  building 'syncbn' extension\n",
      "  Emitting ninja build file /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/build.ninja...\n",
      "  Compiling objects...\n",
      "  Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "  [1/2] c++ -MMD -MF /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/syncbn.o.d -pthread -B /home/workstation04/anaconda3/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/workstation04/anaconda3/include -I/home/workstation04/anaconda3/include -fPIC -O2 -isystem /home/workstation04/anaconda3/include -fPIC -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/TH -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/workstation04/anaconda3/include/python3.9 -c -c /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/syncbn.cpp -o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/syncbn.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
      "  [2/2] /usr/local/cuda/bin/nvcc  -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/TH -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/workstation04/anaconda3/include/python3.9 -c -c /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/welford.cu -o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/welford.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_61,code=compute_61 -gencode=arch=compute_61,code=sm_61 -std=c++17\n",
      "  g++ -pthread -B /home/workstation04/anaconda3/compiler_compat -shared -Wl,-rpath,/home/workstation04/anaconda3/lib -Wl,-rpath-link,/home/workstation04/anaconda3/lib -L/home/workstation04/anaconda3/lib -L/home/workstation04/anaconda3/lib -Wl,-rpath,/home/workstation04/anaconda3/lib -Wl,-rpath-link,/home/workstation04/anaconda3/lib -L/home/workstation04/anaconda3/lib /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/syncbn.o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/welford.o -L/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-39/syncbn.cpython-39-x86_64-linux-gnu.so\n",
      "  building 'fused_layer_norm_cuda' extension\n",
      "  Emitting ninja build file /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/build.ninja...\n",
      "  Compiling objects...\n",
      "  Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "  [1/2] c++ -MMD -MF /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/layer_norm_cuda.o.d -pthread -B /home/workstation04/anaconda3/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/workstation04/anaconda3/include -I/home/workstation04/anaconda3/include -fPIC -O2 -isystem /home/workstation04/anaconda3/include -fPIC -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/TH -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/workstation04/anaconda3/include/python3.9 -c -c /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/layer_norm_cuda.cpp -o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/layer_norm_cuda.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
      "  [2/2] /usr/local/cuda/bin/nvcc  -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/TH -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/workstation04/anaconda3/include/python3.9 -c -c /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/layer_norm_cuda_kernel.cu -o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/layer_norm_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -maxrregcount=50 -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_61,code=compute_61 -gencode=arch=compute_61,code=sm_61 -std=c++17\n",
      "  g++ -pthread -B /home/workstation04/anaconda3/compiler_compat -shared -Wl,-rpath,/home/workstation04/anaconda3/lib -Wl,-rpath-link,/home/workstation04/anaconda3/lib -L/home/workstation04/anaconda3/lib -L/home/workstation04/anaconda3/lib -Wl,-rpath,/home/workstation04/anaconda3/lib -Wl,-rpath-link,/home/workstation04/anaconda3/lib -L/home/workstation04/anaconda3/lib /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/layer_norm_cuda.o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/layer_norm_cuda_kernel.o -L/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-39/fused_layer_norm_cuda.cpython-39-x86_64-linux-gnu.so\n",
      "  building 'mlp_cuda' extension\n",
      "  Emitting ninja build file /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/build.ninja...\n",
      "  Compiling objects...\n",
      "  Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "  [1/2] c++ -MMD -MF /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/mlp.o.d -pthread -B /home/workstation04/anaconda3/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/workstation04/anaconda3/include -I/home/workstation04/anaconda3/include -fPIC -O2 -isystem /home/workstation04/anaconda3/include -fPIC -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/TH -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/workstation04/anaconda3/include/python3.9 -c -c /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp -o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/mlp.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_forward(int, int, std::vector<at::Tensor>)’:\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:57:21: warning: comparison of integer expressions of different signedness: ‘int’ and ‘long unsigned int’ [-Wsign-compare]\n",
      "     57 |   for (int i = 0; i < num_layers; i++) {\n",
      "        |                   ~~^~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:64:76: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "     64 |   auto out = at::empty({batch_size, output_features.back()}, inputs[0].type());\n",
      "        |                                                              ~~~~~~~~~~~~~~^~\n",
      "  In file included from /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/core/Tensor.h:3,\n",
      "                   from /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                   from /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
      "                   from /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
      "                   from /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
      "                   from /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
      "                   from /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
      "                   from /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
      "                   from /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/extension.h:5,\n",
      "                   from /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:1:\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here\n",
      "    225 |   DeprecatedTypeProperties & type() const {\n",
      "        |                              ^~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:65:85: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "     65 |   auto reserved_space = at::empty({static_cast<long>(reserved_size)}, inputs[0].type());\n",
      "        |                                                                       ~~~~~~~~~~~~~~^~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here\n",
      "    225 |   DeprecatedTypeProperties & type() const {\n",
      "        |                              ^~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:67:58: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "     67 |   auto lt_workspace = at::empty({1 << 22}, inputs[0].type());\n",
      "        |                                            ~~~~~~~~~~~~~~^~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here\n",
      "    225 |   DeprecatedTypeProperties & type() const {\n",
      "        |                              ^~~~\n",
      "  In file included from /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/ATen.h:11,\n",
      "                   from /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                   from /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                   from /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                   from /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
      "                   from /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                   from /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                   from /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/all.h:9:\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp: In lambda function:\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:69:53: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
      "        |                                       ~~~~~~~~~~~~~~^~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:215:28: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
      "    215 |     const auto& the_type = TYPE;                                            \\\n",
      "        |                            ^~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here\n",
      "    225 |   DeprecatedTypeProperties & type() const {\n",
      "        |                              ^~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:218:47: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
      "    218 |     at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
      "        |                          ~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:245:3: note: in expansion of macro ‘AT_DISPATCH_SWITCH’\n",
      "    245 |   AT_DISPATCH_SWITCH(                                        \\\n",
      "        |   ^~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:109:23: note: declared here\n",
      "    109 | inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
      "        |                       ^~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp: In lambda function:\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:72:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘long unsigned int’ [-Wsign-compare]\n",
      "     72 |     for (int i = 0; i < num_layers; i++) {\n",
      "        |                     ~~^~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
      "    221 |       __VA_ARGS__                                                           \\\n",
      "        |       ^~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
      "     74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:240:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
      "    240 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
      "        |   ^~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:246:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
      "    246 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
      "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
      "     78 |     auto result = mlp_fp<scalar_t>(\n",
      "        |          ^~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
      "    221 |       __VA_ARGS__                                                           \\\n",
      "        |       ^~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
      "     74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:240:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
      "    240 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
      "        |   ^~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:246:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
      "    246 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
      "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp: In lambda function:\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:72:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘long unsigned int’ [-Wsign-compare]\n",
      "     72 |     for (int i = 0; i < num_layers; i++) {\n",
      "        |                     ~~^~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
      "    221 |       __VA_ARGS__                                                           \\\n",
      "        |       ^~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
      "     74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:241:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
      "    241 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
      "        |   ^~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:246:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
      "    246 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
      "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
      "     78 |     auto result = mlp_fp<scalar_t>(\n",
      "        |          ^~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
      "    221 |       __VA_ARGS__                                                           \\\n",
      "        |       ^~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
      "     74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:241:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
      "    241 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
      "        |   ^~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:246:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
      "    246 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
      "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp: In lambda function:\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:72:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘long unsigned int’ [-Wsign-compare]\n",
      "     72 |     for (int i = 0; i < num_layers; i++) {\n",
      "        |                     ~~^~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
      "    221 |       __VA_ARGS__                                                           \\\n",
      "        |       ^~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
      "     74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:242:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
      "    242 |   AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:246:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
      "    246 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
      "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
      "     78 |     auto result = mlp_fp<scalar_t>(\n",
      "        |          ^~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
      "    221 |       __VA_ARGS__                                                           \\\n",
      "        |       ^~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
      "     74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:242:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
      "    242 |   AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:246:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
      "    246 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
      "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "     69 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_backward(int, int, at::Tensor, std::vector<at::Tensor>, std::vector<at::Tensor>)’:\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:115:21: warning: comparison of integer expressions of different signedness: ‘int’ and ‘long unsigned int’ [-Wsign-compare]\n",
      "    115 |   for (int i = 0; i < num_layers; i++) {\n",
      "        |                   ~~^~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:120:21: warning: comparison of integer expressions of different signedness: ‘int’ and ‘std::vector<at::Tensor>::size_type’ {aka ‘long unsigned int’} [-Wsign-compare]\n",
      "    120 |   for (int i = 0; i < inputs.size(); i++) {\n",
      "        |                   ~~^~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:121:66: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "    121 |     outputs.push_back(at::empty(inputs[i].sizes(), inputs[i].type()));  // clone for testing now\n",
      "        |                                                    ~~~~~~~~~~~~~~^~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here\n",
      "    225 |   DeprecatedTypeProperties & type() const {\n",
      "        |                              ^~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp: In lambda function:\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:124:53: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "        |                                       ~~~~~~~~~~~~~~^~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:215:28: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
      "    215 |     const auto& the_type = TYPE;                                            \\\n",
      "        |                            ^~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here\n",
      "    225 |   DeprecatedTypeProperties & type() const {\n",
      "        |                              ^~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:218:47: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
      "    218 |     at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
      "        |                          ~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:245:3: note: in expansion of macro ‘AT_DISPATCH_SWITCH’\n",
      "    245 |   AT_DISPATCH_SWITCH(                                        \\\n",
      "        |   ^~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:109:23: note: declared here\n",
      "    109 | inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
      "        |                       ^~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp: In lambda function:\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:126:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘long unsigned int’ [-Wsign-compare]\n",
      "    126 |     for (int i = 0; i < num_layers; i++) {\n",
      "        |                     ~~^~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
      "    221 |       __VA_ARGS__                                                           \\\n",
      "        |       ^~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
      "     74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:240:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
      "    240 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
      "        |   ^~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:246:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
      "    246 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
      "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:130:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘std::vector<at::Tensor>::size_type’ {aka ‘long unsigned int’} [-Wsign-compare]\n",
      "    130 |     for (int i = 0; i < inputs.size(); i++) {\n",
      "        |                     ~~^~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
      "    221 |       __VA_ARGS__                                                           \\\n",
      "        |       ^~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
      "     74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:240:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
      "    240 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
      "        |   ^~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:246:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
      "    246 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
      "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:138:98: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "    138 |     auto work_space = at::empty({static_cast<long>(work_size / sizeof(scalar_t))}, inputs[0].type());\n",
      "        |                                                                                    ~~~~~~~~~~~~~~^~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
      "    221 |       __VA_ARGS__                                                           \\\n",
      "        |       ^~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
      "     74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:240:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
      "    240 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
      "        |   ^~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:246:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
      "    246 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
      "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here\n",
      "    225 |   DeprecatedTypeProperties & type() const {\n",
      "        |                              ^~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:140:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
      "    140 |     auto result = mlp_bp<scalar_t>(\n",
      "        |          ^~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
      "    221 |       __VA_ARGS__                                                           \\\n",
      "        |       ^~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
      "     74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:240:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
      "    240 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
      "        |   ^~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:246:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
      "    246 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
      "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp: In lambda function:\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:126:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘long unsigned int’ [-Wsign-compare]\n",
      "    126 |     for (int i = 0; i < num_layers; i++) {\n",
      "        |                     ~~^~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
      "    221 |       __VA_ARGS__                                                           \\\n",
      "        |       ^~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
      "     74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:241:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
      "    241 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
      "        |   ^~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:246:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
      "    246 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
      "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:130:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘std::vector<at::Tensor>::size_type’ {aka ‘long unsigned int’} [-Wsign-compare]\n",
      "    130 |     for (int i = 0; i < inputs.size(); i++) {\n",
      "        |                     ~~^~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
      "    221 |       __VA_ARGS__                                                           \\\n",
      "        |       ^~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
      "     74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:241:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
      "    241 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
      "        |   ^~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:246:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
      "    246 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
      "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:138:98: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "    138 |     auto work_space = at::empty({static_cast<long>(work_size / sizeof(scalar_t))}, inputs[0].type());\n",
      "        |                                                                                    ~~~~~~~~~~~~~~^~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
      "    221 |       __VA_ARGS__                                                           \\\n",
      "        |       ^~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
      "     74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:241:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
      "    241 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
      "        |   ^~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:246:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
      "    246 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
      "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here\n",
      "    225 |   DeprecatedTypeProperties & type() const {\n",
      "        |                              ^~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:140:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
      "    140 |     auto result = mlp_bp<scalar_t>(\n",
      "        |          ^~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
      "    221 |       __VA_ARGS__                                                           \\\n",
      "        |       ^~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
      "     74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:241:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
      "    241 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
      "        |   ^~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:246:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
      "    246 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
      "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp: In lambda function:\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:126:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘long unsigned int’ [-Wsign-compare]\n",
      "    126 |     for (int i = 0; i < num_layers; i++) {\n",
      "        |                     ~~^~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
      "    221 |       __VA_ARGS__                                                           \\\n",
      "        |       ^~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
      "     74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:242:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
      "    242 |   AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:246:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
      "    246 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
      "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:130:23: warning: comparison of integer expressions of different signedness: ‘int’ and ‘std::vector<at::Tensor>::size_type’ {aka ‘long unsigned int’} [-Wsign-compare]\n",
      "    130 |     for (int i = 0; i < inputs.size(); i++) {\n",
      "        |                     ~~^~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
      "    221 |       __VA_ARGS__                                                           \\\n",
      "        |       ^~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
      "     74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:242:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
      "    242 |   AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:246:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
      "    246 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
      "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:138:98: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "    138 |     auto work_space = at::empty({static_cast<long>(work_size / sizeof(scalar_t))}, inputs[0].type());\n",
      "        |                                                                                    ~~~~~~~~~~~~~~^~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
      "    221 |       __VA_ARGS__                                                           \\\n",
      "        |       ^~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
      "     74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:242:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
      "    242 |   AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:246:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
      "    246 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
      "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here\n",
      "    225 |   DeprecatedTypeProperties & type() const {\n",
      "        |                              ^~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:140:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
      "    140 |     auto result = mlp_bp<scalar_t>(\n",
      "        |          ^~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
      "    221 |       __VA_ARGS__                                                           \\\n",
      "        |       ^~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
      "     74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:242:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
      "    242 |   AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:246:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
      "    246 |       TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
      "        |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
      "    124 |   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  [2/2] /usr/local/cuda/bin/nvcc  -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/TH -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/workstation04/anaconda3/include/python3.9 -c -c /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/mlp_cuda.cu -o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/mlp_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_61,code=compute_61 -gencode=arch=compute_61,code=sm_61 -std=c++17\n",
      "  g++ -pthread -B /home/workstation04/anaconda3/compiler_compat -shared -Wl,-rpath,/home/workstation04/anaconda3/lib -Wl,-rpath-link,/home/workstation04/anaconda3/lib -L/home/workstation04/anaconda3/lib -L/home/workstation04/anaconda3/lib -Wl,-rpath,/home/workstation04/anaconda3/lib -Wl,-rpath-link,/home/workstation04/anaconda3/lib -L/home/workstation04/anaconda3/lib /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/mlp.o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/mlp_cuda.o -L/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-39/mlp_cuda.cpython-39-x86_64-linux-gnu.so\n",
      "  building 'fused_dense_cuda' extension\n",
      "  Emitting ninja build file /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/build.ninja...\n",
      "  Compiling objects...\n",
      "  Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "  [1/2] c++ -MMD -MF /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/fused_dense.o.d -pthread -B /home/workstation04/anaconda3/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/workstation04/anaconda3/include -I/home/workstation04/anaconda3/include -fPIC -O2 -isystem /home/workstation04/anaconda3/include -fPIC -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/TH -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/workstation04/anaconda3/include/python3.9 -c -c /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp -o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/fused_dense.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=fused_dense_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp: In function ‘at::Tensor linear_bias_forward(at::Tensor, at::Tensor, at::Tensor)’:\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:30:62: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "     30 |   auto out = at::empty({batch_size, out_features}, input.type());\n",
      "        |                                                    ~~~~~~~~~~^~\n",
      "  In file included from /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/core/Tensor.h:3,\n",
      "                   from /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                   from /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
      "                   from /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
      "                   from /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
      "                   from /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
      "                   from /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
      "                   from /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
      "                   from /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/extension.h:5,\n",
      "                   from /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:1:\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here\n",
      "    225 |   DeprecatedTypeProperties & type() const {\n",
      "        |                              ^~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:33:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "     33 |   auto lt_workspace = at::empty({1 << 22}, input.type());\n",
      "        |                                            ~~~~~~~~~~^~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here\n",
      "    225 |   DeprecatedTypeProperties & type() const {\n",
      "        |                              ^~~~\n",
      "  In file included from /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/ATen.h:11,\n",
      "                   from /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
      "                   from /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
      "                   from /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
      "                   from /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
      "                   from /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
      "                   from /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
      "                   from /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include/torch/all.h:9:\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp: In lambda function:\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:37:15: warning: unused variable ‘b_ptr’ [-Wunused-variable]\n",
      "     37 |     scalar_t* b_ptr = bias.data_ptr<scalar_t>();\n",
      "        |               ^~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
      "    221 |       __VA_ARGS__                                                           \\\n",
      "        |       ^~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
      "     74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:233:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
      "    233 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
      "        |   ^~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:267:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’\n",
      "    267 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
      "    276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
      "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
      "     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:38:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
      "     38 |     auto result = linear_bias_forward_cuda<scalar_t>(\n",
      "        |          ^~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
      "    221 |       __VA_ARGS__                                                           \\\n",
      "        |       ^~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
      "     74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:233:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
      "    233 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
      "        |   ^~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:267:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’\n",
      "    267 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
      "    276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
      "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
      "     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp: In lambda function:\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:37:15: warning: unused variable ‘b_ptr’ [-Wunused-variable]\n",
      "     37 |     scalar_t* b_ptr = bias.data_ptr<scalar_t>();\n",
      "        |               ^~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
      "    221 |       __VA_ARGS__                                                           \\\n",
      "        |       ^~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
      "     74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:234:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
      "    234 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:267:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’\n",
      "    267 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
      "    276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
      "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
      "     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:38:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
      "     38 |     auto result = linear_bias_forward_cuda<scalar_t>(\n",
      "        |          ^~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
      "    221 |       __VA_ARGS__                                                           \\\n",
      "        |       ^~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
      "     74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:234:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
      "    234 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:267:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’\n",
      "    267 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
      "    276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
      "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
      "     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp: In lambda function:\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:37:15: warning: unused variable ‘b_ptr’ [-Wunused-variable]\n",
      "     37 |     scalar_t* b_ptr = bias.data_ptr<scalar_t>();\n",
      "        |               ^~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
      "    221 |       __VA_ARGS__                                                           \\\n",
      "        |       ^~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
      "     74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:268:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
      "    268 |   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)                                \\\n",
      "        |   ^~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
      "    276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
      "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
      "     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:38:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
      "     38 |     auto result = linear_bias_forward_cuda<scalar_t>(\n",
      "        |          ^~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
      "    221 |       __VA_ARGS__                                                           \\\n",
      "        |       ^~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
      "     74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:268:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
      "    268 |   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)                                \\\n",
      "        |   ^~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
      "    276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
      "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
      "     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp: In lambda function:\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:37:15: warning: unused variable ‘b_ptr’ [-Wunused-variable]\n",
      "     37 |     scalar_t* b_ptr = bias.data_ptr<scalar_t>();\n",
      "        |               ^~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
      "    221 |       __VA_ARGS__                                                           \\\n",
      "        |       ^~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
      "     74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:269:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
      "    269 |   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
      "    276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
      "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
      "     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:38:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
      "     38 |     auto result = linear_bias_forward_cuda<scalar_t>(\n",
      "        |          ^~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
      "    221 |       __VA_ARGS__                                                           \\\n",
      "        |       ^~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
      "     74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:269:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
      "    269 |   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
      "    276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
      "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
      "     35 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_forward\", [&] {\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp: In function ‘std::vector<at::Tensor> linear_bias_backward(at::Tensor, at::Tensor, at::Tensor)’:\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:64:68: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "     64 |   auto d_weight = at::empty({out_features, in_features}, input.type());\n",
      "        |                                                          ~~~~~~~~~~^~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here\n",
      "    225 |   DeprecatedTypeProperties & type() const {\n",
      "        |                              ^~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:68:53: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "     68 |   auto d_bias = at::empty({out_features}, input.type());\n",
      "        |                                           ~~~~~~~~~~^~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here\n",
      "    225 |   DeprecatedTypeProperties & type() const {\n",
      "        |                              ^~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:70:65: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "     70 |   auto d_input = at::empty({batch_size, in_features}, input.type());\n",
      "        |                                                       ~~~~~~~~~~^~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here\n",
      "    225 |   DeprecatedTypeProperties & type() const {\n",
      "        |                              ^~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:73:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "     73 |   auto lt_workspace = at::empty({1 << 22}, input.type());\n",
      "        |                                            ~~~~~~~~~~^~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here\n",
      "    225 |   DeprecatedTypeProperties & type() const {\n",
      "        |                              ^~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp: In lambda function:\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:77:15: warning: unused variable ‘d_b_ptr’ [-Wunused-variable]\n",
      "     77 |     scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();\n",
      "        |               ^~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
      "    221 |       __VA_ARGS__                                                           \\\n",
      "        |       ^~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
      "     74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:233:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
      "    233 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
      "        |   ^~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:267:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’\n",
      "    267 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
      "    276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
      "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
      "     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
      "     78 |     auto result = linear_bias_backward_cuda<scalar_t>(\n",
      "        |          ^~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
      "    221 |       __VA_ARGS__                                                           \\\n",
      "        |       ^~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
      "     74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:233:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
      "    233 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
      "        |   ^~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:267:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’\n",
      "    267 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
      "    276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
      "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
      "     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp: In lambda function:\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:77:15: warning: unused variable ‘d_b_ptr’ [-Wunused-variable]\n",
      "     77 |     scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();\n",
      "        |               ^~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
      "    221 |       __VA_ARGS__                                                           \\\n",
      "        |       ^~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
      "     74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:234:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
      "    234 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:267:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’\n",
      "    267 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
      "    276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
      "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
      "     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
      "     78 |     auto result = linear_bias_backward_cuda<scalar_t>(\n",
      "        |          ^~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
      "    221 |       __VA_ARGS__                                                           \\\n",
      "        |       ^~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
      "     74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:234:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
      "    234 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:267:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’\n",
      "    267 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
      "    276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
      "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
      "     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp: In lambda function:\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:77:15: warning: unused variable ‘d_b_ptr’ [-Wunused-variable]\n",
      "     77 |     scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();\n",
      "        |               ^~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
      "    221 |       __VA_ARGS__                                                           \\\n",
      "        |       ^~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
      "     74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:268:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
      "    268 |   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)                                \\\n",
      "        |   ^~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
      "    276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
      "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
      "     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
      "     78 |     auto result = linear_bias_backward_cuda<scalar_t>(\n",
      "        |          ^~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
      "    221 |       __VA_ARGS__                                                           \\\n",
      "        |       ^~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
      "     74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:268:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
      "    268 |   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)                                \\\n",
      "        |   ^~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
      "    276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
      "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
      "     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp: In lambda function:\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:77:15: warning: unused variable ‘d_b_ptr’ [-Wunused-variable]\n",
      "     77 |     scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();\n",
      "        |               ^~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
      "    221 |       __VA_ARGS__                                                           \\\n",
      "        |       ^~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
      "     74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:269:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
      "    269 |   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
      "    276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
      "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
      "     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
      "     78 |     auto result = linear_bias_backward_cuda<scalar_t>(\n",
      "        |          ^~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
      "    221 |       __VA_ARGS__                                                           \\\n",
      "        |       ^~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
      "     74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:269:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
      "    269 |   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
      "    276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
      "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
      "     75 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp: In function ‘std::vector<at::Tensor> linear_gelu_linear_forward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor)’:\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:106:69: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "    106 |   auto output1 = at::empty({batch_size, hidden_features}, input.type());\n",
      "        |                                                           ~~~~~~~~~~^~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here\n",
      "    225 |   DeprecatedTypeProperties & type() const {\n",
      "        |                              ^~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:107:69: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "    107 |   auto gelu_in = at::empty({batch_size, hidden_features}, input.type());\n",
      "        |                                                           ~~~~~~~~~~^~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here\n",
      "    225 |   DeprecatedTypeProperties & type() const {\n",
      "        |                              ^~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:108:66: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "    108 |   auto output2 = at::empty({batch_size, out_features}, input.type());\n",
      "        |                                                        ~~~~~~~~~~^~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here\n",
      "    225 |   DeprecatedTypeProperties & type() const {\n",
      "        |                              ^~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:111:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "    111 |   auto lt_workspace = at::empty({1 << 22}, input.type());\n",
      "        |                                            ~~~~~~~~~~^~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here\n",
      "    225 |   DeprecatedTypeProperties & type() const {\n",
      "        |                              ^~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp: In lambda function:\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:118:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
      "    118 |     auto result = linear_gelu_linear_forward_cuda<scalar_t>(\n",
      "        |          ^~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
      "    221 |       __VA_ARGS__                                                           \\\n",
      "        |       ^~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
      "     74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:233:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
      "    233 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
      "        |   ^~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:267:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’\n",
      "    267 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
      "    276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
      "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:113:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
      "    113 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_gelu_linear_forward\", [&] {\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp: In lambda function:\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:118:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
      "    118 |     auto result = linear_gelu_linear_forward_cuda<scalar_t>(\n",
      "        |          ^~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
      "    221 |       __VA_ARGS__                                                           \\\n",
      "        |       ^~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
      "     74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:234:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
      "    234 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:267:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’\n",
      "    267 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
      "    276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
      "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:113:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
      "    113 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_gelu_linear_forward\", [&] {\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp: In lambda function:\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:118:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
      "    118 |     auto result = linear_gelu_linear_forward_cuda<scalar_t>(\n",
      "        |          ^~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
      "    221 |       __VA_ARGS__                                                           \\\n",
      "        |       ^~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
      "     74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:268:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
      "    268 |   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)                                \\\n",
      "        |   ^~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
      "    276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
      "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:113:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
      "    113 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_gelu_linear_forward\", [&] {\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp: In lambda function:\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:118:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
      "    118 |     auto result = linear_gelu_linear_forward_cuda<scalar_t>(\n",
      "        |          ^~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
      "    221 |       __VA_ARGS__                                                           \\\n",
      "        |       ^~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
      "     74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:269:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
      "    269 |   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
      "    276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
      "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:113:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
      "    113 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_gelu_linear_forward\", [&] {\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp: In function ‘std::vector<at::Tensor> linear_gelu_linear_backward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor)’:\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:149:72: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "    149 |   auto d_weight1 = at::empty({hidden_features, in_features}, input.type());\n",
      "        |                                                              ~~~~~~~~~~^~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here\n",
      "    225 |   DeprecatedTypeProperties & type() const {\n",
      "        |                              ^~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:150:73: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "    150 |   auto d_weight2 = at::empty({out_features, hidden_features}, input.type());\n",
      "        |                                                               ~~~~~~~~~~^~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here\n",
      "    225 |   DeprecatedTypeProperties & type() const {\n",
      "        |                              ^~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:151:57: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "    151 |   auto d_bias1 = at::empty({hidden_features}, input.type());\n",
      "        |                                               ~~~~~~~~~~^~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here\n",
      "    225 |   DeprecatedTypeProperties & type() const {\n",
      "        |                              ^~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:152:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "    152 |   auto d_bias2 = at::empty({out_features}, input.type());\n",
      "        |                                            ~~~~~~~~~~^~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here\n",
      "    225 |   DeprecatedTypeProperties & type() const {\n",
      "        |                              ^~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:153:65: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "    153 |   auto d_input = at::empty({batch_size, in_features}, input.type());\n",
      "        |                                                       ~~~~~~~~~~^~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here\n",
      "    225 |   DeprecatedTypeProperties & type() const {\n",
      "        |                              ^~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:154:71: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "    154 |   auto d_output1 = at::empty({batch_size, hidden_features}, input.type());\n",
      "        |                                                             ~~~~~~~~~~^~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here\n",
      "    225 |   DeprecatedTypeProperties & type() const {\n",
      "        |                              ^~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:157:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "    157 |   auto lt_workspace = at::empty({1 << 22}, input.type());\n",
      "        |                                            ~~~~~~~~~~^~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here\n",
      "    225 |   DeprecatedTypeProperties & type() const {\n",
      "        |                              ^~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp: In lambda function:\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:163:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
      "    163 |     auto result = linear_gelu_linear_backward_cuda<scalar_t>(\n",
      "        |          ^~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
      "    221 |       __VA_ARGS__                                                           \\\n",
      "        |       ^~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
      "     74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:233:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
      "    233 |   AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
      "        |   ^~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:267:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’\n",
      "    267 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
      "    276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
      "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:159:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
      "    159 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp: In lambda function:\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:163:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
      "    163 |     auto result = linear_gelu_linear_backward_cuda<scalar_t>(\n",
      "        |          ^~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
      "    221 |       __VA_ARGS__                                                           \\\n",
      "        |       ^~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
      "     74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:234:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
      "    234 |   AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:267:3: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES’\n",
      "    267 |   AT_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__)                              \\\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
      "    276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
      "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:159:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
      "    159 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp: In lambda function:\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:163:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
      "    163 |     auto result = linear_gelu_linear_backward_cuda<scalar_t>(\n",
      "        |          ^~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
      "    221 |       __VA_ARGS__                                                           \\\n",
      "        |       ^~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
      "     74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:268:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
      "    268 |   AT_DISPATCH_CASE(SCALARTYPE1, __VA_ARGS__)                                \\\n",
      "        |   ^~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
      "    276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
      "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:159:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
      "    159 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp: In lambda function:\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:163:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
      "    163 |     auto result = linear_gelu_linear_backward_cuda<scalar_t>(\n",
      "        |          ^~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:221:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
      "    221 |       __VA_ARGS__                                                           \\\n",
      "        |       ^~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:74:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
      "     74 |   AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:269:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
      "    269 |   AT_DISPATCH_CASE(SCALARTYPE2, __VA_ARGS__)\n",
      "        |   ^~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/ATen/Dispatch.h:276:7: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND2’\n",
      "    276 |       AT_DISPATCH_CASE_FLOATING_TYPES_AND2(    \\\n",
      "        |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense.cpp:159:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND2’\n",
      "    159 |   AT_DISPATCH_FLOATING_TYPES_AND2(at::ScalarType::Half, at::ScalarType::BFloat16, input.scalar_type(), \"linear_bias_backward\", [&] {\n",
      "        |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "  [2/2] /usr/local/cuda/bin/nvcc  -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/TH -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/workstation04/anaconda3/include/python3.9 -c -c /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/fused_dense_cuda.cu -o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/fused_dense_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=fused_dense_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_61,code=compute_61 -gencode=arch=compute_61,code=sm_61 -std=c++17\n",
      "  g++ -pthread -B /home/workstation04/anaconda3/compiler_compat -shared -Wl,-rpath,/home/workstation04/anaconda3/lib -Wl,-rpath-link,/home/workstation04/anaconda3/lib -L/home/workstation04/anaconda3/lib -L/home/workstation04/anaconda3/lib -Wl,-rpath,/home/workstation04/anaconda3/lib -Wl,-rpath-link,/home/workstation04/anaconda3/lib -L/home/workstation04/anaconda3/lib /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/fused_dense.o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/fused_dense_cuda.o -L/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-39/fused_dense_cuda.cpython-39-x86_64-linux-gnu.so\n",
      "  building 'scaled_upper_triang_masked_softmax_cuda' extension\n",
      "  creating /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/megatron\n",
      "  Emitting ninja build file /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/build.ninja...\n",
      "  Compiling objects...\n",
      "  Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "  [1/2] c++ -MMD -MF /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/megatron/scaled_upper_triang_masked_softmax.o.d -pthread -B /home/workstation04/anaconda3/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/workstation04/anaconda3/include -I/home/workstation04/anaconda3/include -fPIC -O2 -isystem /home/workstation04/anaconda3/include -fPIC -I/home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/TH -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/workstation04/anaconda3/include/python3.9 -c -c /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/megatron/scaled_upper_triang_masked_softmax.cpp -o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/megatron/scaled_upper_triang_masked_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
      "  [2/2] /usr/local/cuda/bin/nvcc  -I/home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/TH -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/workstation04/anaconda3/include/python3.9 -c -c /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/megatron/scaled_upper_triang_masked_softmax_cuda.cu -o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/megatron/scaled_upper_triang_masked_softmax_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_61,code=compute_61 -gencode=arch=compute_61,code=sm_61 -std=c++17\n",
      "  g++ -pthread -B /home/workstation04/anaconda3/compiler_compat -shared -Wl,-rpath,/home/workstation04/anaconda3/lib -Wl,-rpath-link,/home/workstation04/anaconda3/lib -L/home/workstation04/anaconda3/lib -L/home/workstation04/anaconda3/lib -Wl,-rpath,/home/workstation04/anaconda3/lib -Wl,-rpath-link,/home/workstation04/anaconda3/lib -L/home/workstation04/anaconda3/lib /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/megatron/scaled_upper_triang_masked_softmax.o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/megatron/scaled_upper_triang_masked_softmax_cuda.o -L/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-39/scaled_upper_triang_masked_softmax_cuda.cpython-39-x86_64-linux-gnu.so\n",
      "  building 'generic_scaled_masked_softmax_cuda' extension\n",
      "  Emitting ninja build file /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/build.ninja...\n",
      "  Compiling objects...\n",
      "  Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "  [1/2] c++ -MMD -MF /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/megatron/generic_scaled_masked_softmax.o.d -pthread -B /home/workstation04/anaconda3/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/workstation04/anaconda3/include -I/home/workstation04/anaconda3/include -fPIC -O2 -isystem /home/workstation04/anaconda3/include -fPIC -I/home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/TH -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/workstation04/anaconda3/include/python3.9 -c -c /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/megatron/generic_scaled_masked_softmax.cpp -o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/megatron/generic_scaled_masked_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=generic_scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
      "  [2/2] /usr/local/cuda/bin/nvcc  -I/home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/TH -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/workstation04/anaconda3/include/python3.9 -c -c /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/megatron/generic_scaled_masked_softmax_cuda.cu -o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/megatron/generic_scaled_masked_softmax_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=generic_scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_61,code=compute_61 -gencode=arch=compute_61,code=sm_61 -std=c++17\n",
      "  g++ -pthread -B /home/workstation04/anaconda3/compiler_compat -shared -Wl,-rpath,/home/workstation04/anaconda3/lib -Wl,-rpath-link,/home/workstation04/anaconda3/lib -L/home/workstation04/anaconda3/lib -L/home/workstation04/anaconda3/lib -Wl,-rpath,/home/workstation04/anaconda3/lib -Wl,-rpath-link,/home/workstation04/anaconda3/lib -L/home/workstation04/anaconda3/lib /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/megatron/generic_scaled_masked_softmax.o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/megatron/generic_scaled_masked_softmax_cuda.o -L/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-39/generic_scaled_masked_softmax_cuda.cpython-39-x86_64-linux-gnu.so\n",
      "  building 'scaled_masked_softmax_cuda' extension\n",
      "  Emitting ninja build file /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/build.ninja...\n",
      "  Compiling objects...\n",
      "  Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "  [1/2] c++ -MMD -MF /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/megatron/scaled_masked_softmax.o.d -pthread -B /home/workstation04/anaconda3/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/workstation04/anaconda3/include -I/home/workstation04/anaconda3/include -fPIC -O2 -isystem /home/workstation04/anaconda3/include -fPIC -I/home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/TH -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/workstation04/anaconda3/include/python3.9 -c -c /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/megatron/scaled_masked_softmax.cpp -o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/megatron/scaled_masked_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
      "  [2/2] /usr/local/cuda/bin/nvcc  -I/home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/TH -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/workstation04/anaconda3/include/python3.9 -c -c /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/megatron/scaled_masked_softmax_cuda.cu -o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/megatron/scaled_masked_softmax_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_61,code=compute_61 -gencode=arch=compute_61,code=sm_61 -std=c++17\n",
      "  g++ -pthread -B /home/workstation04/anaconda3/compiler_compat -shared -Wl,-rpath,/home/workstation04/anaconda3/lib -Wl,-rpath-link,/home/workstation04/anaconda3/lib -L/home/workstation04/anaconda3/lib -L/home/workstation04/anaconda3/lib -Wl,-rpath,/home/workstation04/anaconda3/lib -Wl,-rpath-link,/home/workstation04/anaconda3/lib -L/home/workstation04/anaconda3/lib /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/megatron/scaled_masked_softmax.o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/megatron/scaled_masked_softmax_cuda.o -L/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-39/scaled_masked_softmax_cuda.cpython-39-x86_64-linux-gnu.so\n",
      "  building 'scaled_softmax_cuda' extension\n",
      "  Emitting ninja build file /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/build.ninja...\n",
      "  Compiling objects...\n",
      "  Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "  [1/2] c++ -MMD -MF /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/megatron/scaled_softmax.o.d -pthread -B /home/workstation04/anaconda3/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/workstation04/anaconda3/include -I/home/workstation04/anaconda3/include -fPIC -O2 -isystem /home/workstation04/anaconda3/include -fPIC -I/home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/TH -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/workstation04/anaconda3/include/python3.9 -c -c /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/megatron/scaled_softmax.cpp -o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/megatron/scaled_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=scaled_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
      "  [2/2] /usr/local/cuda/bin/nvcc  -I/home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/TH -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/workstation04/anaconda3/include/python3.9 -c -c /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/megatron/scaled_softmax_cuda.cu -o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/megatron/scaled_softmax_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=scaled_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_61,code=compute_61 -gencode=arch=compute_61,code=sm_61 -std=c++17\n",
      "  g++ -pthread -B /home/workstation04/anaconda3/compiler_compat -shared -Wl,-rpath,/home/workstation04/anaconda3/lib -Wl,-rpath-link,/home/workstation04/anaconda3/lib -L/home/workstation04/anaconda3/lib -L/home/workstation04/anaconda3/lib -Wl,-rpath,/home/workstation04/anaconda3/lib -Wl,-rpath-link,/home/workstation04/anaconda3/lib -L/home/workstation04/anaconda3/lib /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/megatron/scaled_softmax.o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/megatron/scaled_softmax_cuda.o -L/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-39/scaled_softmax_cuda.cpython-39-x86_64-linux-gnu.so\n",
      "  building 'fused_rotary_positional_embedding' extension\n",
      "  Emitting ninja build file /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/build.ninja...\n",
      "  Compiling objects...\n",
      "  Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "  [1/2] c++ -MMD -MF /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/megatron/fused_rotary_positional_embedding.o.d -pthread -B /home/workstation04/anaconda3/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/workstation04/anaconda3/include -I/home/workstation04/anaconda3/include -fPIC -O2 -isystem /home/workstation04/anaconda3/include -fPIC -I/home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/TH -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/workstation04/anaconda3/include/python3.9 -c -c /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/megatron/fused_rotary_positional_embedding.cpp -o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/megatron/fused_rotary_positional_embedding.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=fused_rotary_positional_embedding -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
      "  [2/2] /usr/local/cuda/bin/nvcc  -I/home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/TH -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/workstation04/anaconda3/include/python3.9 -c -c /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/megatron/fused_rotary_positional_embedding_cuda.cu -o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/megatron/fused_rotary_positional_embedding_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=fused_rotary_positional_embedding -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_61,code=compute_61 -gencode=arch=compute_61,code=sm_61 -std=c++17\n",
      "  g++ -pthread -B /home/workstation04/anaconda3/compiler_compat -shared -Wl,-rpath,/home/workstation04/anaconda3/lib -Wl,-rpath-link,/home/workstation04/anaconda3/lib -L/home/workstation04/anaconda3/lib -L/home/workstation04/anaconda3/lib -Wl,-rpath,/home/workstation04/anaconda3/lib -Wl,-rpath-link,/home/workstation04/anaconda3/lib -L/home/workstation04/anaconda3/lib /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/megatron/fused_rotary_positional_embedding.o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/megatron/fused_rotary_positional_embedding_cuda.o -L/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-39/fused_rotary_positional_embedding.cpython-39-x86_64-linux-gnu.so\n",
      "  building 'fused_weight_gradient_mlp_cuda' extension\n",
      "  Emitting ninja build file /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/build.ninja...\n",
      "  Compiling objects...\n",
      "  Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "  [1/3] c++ -MMD -MF /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/megatron/fused_weight_gradient_dense.o.d -pthread -B /home/workstation04/anaconda3/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/workstation04/anaconda3/include -I/home/workstation04/anaconda3/include -fPIC -O2 -isystem /home/workstation04/anaconda3/include -fPIC -I/home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/TH -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/workstation04/anaconda3/include/python3.9 -c -c /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/megatron/fused_weight_gradient_dense.cpp -o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/megatron/fused_weight_gradient_dense.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=fused_weight_gradient_mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
      "  [2/3] /usr/local/cuda/bin/nvcc  -I/home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/TH -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/workstation04/anaconda3/include/python3.9 -c -c /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/megatron/fused_weight_gradient_dense_16bit_prec_cuda.cu -o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/megatron/fused_weight_gradient_dense_16bit_prec_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=fused_weight_gradient_mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
      "  [3/3] /usr/local/cuda/bin/nvcc  -I/home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/TH -I/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/include/THC -I/usr/local/cuda/include -I/home/workstation04/anaconda3/include/python3.9 -c -c /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/csrc/megatron/fused_weight_gradient_dense_cuda.cu -o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/megatron/fused_weight_gradient_dense_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_90,code=sm_90 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=fused_weight_gradient_mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
      "  g++ -pthread -B /home/workstation04/anaconda3/compiler_compat -shared -Wl,-rpath,/home/workstation04/anaconda3/lib -Wl,-rpath-link,/home/workstation04/anaconda3/lib -L/home/workstation04/anaconda3/lib -L/home/workstation04/anaconda3/lib -Wl,-rpath,/home/workstation04/anaconda3/lib -Wl,-rpath-link,/home/workstation04/anaconda3/lib -L/home/workstation04/anaconda3/lib /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/megatron/fused_weight_gradient_dense.o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/megatron/fused_weight_gradient_dense_16bit_prec_cuda.o /home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/apex/build/temp.linux-x86_64-cpython-39/csrc/megatron/fused_weight_gradient_dense_cuda.o -L/home/workstation04/anaconda3/lib/python3.9/site-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-39/fused_weight_gradient_mlp_cuda.cpython-39-x86_64-linux-gnu.so\n",
      "  installing to build/bdist.linux-x86_64/wheel\n",
      "  running install\n",
      "  running install_lib\n",
      "  creating build/bdist.linux-x86_64\n",
      "  creating build/bdist.linux-x86_64/wheel\n",
      "  copying build/lib.linux-x86_64-cpython-39/scaled_upper_triang_masked_softmax_cuda.cpython-39-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
      "  copying build/lib.linux-x86_64-cpython-39/amp_C.cpython-39-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
      "  creating build/bdist.linux-x86_64/wheel/apex\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/_autocast_utils.py -> build/bdist.linux-x86_64/wheel/apex\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/fused_dense/fused_dense.py -> build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/fused_dense/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/optimizers\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/optimizers/fused_adagrad.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/optimizers/fused_novograd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/optimizers/fused_mixed_precision_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/parallel\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/parallel/distributed.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/parallel/optimized_sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/parallel/multiproc.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/parallel/optimized_sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/parallel/sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/parallel/LARC.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/parallel/sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/mlp\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/mlp/mlp.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/mlp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/transformer\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/transformer/_ucc_util.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/transformer/_data/_batchsampler.py -> build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/transformer/_data/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/transformer/functional/fused_softmax.py -> build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/transformer/functional/fused_rope.py -> build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/transformer/functional/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/transformer/microbatches.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/transformer/parallel_state.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/transformer/enums.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/transformer/layers/layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/transformer/layers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/transformer/pipeline_parallel/_timers.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/transformer/pipeline_parallel/p2p_communication.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/transformer/pipeline_parallel/schedules/common.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/transformer/pipeline_parallel/schedules/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/transformer/pipeline_parallel/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/transformer/pipeline_parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/transformer/amp/grad_scaler.py -> build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/transformer/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/transformer/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/transformer/testing/global_vars.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/transformer/testing/distributed_test_base.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/transformer/testing/standalone_bert.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/transformer/testing/arguments.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/transformer/testing/standalone_gpt.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/transformer/testing/commons.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/transformer/testing/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/transformer/testing/standalone_transformer_lm.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/transformer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/transformer/log_util.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/transformer/tensor_parallel/layers.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/transformer/tensor_parallel/memory.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/transformer/tensor_parallel/random.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/transformer/tensor_parallel/cross_entropy.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/transformer/tensor_parallel/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/transformer/tensor_parallel/mappings.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/transformer/tensor_parallel/data.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/transformer/tensor_parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/contrib\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/layer_norm/layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/layer_norm/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/contrib/cudnn_gbn\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/cudnn_gbn/batch_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/cudnn_gbn\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/cudnn_gbn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/cudnn_gbn\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/sparsity/permutation_search_kernels/channel_swap.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/sparsity/permutation_search_kernels/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/sparsity/permutation_lib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/sparsity/asp.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/sparsity/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/sparsity/sparse_masklib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/conv_bias_relu/conv_bias_relu.py -> build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/conv_bias_relu/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/optimizers/distributed_fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/optimizers/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/optimizers/distributed_fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/transducer/_transducer_ref.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/transducer/transducer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/transducer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/contrib/group_norm\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/group_norm/group_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/group_norm\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/group_norm/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/group_norm\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/multihead_attn/self_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/multihead_attn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/fmha/fmha.py -> build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/fmha/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/focal_loss/focal_loss.py -> build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/focal_loss/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/bottleneck/bottleneck.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/bottleneck/halo_exchangers.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/bottleneck/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/bottleneck/test.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/layer_norm\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/test/layer_norm/test_fast_layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/layer_norm\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/test/layer_norm/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/layer_norm\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/cudnn_gbn\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/test/cudnn_gbn/test_cudnn_gbn_with_two_gpus.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/cudnn_gbn\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/test/cudnn_gbn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/cudnn_gbn\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/conv_bias_relu\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/test/conv_bias_relu/test_conv_bias_relu.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/conv_bias_relu\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/test/conv_bias_relu/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/conv_bias_relu\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/optimizers\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/test/optimizers/test_distributed_fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/optimizers\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/test/optimizers/test_dist_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/optimizers\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/test/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/optimizers\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/transducer\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/test/transducer/test_transducer_loss.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/transducer\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/test/transducer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/transducer\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/test/transducer/test_transducer_joint.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/transducer\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/group_norm\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/test/group_norm/test_group_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/group_norm\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/test/group_norm/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/group_norm\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/test/multihead_attn/test_self_multihead_attn_norm_add.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/test/multihead_attn/test_mha_fused_softmax.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/test/multihead_attn/test_encdec_multihead_attn_norm_add.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/test/multihead_attn/test_self_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/test/multihead_attn/test_encdec_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/test/multihead_attn/test_fast_self_multihead_attn_bias.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/test/multihead_attn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/multihead_attn\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/fmha\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/test/fmha/test_fmha.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/fmha\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/test/fmha/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/fmha\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/focal_loss\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/test/focal_loss/test_focal_loss.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/focal_loss\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/test/focal_loss/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/focal_loss\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/bottleneck\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/test/bottleneck/test_bottleneck_module.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/bottleneck\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/test/bottleneck/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/bottleneck\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/index_mul_2d\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/test/index_mul_2d/test_index_mul_2d.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/index_mul_2d\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/test/index_mul_2d/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/index_mul_2d\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/test/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/xentropy\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/test/xentropy/test_label_smoothing.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/xentropy\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/test/xentropy/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/xentropy\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/clip_grad\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/test/clip_grad/test_clip_grad.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/clip_grad\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/test/clip_grad/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/clip_grad\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/contrib/test/peer_memory\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/test/peer_memory/test_peer_halo_exchange_module.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/peer_memory\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/test/peer_memory/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/test/peer_memory\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/groupbn/batch_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/groupbn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/index_mul_2d/index_mul_2d.py -> build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/index_mul_2d/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/xentropy/softmax_xentropy.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/xentropy/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/contrib/openfold_triton\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/openfold_triton/_layer_norm_config_ampere.py -> build/bdist.linux-x86_64/wheel/apex/contrib/openfold_triton\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/openfold_triton/mha.py -> build/bdist.linux-x86_64/wheel/apex/contrib/openfold_triton\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/openfold_triton/_layer_norm_backward_kernels.py -> build/bdist.linux-x86_64/wheel/apex/contrib/openfold_triton\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/openfold_triton/layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/openfold_triton\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/openfold_triton/_mha_kernel.py -> build/bdist.linux-x86_64/wheel/apex/contrib/openfold_triton\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/openfold_triton/fused_adam_swa.py -> build/bdist.linux-x86_64/wheel/apex/contrib/openfold_triton\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/openfold_triton/_layer_norm_config_hopper.py -> build/bdist.linux-x86_64/wheel/apex/contrib/openfold_triton\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/openfold_triton/_layer_norm_forward_kernels.py -> build/bdist.linux-x86_64/wheel/apex/contrib/openfold_triton\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/openfold_triton/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/openfold_triton\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/clip_grad/clip_grad.py -> build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/clip_grad/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/peer_memory/peer_memory.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/contrib/peer_memory/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/amp\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/amp/wrap.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/amp/rnn_compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/amp/compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/amp/_process_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/amp/lists/torch_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/amp/lists/tensor_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/amp/lists/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/amp/lists/functional_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/amp/_amp_state.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/amp/_initialize.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/amp/__version__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/amp/amp.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/amp/opt.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/amp/utils.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/amp/handle.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/amp/frontend.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/amp/scaler.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/multi_tensor_apply/multi_tensor_apply.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/multi_tensor_apply/__init__.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/RNN\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/RNN/models.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/RNN/RNNBackend.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/RNN/cells.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/RNN/__init__.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/fp16_utils/loss_scaler.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/fp16_utils/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/fp16_utils/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/fp16_utils/fp16util.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/__init__.py -> build/bdist.linux-x86_64/wheel/apex\n",
      "  creating build/bdist.linux-x86_64/wheel/apex/normalization\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/normalization/fused_layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
      "  copying build/lib.linux-x86_64-cpython-39/apex/normalization/__init__.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
      "  copying build/lib.linux-x86_64-cpython-39/fused_rotary_positional_embedding.cpython-39-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
      "  copying build/lib.linux-x86_64-cpython-39/generic_scaled_masked_softmax_cuda.cpython-39-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
      "  copying build/lib.linux-x86_64-cpython-39/scaled_softmax_cuda.cpython-39-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
      "  copying build/lib.linux-x86_64-cpython-39/fused_layer_norm_cuda.cpython-39-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
      "  copying build/lib.linux-x86_64-cpython-39/fused_dense_cuda.cpython-39-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
      "  copying build/lib.linux-x86_64-cpython-39/fused_weight_gradient_mlp_cuda.cpython-39-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
      "  copying build/lib.linux-x86_64-cpython-39/syncbn.cpython-39-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
      "  copying build/lib.linux-x86_64-cpython-39/mlp_cuda.cpython-39-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
      "  copying build/lib.linux-x86_64-cpython-39/scaled_masked_softmax_cuda.cpython-39-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel\n",
      "  running install_egg_info\n",
      "  running egg_info\n",
      "  creating apex.egg-info\n",
      "  writing apex.egg-info/PKG-INFO\n",
      "  writing dependency_links to apex.egg-info/dependency_links.txt\n",
      "  writing requirements to apex.egg-info/requires.txt\n",
      "  writing top-level names to apex.egg-info/top_level.txt\n",
      "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
      "  reading manifest file 'apex.egg-info/SOURCES.txt'\n",
      "  adding license file 'LICENSE'\n",
      "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
      "  Copying apex.egg-info to build/bdist.linux-x86_64/wheel/apex-0.1-py3.9.egg-info\n",
      "  running install_scripts\n",
      "  creating build/bdist.linux-x86_64/wheel/apex-0.1.dist-info/WHEEL\n",
      "  creating '/tmp/pip-wheel-omy42rjc/.tmp-zdkpn0pn/apex-0.1-cp39-cp39-linux_x86_64.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
      "  adding 'amp_C.cpython-39-x86_64-linux-gnu.so'\n",
      "  adding 'fused_dense_cuda.cpython-39-x86_64-linux-gnu.so'\n",
      "  adding 'fused_layer_norm_cuda.cpython-39-x86_64-linux-gnu.so'\n",
      "  adding 'fused_rotary_positional_embedding.cpython-39-x86_64-linux-gnu.so'\n",
      "  adding 'fused_weight_gradient_mlp_cuda.cpython-39-x86_64-linux-gnu.so'\n",
      "  adding 'generic_scaled_masked_softmax_cuda.cpython-39-x86_64-linux-gnu.so'\n",
      "  adding 'mlp_cuda.cpython-39-x86_64-linux-gnu.so'\n",
      "  adding 'scaled_masked_softmax_cuda.cpython-39-x86_64-linux-gnu.so'\n",
      "  adding 'scaled_softmax_cuda.cpython-39-x86_64-linux-gnu.so'\n",
      "  adding 'scaled_upper_triang_masked_softmax_cuda.cpython-39-x86_64-linux-gnu.so'\n",
      "  adding 'syncbn.cpython-39-x86_64-linux-gnu.so'\n",
      "  adding 'apex/__init__.py'\n",
      "  adding 'apex/_autocast_utils.py'\n",
      "  adding 'apex/RNN/RNNBackend.py'\n",
      "  adding 'apex/RNN/__init__.py'\n",
      "  adding 'apex/RNN/cells.py'\n",
      "  adding 'apex/RNN/models.py'\n",
      "  adding 'apex/amp/__init__.py'\n",
      "  adding 'apex/amp/__version__.py'\n",
      "  adding 'apex/amp/_amp_state.py'\n",
      "  adding 'apex/amp/_initialize.py'\n",
      "  adding 'apex/amp/_process_optimizer.py'\n",
      "  adding 'apex/amp/amp.py'\n",
      "  adding 'apex/amp/compat.py'\n",
      "  adding 'apex/amp/frontend.py'\n",
      "  adding 'apex/amp/handle.py'\n",
      "  adding 'apex/amp/opt.py'\n",
      "  adding 'apex/amp/rnn_compat.py'\n",
      "  adding 'apex/amp/scaler.py'\n",
      "  adding 'apex/amp/utils.py'\n",
      "  adding 'apex/amp/wrap.py'\n",
      "  adding 'apex/amp/lists/__init__.py'\n",
      "  adding 'apex/amp/lists/functional_overrides.py'\n",
      "  adding 'apex/amp/lists/tensor_overrides.py'\n",
      "  adding 'apex/amp/lists/torch_overrides.py'\n",
      "  adding 'apex/contrib/__init__.py'\n",
      "  adding 'apex/contrib/bottleneck/__init__.py'\n",
      "  adding 'apex/contrib/bottleneck/bottleneck.py'\n",
      "  adding 'apex/contrib/bottleneck/halo_exchangers.py'\n",
      "  adding 'apex/contrib/bottleneck/test.py'\n",
      "  adding 'apex/contrib/clip_grad/__init__.py'\n",
      "  adding 'apex/contrib/clip_grad/clip_grad.py'\n",
      "  adding 'apex/contrib/conv_bias_relu/__init__.py'\n",
      "  adding 'apex/contrib/conv_bias_relu/conv_bias_relu.py'\n",
      "  adding 'apex/contrib/cudnn_gbn/__init__.py'\n",
      "  adding 'apex/contrib/cudnn_gbn/batch_norm.py'\n",
      "  adding 'apex/contrib/fmha/__init__.py'\n",
      "  adding 'apex/contrib/fmha/fmha.py'\n",
      "  adding 'apex/contrib/focal_loss/__init__.py'\n",
      "  adding 'apex/contrib/focal_loss/focal_loss.py'\n",
      "  adding 'apex/contrib/group_norm/__init__.py'\n",
      "  adding 'apex/contrib/group_norm/group_norm.py'\n",
      "  adding 'apex/contrib/groupbn/__init__.py'\n",
      "  adding 'apex/contrib/groupbn/batch_norm.py'\n",
      "  adding 'apex/contrib/index_mul_2d/__init__.py'\n",
      "  adding 'apex/contrib/index_mul_2d/index_mul_2d.py'\n",
      "  adding 'apex/contrib/layer_norm/__init__.py'\n",
      "  adding 'apex/contrib/layer_norm/layer_norm.py'\n",
      "  adding 'apex/contrib/multihead_attn/__init__.py'\n",
      "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn.py'\n",
      "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn_func.py'\n",
      "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py'\n",
      "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py'\n",
      "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_func.py'\n",
      "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py'\n",
      "  adding 'apex/contrib/multihead_attn/mask_softmax_dropout_func.py'\n",
      "  adding 'apex/contrib/multihead_attn/self_multihead_attn.py'\n",
      "  adding 'apex/contrib/multihead_attn/self_multihead_attn_func.py'\n",
      "  adding 'apex/contrib/openfold_triton/__init__.py'\n",
      "  adding 'apex/contrib/openfold_triton/_layer_norm_backward_kernels.py'\n",
      "  adding 'apex/contrib/openfold_triton/_layer_norm_config_ampere.py'\n",
      "  adding 'apex/contrib/openfold_triton/_layer_norm_config_hopper.py'\n",
      "  adding 'apex/contrib/openfold_triton/_layer_norm_forward_kernels.py'\n",
      "  adding 'apex/contrib/openfold_triton/_mha_kernel.py'\n",
      "  adding 'apex/contrib/openfold_triton/fused_adam_swa.py'\n",
      "  adding 'apex/contrib/openfold_triton/layer_norm.py'\n",
      "  adding 'apex/contrib/openfold_triton/mha.py'\n",
      "  adding 'apex/contrib/optimizers/__init__.py'\n",
      "  adding 'apex/contrib/optimizers/distributed_fused_adam.py'\n",
      "  adding 'apex/contrib/optimizers/distributed_fused_lamb.py'\n",
      "  adding 'apex/contrib/optimizers/fp16_optimizer.py'\n",
      "  adding 'apex/contrib/optimizers/fused_adam.py'\n",
      "  adding 'apex/contrib/optimizers/fused_lamb.py'\n",
      "  adding 'apex/contrib/optimizers/fused_sgd.py'\n",
      "  adding 'apex/contrib/peer_memory/__init__.py'\n",
      "  adding 'apex/contrib/peer_memory/peer_halo_exchanger_1d.py'\n",
      "  adding 'apex/contrib/peer_memory/peer_memory.py'\n",
      "  adding 'apex/contrib/sparsity/__init__.py'\n",
      "  adding 'apex/contrib/sparsity/asp.py'\n",
      "  adding 'apex/contrib/sparsity/permutation_lib.py'\n",
      "  adding 'apex/contrib/sparsity/sparse_masklib.py'\n",
      "  adding 'apex/contrib/sparsity/permutation_search_kernels/__init__.py'\n",
      "  adding 'apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py'\n",
      "  adding 'apex/contrib/sparsity/permutation_search_kernels/channel_swap.py'\n",
      "  adding 'apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py'\n",
      "  adding 'apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py'\n",
      "  adding 'apex/contrib/test/__init__.py'\n",
      "  adding 'apex/contrib/test/bottleneck/__init__.py'\n",
      "  adding 'apex/contrib/test/bottleneck/test_bottleneck_module.py'\n",
      "  adding 'apex/contrib/test/clip_grad/__init__.py'\n",
      "  adding 'apex/contrib/test/clip_grad/test_clip_grad.py'\n",
      "  adding 'apex/contrib/test/conv_bias_relu/__init__.py'\n",
      "  adding 'apex/contrib/test/conv_bias_relu/test_conv_bias_relu.py'\n",
      "  adding 'apex/contrib/test/cudnn_gbn/__init__.py'\n",
      "  adding 'apex/contrib/test/cudnn_gbn/test_cudnn_gbn_with_two_gpus.py'\n",
      "  adding 'apex/contrib/test/fmha/__init__.py'\n",
      "  adding 'apex/contrib/test/fmha/test_fmha.py'\n",
      "  adding 'apex/contrib/test/focal_loss/__init__.py'\n",
      "  adding 'apex/contrib/test/focal_loss/test_focal_loss.py'\n",
      "  adding 'apex/contrib/test/group_norm/__init__.py'\n",
      "  adding 'apex/contrib/test/group_norm/test_group_norm.py'\n",
      "  adding 'apex/contrib/test/index_mul_2d/__init__.py'\n",
      "  adding 'apex/contrib/test/index_mul_2d/test_index_mul_2d.py'\n",
      "  adding 'apex/contrib/test/layer_norm/__init__.py'\n",
      "  adding 'apex/contrib/test/layer_norm/test_fast_layer_norm.py'\n",
      "  adding 'apex/contrib/test/multihead_attn/__init__.py'\n",
      "  adding 'apex/contrib/test/multihead_attn/test_encdec_multihead_attn.py'\n",
      "  adding 'apex/contrib/test/multihead_attn/test_encdec_multihead_attn_norm_add.py'\n",
      "  adding 'apex/contrib/test/multihead_attn/test_fast_self_multihead_attn_bias.py'\n",
      "  adding 'apex/contrib/test/multihead_attn/test_mha_fused_softmax.py'\n",
      "  adding 'apex/contrib/test/multihead_attn/test_self_multihead_attn.py'\n",
      "  adding 'apex/contrib/test/multihead_attn/test_self_multihead_attn_norm_add.py'\n",
      "  adding 'apex/contrib/test/optimizers/__init__.py'\n",
      "  adding 'apex/contrib/test/optimizers/test_dist_adam.py'\n",
      "  adding 'apex/contrib/test/optimizers/test_distributed_fused_lamb.py'\n",
      "  adding 'apex/contrib/test/peer_memory/__init__.py'\n",
      "  adding 'apex/contrib/test/peer_memory/test_peer_halo_exchange_module.py'\n",
      "  adding 'apex/contrib/test/transducer/__init__.py'\n",
      "  adding 'apex/contrib/test/transducer/test_transducer_joint.py'\n",
      "  adding 'apex/contrib/test/transducer/test_transducer_loss.py'\n",
      "  adding 'apex/contrib/test/xentropy/__init__.py'\n",
      "  adding 'apex/contrib/test/xentropy/test_label_smoothing.py'\n",
      "  adding 'apex/contrib/transducer/__init__.py'\n",
      "  adding 'apex/contrib/transducer/_transducer_ref.py'\n",
      "  adding 'apex/contrib/transducer/transducer.py'\n",
      "  adding 'apex/contrib/xentropy/__init__.py'\n",
      "  adding 'apex/contrib/xentropy/softmax_xentropy.py'\n",
      "  adding 'apex/fp16_utils/__init__.py'\n",
      "  adding 'apex/fp16_utils/fp16_optimizer.py'\n",
      "  adding 'apex/fp16_utils/fp16util.py'\n",
      "  adding 'apex/fp16_utils/loss_scaler.py'\n",
      "  adding 'apex/fused_dense/__init__.py'\n",
      "  adding 'apex/fused_dense/fused_dense.py'\n",
      "  adding 'apex/mlp/__init__.py'\n",
      "  adding 'apex/mlp/mlp.py'\n",
      "  adding 'apex/multi_tensor_apply/__init__.py'\n",
      "  adding 'apex/multi_tensor_apply/multi_tensor_apply.py'\n",
      "  adding 'apex/normalization/__init__.py'\n",
      "  adding 'apex/normalization/fused_layer_norm.py'\n",
      "  adding 'apex/optimizers/__init__.py'\n",
      "  adding 'apex/optimizers/fused_adagrad.py'\n",
      "  adding 'apex/optimizers/fused_adam.py'\n",
      "  adding 'apex/optimizers/fused_lamb.py'\n",
      "  adding 'apex/optimizers/fused_mixed_precision_lamb.py'\n",
      "  adding 'apex/optimizers/fused_novograd.py'\n",
      "  adding 'apex/optimizers/fused_sgd.py'\n",
      "  adding 'apex/parallel/LARC.py'\n",
      "  adding 'apex/parallel/__init__.py'\n",
      "  adding 'apex/parallel/distributed.py'\n",
      "  adding 'apex/parallel/multiproc.py'\n",
      "  adding 'apex/parallel/optimized_sync_batchnorm.py'\n",
      "  adding 'apex/parallel/optimized_sync_batchnorm_kernel.py'\n",
      "  adding 'apex/parallel/sync_batchnorm.py'\n",
      "  adding 'apex/parallel/sync_batchnorm_kernel.py'\n",
      "  adding 'apex/transformer/__init__.py'\n",
      "  adding 'apex/transformer/_ucc_util.py'\n",
      "  adding 'apex/transformer/enums.py'\n",
      "  adding 'apex/transformer/log_util.py'\n",
      "  adding 'apex/transformer/microbatches.py'\n",
      "  adding 'apex/transformer/parallel_state.py'\n",
      "  adding 'apex/transformer/utils.py'\n",
      "  adding 'apex/transformer/_data/__init__.py'\n",
      "  adding 'apex/transformer/_data/_batchsampler.py'\n",
      "  adding 'apex/transformer/amp/__init__.py'\n",
      "  adding 'apex/transformer/amp/grad_scaler.py'\n",
      "  adding 'apex/transformer/functional/__init__.py'\n",
      "  adding 'apex/transformer/functional/fused_rope.py'\n",
      "  adding 'apex/transformer/functional/fused_softmax.py'\n",
      "  adding 'apex/transformer/layers/__init__.py'\n",
      "  adding 'apex/transformer/layers/layer_norm.py'\n",
      "  adding 'apex/transformer/pipeline_parallel/__init__.py'\n",
      "  adding 'apex/transformer/pipeline_parallel/_timers.py'\n",
      "  adding 'apex/transformer/pipeline_parallel/p2p_communication.py'\n",
      "  adding 'apex/transformer/pipeline_parallel/utils.py'\n",
      "  adding 'apex/transformer/pipeline_parallel/schedules/__init__.py'\n",
      "  adding 'apex/transformer/pipeline_parallel/schedules/common.py'\n",
      "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py'\n",
      "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py'\n",
      "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py'\n",
      "  adding 'apex/transformer/tensor_parallel/__init__.py'\n",
      "  adding 'apex/transformer/tensor_parallel/cross_entropy.py'\n",
      "  adding 'apex/transformer/tensor_parallel/data.py'\n",
      "  adding 'apex/transformer/tensor_parallel/layers.py'\n",
      "  adding 'apex/transformer/tensor_parallel/mappings.py'\n",
      "  adding 'apex/transformer/tensor_parallel/memory.py'\n",
      "  adding 'apex/transformer/tensor_parallel/random.py'\n",
      "  adding 'apex/transformer/tensor_parallel/utils.py'\n",
      "  adding 'apex/transformer/testing/__init__.py'\n",
      "  adding 'apex/transformer/testing/arguments.py'\n",
      "  adding 'apex/transformer/testing/commons.py'\n",
      "  adding 'apex/transformer/testing/distributed_test_base.py'\n",
      "  adding 'apex/transformer/testing/global_vars.py'\n",
      "  adding 'apex/transformer/testing/standalone_bert.py'\n",
      "  adding 'apex/transformer/testing/standalone_gpt.py'\n",
      "  adding 'apex/transformer/testing/standalone_transformer_lm.py'\n",
      "  adding 'apex-0.1.dist-info/LICENSE'\n",
      "  adding 'apex-0.1.dist-info/METADATA'\n",
      "  adding 'apex-0.1.dist-info/WHEEL'\n",
      "  adding 'apex-0.1.dist-info/top_level.txt'\n",
      "  adding 'apex-0.1.dist-info/RECORD'\n",
      "  removing build/bdist.linux-x86_64/wheel\n",
      "\u001b[?25hdone\n",
      "  Created wheel for apex: filename=apex-0.1-cp39-cp39-linux_x86_64.whl size=5137217 sha256=8a052325c8db7df5f7560db2616cd916071b42bf81d10a584d69726bfaa3219a\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-zhgcy24t/wheels/52/00/77/9c83781f0c6ead3503472fcadd2df0584ddbd01d2a4592b1cb\n",
      "Successfully built apex\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/workstation04/anaconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: apex\n",
      "  Attempting uninstall: apex\n",
      "    Found existing installation: apex 0.1\n",
      "    Uninstalling apex-0.1:\n",
      "      Removing file or directory /home/workstation04/anaconda3/lib/python3.9/site-packages/amp_C.cpython-39-x86_64-linux-gnu.so\n",
      "      Removing file or directory /home/workstation04/anaconda3/lib/python3.9/site-packages/apex-0.1.dist-info/\n",
      "      Removing file or directory /home/workstation04/anaconda3/lib/python3.9/site-packages/apex/\n",
      "      Removing file or directory /home/workstation04/anaconda3/lib/python3.9/site-packages/fused_dense_cuda.cpython-39-x86_64-linux-gnu.so\n",
      "      Removing file or directory /home/workstation04/anaconda3/lib/python3.9/site-packages/fused_layer_norm_cuda.cpython-39-x86_64-linux-gnu.so\n",
      "      Removing file or directory /home/workstation04/anaconda3/lib/python3.9/site-packages/fused_rotary_positional_embedding.cpython-39-x86_64-linux-gnu.so\n",
      "      Removing file or directory /home/workstation04/anaconda3/lib/python3.9/site-packages/fused_weight_gradient_mlp_cuda.cpython-39-x86_64-linux-gnu.so\n",
      "      Removing file or directory /home/workstation04/anaconda3/lib/python3.9/site-packages/generic_scaled_masked_softmax_cuda.cpython-39-x86_64-linux-gnu.so\n",
      "      Removing file or directory /home/workstation04/anaconda3/lib/python3.9/site-packages/mlp_cuda.cpython-39-x86_64-linux-gnu.so\n",
      "      Removing file or directory /home/workstation04/anaconda3/lib/python3.9/site-packages/scaled_masked_softmax_cuda.cpython-39-x86_64-linux-gnu.so\n",
      "      Removing file or directory /home/workstation04/anaconda3/lib/python3.9/site-packages/scaled_softmax_cuda.cpython-39-x86_64-linux-gnu.so\n",
      "      Removing file or directory /home/workstation04/anaconda3/lib/python3.9/site-packages/scaled_upper_triang_masked_softmax_cuda.cpython-39-x86_64-linux-gnu.so\n",
      "      Removing file or directory /home/workstation04/anaconda3/lib/python3.9/site-packages/syncbn.cpython-39-x86_64-linux-gnu.so\n",
      "      Successfully uninstalled apex-0.1\n",
      "Successfully installed apex-0.1\n"
     ]
    }
   ],
   "source": [
    "! pip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --config-settings \"--build-option=--cuda_ext\" ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/workstation04/anaconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting git+https://github.com/NVIDIA/dllogger\n",
      "  Cloning https://github.com/NVIDIA/dllogger to /tmp/pip-req-build-gzp99nwq\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/NVIDIA/dllogger /tmp/pip-req-build-gzp99nwq\n",
      "  Resolved https://github.com/NVIDIA/dllogger to commit 0540a43971f4a8a16693a9de9de73c1072020769\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/workstation04/anaconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install DLLogger\n",
    "!pip install 'git+https://github.com/NVIDIA/dllogger'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/workstation04/anaconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: rich in /home/workstation04/anaconda3/lib/python3.9/site-packages (13.7.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from rich) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from rich) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/workstation04/anaconda3/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich) (0.1.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/workstation04/anaconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install or update the Rich library\n",
    "!pip install -U rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Navigate to the Apex directory\n",
    "import os\n",
    "os.chdir(\"/home/workstation04/GP_MMMAI/Notebooks/NVIDIA_nnUNet/\")\n",
    "os. getcwd()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
