{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 7161487,
     "sourceType": "datasetVersion",
     "datasetId": 4136399
    }
   ],
   "dockerImageVersionId": 30648,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2024-02-13T03:53:34.437897Z",
     "iopub.execute_input": "2024-02-13T03:53:34.438587Z",
     "iopub.status.idle": "2024-02-13T03:53:35.250643Z",
     "shell.execute_reply.started": "2024-02-13T03:53:34.438558Z",
     "shell.execute_reply": "2024-02-13T03:53:35.249697Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import subprocess\n",
    "import nibabel as nib\n",
    "import tensorflow as tf\n",
    "from skimage import exposure\n",
    "from math import ceil\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, data_path, split_ratio, batch_size=None):\n",
    "        self.data_path = data_path\n",
    "        self.split_ratio = split_ratio\n",
    "        self.all_subjects = None\n",
    "        self.subjects_lists = []\n",
    "        self.labels = {'train': 0, 'test': 1, 'validation': 2}\n",
    "        self.size = [0, 0, 0]\n",
    "        self.batch_size = batch_size\n",
    "        self.slices_number = None\n",
    "\n",
    "    def list_subjects(self):\n",
    "        subjects = os.listdir(self.data_path)\n",
    "        subjects = [item for item in subjects if item.startswith('sub')]\n",
    "        self.all_subjects = subjects\n",
    "\n",
    "    def get_nifti_path(self, subject, number_of_motion='1'):\n",
    "        ref_path_stand = f'{self.data_path}/{subject}/anat/{subject}_acq-standard_T1w.nii/'\n",
    "        select_path_stand = subprocess.run(['ls', ref_path_stand], capture_output=True, text=True).stdout.replace(\"\\n\", \"\")\n",
    "\n",
    "        ref_path_motion = f'{self.data_path}/{subject}/anat/{subject}_acq-headmotion{number_of_motion}_T1w.nii/'\n",
    "        select_path_motion = subprocess.run(['ls', ref_path_motion], capture_output=True, text=True).stdout.replace(\"\\n\", \"\")\n",
    "\n",
    "        return [ref_path_stand + select_path_stand, ref_path_motion + select_path_motion]\n",
    "\n",
    "    def get_paired_volumes(self, path):\n",
    "        if os.path.exists(path[0]) and os.path.exists(path[1]):\n",
    "            free_data = nib.load(path[0]).get_fdata()\n",
    "#             free_data = exposure.rescale_intensity(free_data, out_range=(0.0, 1.0))\n",
    "            free_data = exposure.rescale_intensity(free_data, out_range=(-1.0, 1.0))\n",
    "\n",
    "            motion_data = nib.load(path[1]).get_fdata()\n",
    "#             motion_data = exposure.rescale_intensity(motion_data, out_range=(0.0, 1.0))\n",
    "            motion_data = exposure.rescale_intensity(motion_data, out_range=(-1.0, 1.0))\n",
    "            return tf.convert_to_tensor(free_data), tf.convert_to_tensor(motion_data)\n",
    "        else:\n",
    "            return None, None\n",
    "\n",
    "    def split_data(self):\n",
    "        self.list_subjects()\n",
    "        if ceil(sum(self.split_ratio)) == 1 and len(self.split_ratio) <= 3:\n",
    "            self.split_ratio.insert(0, 0)\n",
    "            cumulative_sum = [sum(self.split_ratio[:i + 1]) for i in range(len(self.split_ratio))]\n",
    "            number_of_subjects = len(self.all_subjects)\n",
    "\n",
    "            for i in range(1, len(self.split_ratio)):\n",
    "                self.subjects_lists.append(\n",
    "                    self.all_subjects[int(round(cumulative_sum[i - 1] * number_of_subjects)):int(\n",
    "                        round(cumulative_sum[i] * number_of_subjects))])\n",
    "\n",
    "                self.size[i - 1] = len(self.subjects_lists[i - 1])  * 2 * 190\n",
    "\n",
    "                if i - 1 == 0:\n",
    "                    self.size[i - 1] -= 8  * 2 * 190\n",
    "        else:\n",
    "            print(\"The Summation of ratios is not equal to 1\")\n",
    "       \n",
    "    def generator(self, mode):\n",
    "        subjects = self.subjects_lists[self.labels[mode]]\n",
    "\n",
    "        def data_gen():\n",
    "            for subject in subjects:\n",
    "                for i in range(2):\n",
    "                    pathes = self.get_nifti_path(subject, str(i + 1))\n",
    "                    free, motion = self.get_paired_volumes(pathes)\n",
    "                    if motion is not None:\n",
    "                        self.slices_number = motion.shape[0]\n",
    "\n",
    "                        for slice_id in range(0, self.slices_number):\n",
    "                            start_idx = slice_id + 1\n",
    "                            end_idx = (slice_id + 1) + 1\n",
    "                            if (end_idx < self.slices_number-1):\n",
    "                                free_slice = free[start_idx:end_idx]\n",
    "                                free_slice = tf.transpose(free_slice, perm=[1, 2, 0])\n",
    "                                \n",
    "                                motion_slice = motion[start_idx:end_idx]\n",
    "                                motion_slice = tf.transpose(motion_slice, perm=[1, 2, 0])\n",
    "                                \n",
    "                                motion_before_slice = motion[start_idx-1:end_idx-1]\n",
    "                                motion_before_slice = tf.transpose(motion_before_slice, perm=[1, 2, 0])\n",
    "                                \n",
    "                                motion_after_slice = motion[start_idx+1:end_idx+1]\n",
    "                                motion_after_slice = tf.transpose(motion_after_slice, perm=[1, 2, 0])\n",
    "\n",
    "                                yield (\n",
    "                                (motion_before_slice, motion_slice, motion_after_slice),\n",
    "                                free_slice\n",
    "                                )\n",
    "\n",
    "        input_signature = (\n",
    "            (tf.TensorSpec(shape=(256, 256, 1), dtype=tf.float32),\n",
    "             tf.TensorSpec(shape=(256, 256, 1), dtype=tf.float32),\n",
    "             tf.TensorSpec(shape=(256, 256, 1), dtype=tf.float32)),\n",
    "            tf.TensorSpec(shape=(256, 256, 1), dtype=tf.float32)\n",
    "        )\n",
    "\n",
    "        dataset = tf.data.Dataset.from_generator(data_gen, output_signature=input_signature)\n",
    "        dataset = dataset.batch(self.batch_size)\n",
    "\n",
    "        return dataset"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-02-13T03:53:35.252357Z",
     "iopub.execute_input": "2024-02-13T03:53:35.252715Z",
     "iopub.status.idle": "2024-02-13T03:53:47.360243Z",
     "shell.execute_reply.started": "2024-02-13T03:53:35.252691Z",
     "shell.execute_reply": "2024-02-13T03:53:47.359472Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "text": "2024-02-13 03:53:37.292180: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-13 03:53:37.292291: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-13 03:53:37.429653: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def create_center_rectangle_mask(mask_shape, rect_height, rect_width):\n",
    "    mask_shape = mask_shape[1:]\n",
    "    # Create a mask with a central rectangle of zeros\n",
    "    mask = np.ones(mask_shape, dtype=np.float32)\n",
    "\n",
    "    # Calculate the position of the top-left corner of the rectangle\n",
    "    rect_top = (mask_shape[0] - rect_height) // 2\n",
    "    rect_left = (mask_shape[1] - rect_width) // 2\n",
    "\n",
    "    # Update the mask with the rectangle at the center\n",
    "    mask[rect_top:rect_top+rect_height, rect_left:rect_left+rect_width,:] = 0\n",
    "\n",
    "    # Convert the NumPy array to a TensorFlow tensor\n",
    "    mask_tensor = tf.convert_to_tensor(mask, dtype=tf.float32)\n",
    "\n",
    "    return mask_tensor\n",
    "\n",
    "def crop_center_rectangle_mask(tensor, rect_height=50, rect_width=100):\n",
    "    mask = create_center_rectangle_mask(tensor.shape, rect_height, rect_height)\n",
    "    return tf.multiply(tensor, mask)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-02-13T03:53:47.361190Z",
     "iopub.execute_input": "2024-02-13T03:53:47.361703Z",
     "iopub.status.idle": "2024-02-13T03:53:47.368545Z",
     "shell.execute_reply.started": "2024-02-13T03:53:47.361678Z",
     "shell.execute_reply": "2024-02-13T03:53:47.367563Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "IMAGE_ORDERING_CHANNELS_LAST = \"channels_last\"\n",
    "IMAGE_ORDERING_CHANNELS_FIRST = \"channels_first\"\n",
    "\n",
    "# Default IMAGE_ORDERING = channels_last\n",
    "IMAGE_ORDERING = IMAGE_ORDERING_CHANNELS_LAST\n",
    "\n",
    "if IMAGE_ORDERING == 'channels_first':\n",
    "\tMERGE_AXIS = 1\n",
    "elif IMAGE_ORDERING == 'channels_last':\n",
    "\tMERGE_AXIS = -1\n",
    "\n",
    "# CBAM --------------------------------------------\n",
    "# Convolutional Block Attention Module(CBAM) block\n",
    "def cbam_block(cbam_feature, ratio=8):\n",
    "\tcbam_feature = channel_attention(cbam_feature, ratio)\n",
    "\tcbam_feature = spatial_attention(cbam_feature)\n",
    "\treturn cbam_feature\n",
    "\n",
    "def channel_attention(input_feature, ratio=8):\n",
    "\n",
    "\tchannel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "\tchannel = input_feature.shape[channel_axis]  # input_feature._keras_shape[channel_axis]\n",
    "\n",
    "\tshared_layer_one = Dense(channel//ratio,\n",
    "\t\t\t\t\t\t\t activation='relu',\n",
    "\t\t\t\t\t\t\t kernel_initializer='he_normal',\n",
    "\t\t\t\t\t\t\t use_bias=True,\n",
    "\t\t\t\t\t\t\t bias_initializer='zeros')\n",
    "\tshared_layer_two = Dense(channel,\n",
    "\t\t\t\t\t\t\t kernel_initializer='he_normal',\n",
    "\t\t\t\t\t\t\t use_bias=True,\n",
    "\t\t\t\t\t\t\t bias_initializer='zeros')\n",
    "\n",
    "\tavg_pool = GlobalAveragePooling2D()(input_feature)\n",
    "\tavg_pool = Reshape((1,1,channel))(avg_pool)\n",
    "\tassert avg_pool.shape[1:] == (1,1,channel)\n",
    "\tavg_pool = shared_layer_one(avg_pool)\n",
    "\tassert avg_pool.shape[1:] == (1,1,channel//ratio)\n",
    "\tavg_pool = shared_layer_two(avg_pool)\n",
    "\tassert avg_pool.shape[1:] == (1,1,channel)\n",
    "\n",
    "\tmax_pool = GlobalMaxPooling2D()(input_feature)\n",
    "\tmax_pool = Reshape((1,1,channel))(max_pool)\n",
    "\tassert max_pool.shape[1:] == (1,1,channel)\n",
    "\tmax_pool = shared_layer_one(max_pool)\n",
    "\tassert max_pool.shape[1:] == (1,1,channel//ratio)\n",
    "\tmax_pool = shared_layer_two(max_pool)\n",
    "\tassert max_pool.shape[1:] == (1,1,channel)\n",
    "\n",
    "\tcbam_feature = Add()([avg_pool,max_pool])\n",
    "\tcbam_feature = Activation('sigmoid')(cbam_feature)\n",
    "\n",
    "\tif K.image_data_format() == \"channels_first\":\n",
    "\t\tcbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "\n",
    "\treturn multiply([input_feature, cbam_feature])\n",
    "\n",
    "def spatial_attention(input_feature):\n",
    "\tkernel_size = 7\n",
    "\n",
    "\tif K.image_data_format() == \"channels_first\":\n",
    "\t\tchannel = input_feature._keras_shape[1]\n",
    "\t\tcbam_feature = Permute((2,3,1))(input_feature)\n",
    "\telse:\n",
    "\t\tchannel = input_feature.shape[-1]\n",
    "\t\tcbam_feature = input_feature\n",
    "\n",
    "\tavg_pool = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(cbam_feature)\n",
    "\tassert avg_pool.shape[-1] == 1\n",
    "\tmax_pool = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(cbam_feature)\n",
    "\tassert max_pool.shape[-1] == 1\n",
    "\tconcat = Concatenate(axis=3)([avg_pool, max_pool])\n",
    "\tassert concat.shape[-1] == 2\n",
    "\tcbam_feature = Conv2D(filters = 1,\n",
    "\t\t\t\t\tkernel_size=kernel_size,\n",
    "\t\t\t\t\tstrides=1,\n",
    "\t\t\t\t\tpadding='same',\n",
    "\t\t\t\t\tactivation='sigmoid',\n",
    "\t\t\t\t\tkernel_initializer='he_normal',\n",
    "\t\t\t\t\tuse_bias=False)(concat)\n",
    "\tassert cbam_feature.shape[-1] == 1\n",
    "\n",
    "\tif K.image_data_format() == \"channels_first\":\n",
    "\t\tcbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "\n",
    "\treturn multiply([input_feature, cbam_feature])\n",
    "\n",
    "def UNet(img_input):\n",
    "\tk1 = 32\n",
    "\tk2 = 64\n",
    "\tk3 = 128\n",
    "\tk4 = 256\n",
    "\t# Block 1 in Contracting Path\n",
    "\tconv1 = Conv2D(k1, (3, 3), data_format=IMAGE_ORDERING,padding='same', dilation_rate=1)(img_input)\n",
    "\tconv1 = BatchNormalization()(conv1)\n",
    "\tconv1 = Activation(tf.nn.leaky_relu)(conv1)\n",
    "\t#conv1 = Dropout(0.2)(conv1)\n",
    "\tconv1 = Conv2D(k1, (3, 3), data_format=IMAGE_ORDERING, padding='same', dilation_rate=1)(conv1)\n",
    "\tconv1 = BatchNormalization()(conv1)\n",
    "\tconv1 = Activation(tf.nn.leaky_relu)(conv1)\n",
    "\n",
    "\tconv1 = cbam_block(conv1)    # Convolutional Block Attention Module(CBAM) block\n",
    "\n",
    "\to = AveragePooling2D((2, 2), strides=(2, 2))(conv1)\n",
    "\n",
    "\t# Block 2 in Contracting Path\n",
    "\tconv2 = Conv2D(k2, (3, 3), data_format=IMAGE_ORDERING, padding='same', dilation_rate=1)(o)\n",
    "\tconv2 = BatchNormalization()(conv2)\n",
    "\tconv2 = Activation(tf.nn.leaky_relu)(conv2)\n",
    "\tconv2 = Dropout(0.2)(conv2)\n",
    "\tconv2 = Conv2D(k2, (3, 3), data_format=IMAGE_ORDERING, padding='same', dilation_rate=1)(conv2)\n",
    "\tconv2 = BatchNormalization()(conv2)\n",
    "\tconv2 = Activation(tf.nn.leaky_relu)(conv2)\n",
    "\n",
    "\tconv2 = cbam_block(conv2)    # Convolutional Block Attention Module(CBAM) block\n",
    "\n",
    "\to = AveragePooling2D((2, 2), strides=(2, 2))(conv2)\n",
    "\n",
    "\t# Block 3 in Contracting Path\n",
    "\tconv3 = Conv2D(k3, (3, 3), data_format=IMAGE_ORDERING, padding='same', dilation_rate=1)(o)\n",
    "\tconv3 = BatchNormalization()(conv3)\n",
    "\tconv3 = Activation(tf.nn.leaky_relu)(conv3)\n",
    "\t#conv3 = Dropout(0.2)(conv3)\n",
    "\tconv3 = Conv2D(k3, (3, 3), data_format=IMAGE_ORDERING, padding='same', dilation_rate=1)(conv3)\n",
    "\tconv3 = BatchNormalization()(conv3)\n",
    "\tconv3 = Activation(tf.nn.leaky_relu)(conv3)\n",
    "\n",
    "\tconv3 = cbam_block(conv3)    # Convolutional Block Attention Module(CBAM) block\n",
    "\n",
    "\to = AveragePooling2D((2, 2), strides=(2, 2))(conv3)\n",
    "\n",
    "\t # Transition layer between contracting and expansive paths:\n",
    "\tconv4 = Conv2D(k4, (3, 3), data_format=IMAGE_ORDERING, padding='same', dilation_rate=1)(o)\n",
    "\tconv4 = BatchNormalization()(conv4)\n",
    "\tconv4 = Activation(tf.nn.leaky_relu)(conv4)\n",
    "\t#conv4 = Dropout(0.2)(conv4)\n",
    "\tconv4 = Conv2D(k4, (3, 3), data_format=IMAGE_ORDERING, padding='same', dilation_rate=1)(conv4)\n",
    "\tconv4 = BatchNormalization()(conv4)\n",
    "\tconv4 =Activation(tf.nn.leaky_relu)(conv4)\n",
    "\n",
    "\tconv4 = cbam_block(conv4)    # Convolutional Block Attention Module(CBAM) block\n",
    "\n",
    "\n",
    "\t# Block 1 in Expansive Path\n",
    "\tup1 = UpSampling2D((2, 2), data_format=IMAGE_ORDERING)(conv4)\n",
    "\tup1 = concatenate([up1, conv3], axis=MERGE_AXIS)\n",
    "\tdeconv1 =  Conv2D(k3, (3, 3), data_format=IMAGE_ORDERING, padding='same', dilation_rate=1)(up1)\n",
    "\tdeconv1 = BatchNormalization()(deconv1)\n",
    "\tdeconv1 = Activation(tf.nn.leaky_relu)(deconv1)\n",
    "\t#deconv1 = Dropout(0.2)(deconv1)\n",
    "\tdeconv1 =  Conv2D(k3, (3, 3), data_format=IMAGE_ORDERING, padding='same', dilation_rate=1)(deconv1)\n",
    "\tdeconv1 = BatchNormalization()(deconv1)\n",
    "\tdeconv1 = Activation(tf.nn.leaky_relu)(deconv1)\n",
    "\n",
    "\tdeconv1 = cbam_block(deconv1)    # Convolutional Block Attention Module(CBAM) block\n",
    "\n",
    "\t# Block 2 in Expansive Path\n",
    "\tup2 = UpSampling2D((2, 2), data_format=IMAGE_ORDERING)(deconv1)\n",
    "\tup2 = concatenate([up2, conv2], axis=MERGE_AXIS)\n",
    "\tdeconv2 = Conv2D(k2, (3, 3), data_format=IMAGE_ORDERING, padding='same', dilation_rate=1)(up2)\n",
    "\tdeconv2 = BatchNormalization()(deconv2)\n",
    "\tdeconv2 = Activation(tf.nn.leaky_relu)(deconv2)\n",
    "\t#deconv2 = Dropout(0.2)(deconv2)\n",
    "\tdeconv2 = Conv2D(k2, (3, 3), data_format=IMAGE_ORDERING, padding='same', dilation_rate=1)(deconv2)\n",
    "\tdeconv2 = BatchNormalization()(deconv2)\n",
    "\tdeconv2 = Activation(tf.nn.leaky_relu)(deconv2)\n",
    "\n",
    "\tdeconv2 = cbam_block(deconv2)    # Convolutional Block Attention Module(CBAM) block\n",
    "\n",
    "\t# Block 3 in Expansive Path\n",
    "\tup3 = UpSampling2D((2, 2), data_format=IMAGE_ORDERING)(deconv2)\n",
    "\tup3 = concatenate([up3, conv1], axis=MERGE_AXIS)\n",
    "\tdeconv3 = Conv2D(k1, (3, 3), data_format=IMAGE_ORDERING, padding='same', dilation_rate=1)(up3)\n",
    "\tdeconv3 = BatchNormalization()(deconv3)\n",
    "\tdeconv3 = Activation(tf.nn.leaky_relu)(deconv3)\n",
    "\t#deconv3 = Dropout(0.2)(deconv3)\n",
    "\tdeconv3 = Conv2D(k1, (3, 3), data_format=IMAGE_ORDERING, padding='same', dilation_rate=1)(deconv3)\n",
    "\tdeconv3 = BatchNormalization()(deconv3)\n",
    "\tdeconv3 = Activation(tf.nn.leaky_relu)(deconv3)\n",
    "\n",
    "\tdeconv3 = cbam_block(deconv3)    # Convolutional Block Attention Module(CBAM) block\n",
    "\n",
    "\toutput = Conv2D(1, (3, 3), data_format=IMAGE_ORDERING, padding='same')(deconv3)\n",
    "# \toutput = Activation('sigmoid')(output)\n",
    "\toutput = Activation('tanh')(output)\n",
    "\treturn output\n",
    "\n",
    "def Correction_Multi_input(input_height, input_width):\n",
    "\tassert input_height % 32 == 0\n",
    "\tassert input_width % 32 == 0\n",
    "\n",
    "#   UNET\n",
    "\timg_input_1 = Input(shape=(input_height, input_width, 1))\n",
    "\timg_input_2 = Input(shape=(input_height, input_width, 1))\n",
    "\timg_input_3 = Input(shape=(input_height, input_width, 1))\n",
    "# \tkk = 32\n",
    "\tkk = 64\n",
    "\tconv1 = Conv2D(kk, (3, 3), data_format=IMAGE_ORDERING,padding='same', dilation_rate=1)(img_input_1) # dilation_rate=6\n",
    "\tconv1 = BatchNormalization()(conv1)\n",
    "\tconv1 = Activation('relu')(conv1)\n",
    "\tconv2 = Conv2D(kk, (3, 3), data_format=IMAGE_ORDERING,padding='same', dilation_rate=1)(img_input_2) # dilation_rate=6\n",
    "\tconv2 = BatchNormalization()(conv2)\n",
    "\tconv2 = Activation('relu')(conv2)\n",
    "\tconv3 = Conv2D(kk, (3, 3), data_format=IMAGE_ORDERING,padding='same', dilation_rate=1)(img_input_3) # dilation_rate=6\n",
    "\tconv3 = BatchNormalization()(conv3)\n",
    "\tconv3 = Activation('relu')(conv3)\n",
    "\n",
    "\tinput_concat = concatenate([conv1, conv2, conv3], axis=MERGE_AXIS)  #conv4\n",
    "\t# dataset = tf.data.Dataset.from_tensor_slices((img_input_1, img_input_2, img_input_3)\n",
    "\n",
    "\t## Two Stacked Nets:\n",
    "\tpred_1  = UNet(input_concat)\n",
    "\tinput_2 = concatenate([input_concat, pred_1], axis=MERGE_AXIS)\n",
    "\tpred_2  = UNet(input_2) #\n",
    "\n",
    "\tmodel = Model(inputs=[img_input_1,img_input_2,img_input_3], outputs=pred_2)\n",
    "\n",
    "\n",
    "\treturn model"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-02-13T03:53:47.371222Z",
     "iopub.execute_input": "2024-02-13T03:53:47.371555Z",
     "iopub.status.idle": "2024-02-13T03:53:47.507045Z",
     "shell.execute_reply.started": "2024-02-13T03:53:47.371520Z",
     "shell.execute_reply": "2024-02-13T03:53:47.506352Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, LearningRateScheduler\n",
    "import math\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "# Constants\n",
    "TRAIN = 1  # True False\n",
    "TEST = 0  # True False\n",
    "NB_EPOCH = 30\n",
    "LEARNING_RATE = 0.001  # 0.001 (default)\n",
    "HEIGHT, WIDTH = 256, 256\n",
    "PREDICTION_PATH = '/kaggle/working/Prediction'\n",
    "WEIGHTS_PATH = '/kaggle/working/Weights'\n",
    "\n",
    "print('Reading Data ... ')\n",
    "data_path = \"/kaggle/input/mahmoud-dataset\"\n",
    "# split_ratio = [0.7, 0.2, 0.1]\n",
    "split_ratio = [0.03, 0.92, 0.03]\n",
    "batch_size = 14\n",
    "\n",
    "data_loader = DataLoader(data_path, split_ratio, batch_size)\n",
    "data_loader.split_data()\n",
    "\n",
    "train_dataset = data_loader.generator('train')\n",
    "test_dataset = data_loader.generator('test')\n",
    "validation_dataset = data_loader.generator('validation')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-02-13T03:56:18.989282Z",
     "iopub.execute_input": "2024-02-13T03:56:18.990252Z",
     "iopub.status.idle": "2024-02-13T03:56:19.082914Z",
     "shell.execute_reply.started": "2024-02-13T03:56:18.990220Z",
     "shell.execute_reply": "2024-02-13T03:56:19.082006Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "text": "Reading Data ... \n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def ssim_score(y_true, y_pred):\n",
    "    score = tf.reduce_mean(tf.image.ssim(y_true, y_pred, 2.0))\n",
    "    return score\n",
    "\n",
    "def ssim_loss(y_true, y_pred):\n",
    "    loss_ssim = 1.0 - tf.reduce_mean(tf.image.ssim(y_true, y_pred, 2.0))\n",
    "    return loss_ssim\n",
    "\n",
    "def l2_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the L2 loss between the ground truth and predicted tensors.\n",
    "\n",
    "    Parameters:\n",
    "        y_true (tf.Tensor): Ground truth tensor.\n",
    "        y_pred (tf.Tensor): Predicted tensor.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: Normalized L2 loss.\n",
    "\n",
    "    This function calculates the mean squared error (MSE) between the ground truth\n",
    "    and predicted tensors. It then reduces the MSE along the spatial dimensions,\n",
    "    typically representing the height and width of the tensors, resulting in a\n",
    "    tensor of shape (batch_size,), where each element represents the mean MSE\n",
    "    for a single sample in the batch.\n",
    "\n",
    "    The loss is then normalized using L2 normalization to ensure that it falls\n",
    "    within the range of 0 to 1. Finally, the mean of the normalized loss across\n",
    "    the batch is computed and returned.\n",
    "    \"\"\"\n",
    "    mse = tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
    "\n",
    "    # Reduce on spatial information\n",
    "    batch_mse = tf.reduce_mean(mse, axis=(1, 2))\n",
    "\n",
    "    # Normalize the loss function to be between 0 and 1\n",
    "    normalized_loss = tf.nn.l2_normalize(batch_mse, axis=-1)\n",
    "    \n",
    "    # Compute the mean of the normalized loss across the batch\n",
    "    normalized_reduced_loss = tf.reduce_mean(batch_mse)\n",
    "\n",
    "    return normalized_reduced_loss\n",
    "\n",
    "def spatial_fft_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Custom loss function for spatial loss with FFT features.\n",
    "\n",
    "    Args:\n",
    "        y_true: Ground truth image(s).\n",
    "        y_pred: Predicted image(s).\n",
    "\n",
    "    Returns:\n",
    "        Normalized reduced spatial loss.\n",
    "\n",
    "    This function defines a custom loss for training neural networks. It applies a Fourier Transform\n",
    "    to the true and predicted images, extracts the real and imaginary parts of the transformed\n",
    "    features, and calculates the mean squared error between them. The loss is then normalized and\n",
    "    reduced to a single scalar value.\n",
    "\n",
    "    \"\"\"\n",
    "    # Apply Fourier Transform to the true and predicted images\n",
    "    true_fft = tf.signal.fft2d(tf.cast(y_true, dtype=tf.complex64))\n",
    "    pred_fft = tf.signal.fft2d(tf.cast(y_pred, dtype=tf.complex64))\n",
    "\n",
    "    # Extract Real & Imaginary parts\n",
    "    true_fft_real = tf.math.real(true_fft)\n",
    "    true_fft_imag = tf.math.imag(true_fft)\n",
    "    pred_fft_real = tf.math.real(pred_fft)\n",
    "    pred_fft_imag = tf.math.imag(pred_fft)\n",
    "\n",
    "    # Crop center rectangles for real and imag\n",
    "    true_fft_real_cropped = crop_center_rectangle_mask(true_fft_real)\n",
    "    true_fft_imag_cropped = crop_center_rectangle_mask(true_fft_imag)\n",
    "    pred_fft_real_cropped = crop_center_rectangle_mask(pred_fft_real)\n",
    "    pred_fft_imag_cropped = crop_center_rectangle_mask(pred_fft_imag)\n",
    "\n",
    "    # Calculate L2 loss\n",
    "    mse_real = tf.keras.losses.mean_squared_error(true_fft_real_cropped, pred_fft_real_cropped)\n",
    "    mse_imag = tf.keras.losses.mean_squared_error(true_fft_imag_cropped, pred_fft_imag_cropped)\n",
    "\n",
    "    # Total L2 loss\n",
    "    total_loss = 0.5 * (mse_real + mse_imag)\n",
    "    \n",
    "    # Reduce on spatial information\n",
    "    batch_loss = tf.reduce_mean(total_loss, axis=(1, 2))\n",
    "    \n",
    "    # Normalize the loss function to be between 0 and 1\n",
    "    normalized_loss = tf.nn.l2_normalize(batch_loss, axis=-1)\n",
    "\n",
    "    normalized_reduced_loss = tf.reduce_mean(normalized_loss)\n",
    "\n",
    "    return normalized_reduced_loss\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def init_vgg16_model(perceptual_layer_name='block3_conv3'):\n",
    "    \"\"\"\n",
    "    Initialize a pre-trained VGG16 model for feature extraction.\n",
    "\n",
    "    Args:\n",
    "        perceptual_layer_name: Name of the layer to extract features from.\n",
    "\n",
    "    Returns:\n",
    "        Pre-trained VGG16 model with specified layer for feature extraction.\n",
    "\n",
    "    This function loads a pre-trained VGG16 model with ImageNet weights and removes the top\n",
    "    classification layers. It then extracts the specified layer for feature extraction and\n",
    "    freezes the model's layers to prevent further training.\n",
    "\n",
    "    \"\"\"\n",
    "    # Load pre-trained VGG16 model without the top classification layers\n",
    "    vgg_model = VGG16(include_top=False, weights='imagenet', input_shape=(256, 256, 3))\n",
    "\n",
    "    # Extract the specified layer from the VGG16 model\n",
    "    perceptual_model = Model(inputs=vgg_model.input, outputs=vgg_model.get_layer(perceptual_layer_name).output)\n",
    "\n",
    "    # Freeze the layers in the perceptual model so they are not trained further\n",
    "    for layer in perceptual_model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    print(\"VGG16 Model Initialized\")\n",
    "    return perceptual_model\n",
    "\n",
    "# Initialize VGG16 model for feature extraction\n",
    "perceptual_model = init_vgg16_model()\n",
    "\n",
    "def perceptual_fft_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Custom loss function for perceptual loss with FFT features.\n",
    "\n",
    "    Args:\n",
    "        y_true: Ground truth image(s).\n",
    "        y_pred: Predicted image(s).\n",
    "\n",
    "    Returns:\n",
    "        Normalized reduced perceptual loss.\n",
    "\n",
    "    This function defines a custom loss for training neural networks. It extracts features from\n",
    "    true and predicted images using a pre-trained VGG16 model, applies a Fourier Transform to these\n",
    "    features, and calculates the mean squared error between the real and imaginary parts of the\n",
    "    transformed features. The loss is then normalized and reduced to a single scalar value.\n",
    "\n",
    "    \"\"\"\n",
    "    # Convert single-channel images to RGB\n",
    "    y_true_rgb = tf.repeat(y_true, 3, axis=-1)\n",
    "    y_pred_rgb = tf.repeat(y_pred, 3, axis=-1)\n",
    "\n",
    "    # Preprocess images for VGG16\n",
    "    y_true_processed = tf.keras.applications.vgg16.preprocess_input(y_true_rgb)\n",
    "    y_pred_processed = tf.keras.applications.vgg16.preprocess_input(y_pred_rgb)\n",
    "\n",
    "    # Extract features from specified layer for true and predicted images\n",
    "    features_true = perceptual_model(y_true_processed)\n",
    "    features_pred = perceptual_model(y_pred_processed)\n",
    "\n",
    "    # Apply Fourier Transform to the true and predicted images\n",
    "    true_fft = tf.signal.fft2d(tf.cast(features_true, dtype=tf.complex64))\n",
    "    pred_fft = tf.signal.fft2d(tf.cast(features_pred, dtype=tf.complex64))\n",
    "\n",
    "    # Extract Real & Imaginary parts\n",
    "    true_fft_real = tf.math.real(true_fft)\n",
    "    true_fft_imag = tf.math.imag(true_fft)\n",
    "    pred_fft_real = tf.math.real(pred_fft)\n",
    "    pred_fft_imag = tf.math.imag(pred_fft)\n",
    "\n",
    "    # Crop center rectangles for real and imag\n",
    "    true_fft_real_cropped = crop_center_rectangle_mask(true_fft_real)\n",
    "    true_fft_imag_cropped = crop_center_rectangle_mask(true_fft_imag)\n",
    "    pred_fft_real_cropped = crop_center_rectangle_mask(pred_fft_real)\n",
    "    pred_fft_imag_cropped = crop_center_rectangle_mask(pred_fft_imag)\n",
    "\n",
    "    # Calculate L2 loss\n",
    "    mse_real = tf.keras.losses.mean_squared_error(true_fft_real_cropped, pred_fft_real_cropped)\n",
    "    mse_imag = tf.keras.losses.mean_squared_error(true_fft_imag_cropped, pred_fft_imag_cropped)\n",
    "\n",
    "    # Total L2 loss\n",
    "    total_loss = 0.5 * (mse_real + mse_imag)\n",
    "    \n",
    "    # Reduce on spatial information\n",
    "    batch_loss = tf.reduce_mean(total_loss, axis=(1, 2))\n",
    "    \n",
    "    # Normalize the loss function to be between 0 and 1\n",
    "    normalized_loss = tf.nn.l2_normalize(batch_loss, axis=-1)\n",
    "\n",
    "    normalized_reduced_loss = tf.reduce_mean(normalized_loss)\n",
    "\n",
    "    return normalized_reduced_loss\n",
    "\n",
    "def perceptual_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Custom loss function for perceptual loss.\n",
    "\n",
    "    Args:\n",
    "        y_true: Ground truth image(s).\n",
    "        y_pred: Predicted image(s).\n",
    "\n",
    "    Returns:\n",
    "        Normalized reduced perceptual loss.\n",
    "\n",
    "    This function defines a custom loss for training neural networks. It converts single-channel\n",
    "    images to RGB, preprocesses them for VGG16, and extracts features from a specified layer\n",
    "    using a pre-trained VGG16 model. It then calculates the mean squared error between the features\n",
    "    of the true and predicted images. The loss is normalized and reduced to a single scalar value.\n",
    "\n",
    "    \"\"\"\n",
    "    # Convert single-channel images to RGB\n",
    "    y_true_rgb = tf.repeat(y_true, 3, axis=-1)\n",
    "    y_pred_rgb = tf.repeat(y_pred, 3, axis=-1)\n",
    "\n",
    "    # Preprocess images for VGG16\n",
    "    y_true_processed = tf.keras.applications.vgg16.preprocess_input(y_true_rgb)\n",
    "    y_pred_processed = tf.keras.applications.vgg16.preprocess_input(y_pred_rgb)\n",
    "\n",
    "    # Extract features from specified layer for true and predicted images\n",
    "    features_true = perceptual_model(y_true_processed)\n",
    "    features_pred = perceptual_model(y_pred_processed)\n",
    "\n",
    "    # Calculate L2 loss\n",
    "    mse = tf.keras.losses.mean_squared_error(features_true, features_pred)\n",
    "\n",
    "    # Reduce on spatial information\n",
    "    batch_loss = tf.reduce_mean(mse, axis=(1, 2))\n",
    "    \n",
    "    # Normalize the loss function to be between 0 and 1\n",
    "    normalized_loss = tf.nn.l2_normalize(batch_loss, axis=-1)\n",
    "\n",
    "    normalized_reduced_loss = tf.reduce_mean(normalized_loss)\n",
    "\n",
    "    return normalized_reduced_loss\n",
    "\n",
    "def psnr(y_true, y_pred):\n",
    "    return tf.reduce_mean(-tf.image.psnr(y_true, y_pred, max_val=2.0))  # Adjust max_val for data normalized between -1 and 1\n",
    "\n",
    "def total_loss(y_true, y_pred):\n",
    "    return (1/3)*(perceptual_loss(y_true, y_pred)+ssim_loss(y_true, y_pred)+l2_loss(y_true, y_pred))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-02-13T03:56:21.452565Z",
     "iopub.execute_input": "2024-02-13T03:56:21.453155Z",
     "iopub.status.idle": "2024-02-13T03:56:21.809184Z",
     "shell.execute_reply.started": "2024-02-13T03:56:21.453127Z",
     "shell.execute_reply": "2024-02-13T03:56:21.808269Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "text": "VGG16 Model Initialized\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import CSVLogger, LearningRateScheduler, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "def save_model(path_weight, model, md='lstm'):\n",
    "    model_json = model.to_json()\n",
    "    with open(f\"{path_weight}model_{md}.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save(f\"{path_weight}model_{md}.h5\")\n",
    "    print(\"The model is successfully saved\")\n",
    "\n",
    "\n",
    "def load_model(path_weight, md='lstm', custom_objects=None):\n",
    "    json_file = open(f\"{path_weight}model_{md}.json\", 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json, custom_objects=custom_objects)\n",
    "    loaded_model.load_weights(f\"{path_weight}model_{md}.h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    return loaded_model\n",
    "\n",
    "\n",
    "def scheduler(epoch):\n",
    "    ep = 10\n",
    "    if epoch < ep:\n",
    "        return LEARNING_RATE\n",
    "    else:\n",
    "        return LEARNING_RATE * math.exp(0.1 * (ep - epoch))\n",
    "\n",
    "\n",
    "def main():\n",
    "    if TRAIN:\n",
    "        print('---------------------------------')\n",
    "        print('Model Training ...')\n",
    "        print('---------------------------------')\n",
    "        model = Correction_Multi_input(HEIGHT, WIDTH)\n",
    "        \n",
    "#         # Define the path to the model file\n",
    "#         model_path = \"/kaggle/working/stacked_model_fft_01_val_loss_0.2138.h5\"\n",
    "#         # Load the model with custom loss function\n",
    "#         model = tf.keras.models.load_model(model_path, custom_objects={'total_loss': total_loss,\n",
    "#                                                                       'ssim_score': ssim_score})\n",
    "\n",
    "        csv_logger = CSVLogger(f'{WEIGHTS_PATH}_Loss_Acc.csv', append=True, separator=',')\n",
    "        reduce_lr = LearningRateScheduler(scheduler)\n",
    "        model.compile(loss=total_loss, optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "                      metrics=[ssim_score, 'mse', psnr])\n",
    "        \n",
    "        checkpoint_path = '/kaggle/working/stacked_model_ssim_Perc_{epoch:02d}_val_loss_{val_loss:.4f}.h5'\n",
    "        model_checkpoint = ModelCheckpoint(checkpoint_path,\n",
    "                                   monitor='val_loss',\n",
    "                                   save_best_only=False,\n",
    "                                   save_weights_only=False,\n",
    "                                   mode='min',\n",
    "                                   verbose=1)\n",
    "        \n",
    "        hist = model.fit(train_dataset,\n",
    "                         epochs=NB_EPOCH,\n",
    "#                          steps_per_epoch=data_loader.size[0] // batch_size,\n",
    "                         verbose=1,\n",
    "                         validation_data=validation_dataset,\n",
    "#                          validation_steps=data_loader.size[2] // batch_size,\n",
    "                         callbacks=[csv_logger, reduce_lr, model_checkpoint])\n",
    "\n",
    "    if TEST:\n",
    "        print('========================================Load Model-s Weights=====================================')\n",
    "        custom_objects = {'ssim_loss': ssim_loss, 'ssim_score': ssim_score}\n",
    "        model = load_model(WEIGHTS_PATH, 'lstm', custom_objects=custom_objects)\n",
    "        print('---------------------------------')\n",
    "        print('Evaluate Model on Testing Set ...')\n",
    "        print('---------------------------------')\n",
    "        pred = model.predict(test_dataset,\n",
    "                             steps=data_loader.size[1] // batch_size)\n",
    "        print('==================================')\n",
    "        print('Predictions=', pred.shape)\n",
    "        print('==================================')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-02-13T03:56:21.811141Z",
     "iopub.execute_input": "2024-02-13T03:56:21.811509Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "---------------------------------\nModel Training ...\n---------------------------------\nEpoch 1/30\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1707796616.220910      98 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "     67/Unknown - 110s 849ms/step - loss: 0.2669 - ssim_score: 0.6497 - mse: 0.1909 - psnr: -17.3744",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
