{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8456774,"sourceType":"datasetVersion","datasetId":5040362},{"sourceId":8736315,"sourceType":"datasetVersion","datasetId":5239933}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","scrolled":true,"execution":{"iopub.status.busy":"2024-06-22T15:44:02.527348Z","iopub.execute_input":"2024-06-22T15:44:02.528144Z","iopub.status.idle":"2024-06-22T15:44:04.467888Z","shell.execute_reply.started":"2024-06-22T15:44:02.528107Z","shell.execute_reply":"2024-06-22T15:44:04.466592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = {\n    'image_save_iter': 10000,       # How often do you want to save output images during training\n    'image_display_iter': 100, \n    'epochs':20,# How often do you want to display output images during training\n    'display_size': 16,             # How many images do you want to display each time\n    'snapshot_save_iter': 24544,    # How often do you want to save trained models\n    'log_iter': 10,                 # How often do you want to log the training stats\n    'model':'UNIT',\n    # Optimization options\n    'max_iter': 2454400,            # Maximum number of training iterations\n    'batch_size': 1,                # Batch size\n    'weight_decay': 0.0001,         # Weight decay\n    'beta1': 0.5,                   # Adam parameter\n    'beta2': 0.999,                 # Adam parameter\n    'init': 'kaiming',              # Initialization [gaussian/kaiming/xavier/orthogonal]\n    'lr': 0.0001,                   # Initial learning rate\n    'lr_policy': 'step',            # Learning rate scheduler\n    'step_size': 100000,            # How often to decay learning rate\n    'gamma': 0.5,                   # How much to decay learning rate\n    'gan_w': 1,                     # Weight of adversarial loss\n    'recon_x_w': 10,                # Weight of image reconstruction loss\n    'recon_h_w': 0,                 # Weight of hidden reconstruction loss\n    'recon_kl_w': 0.01,             # Weight of KL loss for reconstruction\n    'recon_x_cyc_w': 10,            # Weight of cycle consistency loss\n    'recon_kl_cyc_w': 0.01,         # Weight of KL loss for cycle consistency\n    'vgg_w': 0,                     # Weight of domain-invariant perceptual loss\n    \n    # Model options\n    'gen': {\n        'dim': 64,                  # Number of filters in the bottommost layer\n        'activ': 'relu',            # Activation function [relu/lrelu/prelu/selu/tanh]\n        'n_downsample': 2,          # Number of downsampling layers in content encoder\n        'n_res': 4,                 # Number of residual blocks in content encoder/decoder\n        'pad_type': 'reflect'       # Padding type [zero/reflect]\n    },\n    'dis': {\n        'dim': 64,                  # Number of filters in the bottommost layer\n        'norm': 'none',             # Normalization layer [none/bn/in/ln]\n        'activ': 'lrelu',           # Activation function [relu/lrelu/prelu/selu/tanh]\n        'n_layer': 4,               # Number of layers in D\n        'gan_type': 'lsgan',        # GAN loss [lsgan/nsgan]\n        'num_scales': 3,            # Number of scales\n        'pad_type': 'reflect'       # Padding type [zero/reflect]\n    },\n    \n    # Data options\n    'input_dim_a': 1,                   # Number of image channels [1/3]\n    'input_dim_b': 1,                   # Number of image channels [1/3]\n    'num_workers': 8,                   # Number of data loading threads\n    'new_size': 256,                    # First resize the shortest image side to this size\n    'crop_image_height': 256,           # Random crop image of this height\n    'crop_image_width': 256,            # Random crop image of this width\n    'data_root': './datasets/edges2handbags/'  # Dataset folder location\n}\n","metadata":{"execution":{"iopub.status.busy":"2024-06-22T15:44:04.469707Z","iopub.execute_input":"2024-06-22T15:44:04.470106Z","iopub.status.idle":"2024-06-22T15:44:04.481200Z","shell.execute_reply.started":"2024-06-22T15:44:04.470079Z","shell.execute_reply":"2024-06-22T15:44:04.479785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport os\nimport subprocess\nimport nibabel as nib\nimport torch\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom skimage import exposure\nfrom math import ceil\nimport random\n\nclass MotionArtifactDataset(Dataset):\n    def __init__(self, data_path, subjects, batch_size):\n        self.data_path = data_path\n        self.subjects = subjects\n        self.batch_size = batch_size\n        self.samples = self.generate_samples()\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        return self.samples[idx]\n\n    def generate_samples(self):\n        samples = []\n        for subject in self.subjects:\n            for i in range(2):\n                pathes = self.get_nifti_path(subject, str(i + 1))\n                free, motion = self.get_paired_volumes(pathes)\n\n                if motion is not None:\n                    number_of_batches = int(motion.shape[0] // self.batch_size)\n\n                    for batch_idx in range(number_of_batches):\n                        start_idx = batch_idx * self.batch_size + 1\n                        end_idx = (batch_idx + 1) * self.batch_size + 1\n                        free_batch = free[start_idx:end_idx]\n                        motion_batch = motion[start_idx:end_idx]\n                        motion_before_batch = motion[batch_idx * self.batch_size:(batch_idx + 1) * self.batch_size]\n                        motion_after_batch = motion[batch_idx * self.batch_size + 2:(batch_idx + 1) * self.batch_size + 2]\n\n                        samples.append ((motion_batch, free_batch))\n        return samples\n                                \n#                 samples.append(((motion[:-2], motion[1:-1], motion[2:]), free))\n#                 print(samples)\n\n#         return samples\n\n    def get_nifti_path(self, subject, motion_severity):\n        ref_path_stand = f'{self.data_path}/{subject}/anat/{subject}_acq-standard_T1w.nii'\n#         select_path_stand = subprocess.run(['ls', ref_path_stand], capture_output=True, text=True).stdout.replace(\"\\n\", \"\")\n\n        ref_path_motion = f'{self.data_path}/{subject}/anat/{subject}_acq-headmotion{motion_severity}_T1w.nii'\n#         select_path_motion = subprocess.run(['ls', ref_path_motion], capture_output=True, text=True).stdout.replace(\"\\n\", \"\")\n\n        return [ref_path_stand, ref_path_motion]\n\n    def get_paired_volumes(self, path):\n        if os.path.exists(path[0]) and os.path.exists(path[1]):\n            free_data = nib.load(path[0]).get_fdata()\n            free_data = exposure.rescale_intensity(free_data[37:-37], out_range=(-1.0, 1.0))\n            motion_data = nib.load(path[1]).get_fdata()\n            motion_data = exposure.rescale_intensity(motion_data[37:-37], out_range=(-1.0, 1.0))\n            return torch.Tensor(free_data), torch.Tensor(motion_data)\n        else:\n            return None, None\n\n\nclass MotionArtifactDataLoader:\n    def __init__(self, data_path, split_ratio, batch_size):\n        self.data_path = data_path\n        self.split_ratio = split_ratio\n        self.all_subjects = None\n        self.subjects_lists = []\n        self.labels = {'train': 0, 'test': 1, 'validation': 2}\n        self.size = [0, 0, 0]\n        self.batch_size = batch_size\n\n        \n    def list_subjects(self):\n        subjects = os.listdir(self.data_path)\n        subjects = [item for item in subjects if item.startswith('sub')]\n        self.all_subjects = subjects\n\n    def split_data(self):\n        self.list_subjects()\n        if ceil(sum(self.split_ratio)) == 1 and len(self.split_ratio) <= 3:\n            self.split_ratio.insert(0, 0)\n            cumulative_sum = [sum(self.split_ratio[:i + 1]) for i in range(len(self.split_ratio))]\n            number_of_subjects = len(self.all_subjects)\n\n            for i in range(1, len(self.split_ratio)):\n                self.subjects_lists.append(\n                    self.all_subjects[int(round(cumulative_sum[i - 1] * number_of_subjects)):int(\n                        round(cumulative_sum[i] * number_of_subjects))])\n\n                self.size[i - 1] = len(self.subjects_lists[i - 1]) * 2 * 190\n\n                if i - 1 == 0:\n                    self.size[i - 1] -= 8 * 2 * 190\n        else:\n            print(\"The Summation of ratios is not equal to 1\")\n\n    def generator(self, mode):\n        subjects = self.subjects_lists[self.labels[mode]]\n\n        transform = transforms.Compose([\n            transforms.ToTensor(),\n        ])\n\n        dataset = MotionArtifactDataset(self.data_path, subjects, self.batch_size)\n        dataloader = DataLoader(dataset, shuffle=True)\n\n        return dataloader\n\n\n# Example usage:\ndata_path = \"/kaggle/input/mmmai-regist-data/MR-ART-Regist\"\nsplit_ratio = [0.7, 0.2, 0.1]\n# split_ratio = [0.01, 0.98, 0.01]\nbatch_size = config['batch_size']\n\ndata_loader = MotionArtifactDataLoader(data_path, split_ratio, batch_size)\ndata_loader.split_data()\n\ntrain_dataloader = data_loader.generator('train')\n# train_dataloader = torch.load(file_path)\n# test_dataloader = data_loader.generator('test')\nvalidation_dataloader = data_loader.generator('validation')","metadata":{"execution":{"iopub.status.busy":"2024-06-22T15:44:04.482972Z","iopub.execute_input":"2024-06-22T15:44:04.483392Z","iopub.status.idle":"2024-06-22T15:47:52.848008Z","shell.execute_reply.started":"2024-06-22T15:44:04.483359Z","shell.execute_reply":"2024-06-22T15:47:52.846921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nCopyright (C) 2018 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n\"\"\"\n# from torch.utils.serialization import load_lua\n# from torch.utils.data import DataLoader\n# from networks import Vgg16\nfrom torch.autograd import Variable\nfrom torch.optim import lr_scheduler\nfrom torchvision import transforms\n# from data import ImageFilelist, ImageFolder\nimport torch\nimport os\nimport math\nimport torchvision.utils as vutils\nimport yaml\nimport numpy as np\nimport torch.nn.init as init\nimport time\n# Methods\n# get_all_data_loaders      : primary data loader interface (load trainA, testA, trainB, testB)\n# get_data_loader_list      : list-based data loader\n# get_data_loader_folder    : folder-based data loader\n# get_config                : load yaml file\n# eformat                   :\n# write_2images             : save output image\n# prepare_sub_folder        : create checkpoints and images folders for saving outputs\n# write_one_row_html        : write one row of the html file for output images\n# write_html                : create the html file.\n# write_loss\n# slerp\n# get_slerp_interp\n# get_model_list\n# load_vgg16\n# vgg_preprocess\n# get_scheduler\n# weights_init\n\ndef get_all_data_loaders(conf):\n    batch_size = conf['batch_size']\n    num_workers = conf['num_workers']\n    if 'new_size' in conf:\n        new_size_a = new_size_b = conf['new_size']\n    else:\n        new_size_a = conf['new_size_a']\n        new_size_b = conf['new_size_b']\n    height = conf['crop_image_height']\n    width = conf['crop_image_width']\n\n    if 'data_root' in conf:\n        train_loader_a = get_data_loader_folder(os.path.join(conf['data_root'], 'trainA'), batch_size, True,\n                                              new_size_a, height, width, num_workers, True)\n        test_loader_a = get_data_loader_folder(os.path.join(conf['data_root'], 'testA'), batch_size, False,\n                                             new_size_a, new_size_a, new_size_a, num_workers, True)\n        train_loader_b = get_data_loader_folder(os.path.join(conf['data_root'], 'trainB'), batch_size, True,\n                                              new_size_b, height, width, num_workers, True)\n        test_loader_b = get_data_loader_folder(os.path.join(conf['data_root'], 'testB'), batch_size, False,\n                                             new_size_b, new_size_b, new_size_b, num_workers, True)\n    else:\n        train_loader_a = get_data_loader_list(conf['data_folder_train_a'], conf['data_list_train_a'], batch_size, True,\n                                                new_size_a, height, width, num_workers, True)\n        test_loader_a = get_data_loader_list(conf['data_folder_test_a'], conf['data_list_test_a'], batch_size, False,\n                                                new_size_a, new_size_a, new_size_a, num_workers, True)\n        train_loader_b = get_data_loader_list(conf['data_folder_train_b'], conf['data_list_train_b'], batch_size, True,\n                                                new_size_b, height, width, num_workers, True)\n        test_loader_b = get_data_loader_list(conf['data_folder_test_b'], conf['data_list_test_b'], batch_size, False,\n                                                new_size_b, new_size_b, new_size_b, num_workers, True)\n    return train_loader_a, train_loader_b, test_loader_a, test_loader_b\n\n\ndef get_data_loader_list(root, file_list, batch_size, train, new_size=None,\n                           height=256, width=256, num_workers=4, crop=True):\n    transform_list = [transforms.ToTensor(),\n                      transforms.Normalize((0.5, 0.5, 0.5),\n                                           (0.5, 0.5, 0.5))]\n    transform_list = [transforms.RandomCrop((height, width))] + transform_list if crop else transform_list\n    transform_list = [transforms.Resize(new_size)] + transform_list if new_size is not None else transform_list\n    transform_list = [transforms.RandomHorizontalFlip()] + transform_list if train else transform_list\n    transform = transforms.Compose(transform_list)\n    dataset = ImageFilelist(root, file_list, transform=transform)\n    loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=train, drop_last=True, num_workers=num_workers)\n    return loader\n\ndef get_data_loader_folder(input_folder, batch_size, train, new_size=None,\n                           height=256, width=256, num_workers=4, crop=True):\n    transform_list = [transforms.ToTensor(),\n                      transforms.Normalize((0.5, 0.5, 0.5),\n                                           (0.5, 0.5, 0.5))]\n    transform_list = [transforms.RandomCrop((height, width))] + transform_list if crop else transform_list\n    transform_list = [transforms.Resize(new_size)] + transform_list if new_size is not None else transform_list\n    transform_list = [transforms.RandomHorizontalFlip()] + transform_list if train else transform_list\n    transform = transforms.Compose(transform_list)\n    dataset = ImageFolder(input_folder, transform=transform)\n    loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=train, drop_last=True, num_workers=num_workers)\n    return loader\n\n\ndef get_config(config):\n    with open(config, 'r') as stream:\n        return yaml.load(stream)\n\n\ndef eformat(f, prec):\n    s = \"%.*e\"%(prec, f)\n    mantissa, exp = s.split('e')\n    # add 1 to digits as 1 is taken by sign +/-\n    return \"%se%d\"%(mantissa, int(exp))\n\n\ndef __write_images(image_outputs, display_image_num, file_name):\n    image_outputs = [images.expand(-1, 3, -1, -1) for images in image_outputs] # expand gray-scale images to 3 channels\n    image_tensor = torch.cat([images[:display_image_num] for images in image_outputs], 0)\n    image_grid = vutils.make_grid(image_tensor.data, nrow=display_image_num, padding=0, normalize=True)\n    vutils.save_image(image_grid, file_name, nrow=1)\n\n\ndef write_2images(image_outputs, display_image_num, image_directory, postfix):\n    n = len(image_outputs)\n    __write_images(image_outputs[0:n//2], display_image_num, '%s/gen_a2b_%s.jpg' % (image_directory, postfix))\n    __write_images(image_outputs[n//2:n], display_image_num, '%s/gen_b2a_%s.jpg' % (image_directory, postfix))\n\n\ndef prepare_sub_folder(output_directory):\n    image_directory = os.path.join(output_directory, 'images')\n    if not os.path.exists(image_directory):\n        print(\"Creating directory: {}\".format(image_directory))\n        os.makedirs(image_directory)\n    checkpoint_directory = os.path.join(output_directory, 'checkpoints')\n    if not os.path.exists(checkpoint_directory):\n        print(\"Creating directory: {}\".format(checkpoint_directory))\n        os.makedirs(checkpoint_directory)\n    return checkpoint_directory, image_directory\n\n\ndef write_one_row_html(html_file, iterations, img_filename, all_size):\n    html_file.write(\"<h3>iteration [%d] (%s)</h3>\" % (iterations,img_filename.split('/')[-1]))\n    html_file.write(\"\"\"\n        <p><a href=\"%s\">\n          <img src=\"%s\" style=\"width:%dpx\">\n        </a><br>\n        <p>\n        \"\"\" % (img_filename, img_filename, all_size))\n    return\n\n\ndef write_html(filename, iterations, image_save_iterations, image_directory, all_size=1536):\n    html_file = open(filename, \"w\")\n    html_file.write('''\n    <!DOCTYPE html>\n    <html>\n    <head>\n      <title>Experiment name = %s</title>\n      <meta http-equiv=\"refresh\" content=\"30\">\n    </head>\n    <body>\n    ''' % os.path.basename(filename))\n    html_file.write(\"<h3>current</h3>\")\n    write_one_row_html(html_file, iterations, '%s/gen_a2b_train_current.jpg' % (image_directory), all_size)\n    write_one_row_html(html_file, iterations, '%s/gen_b2a_train_current.jpg' % (image_directory), all_size)\n    for j in range(iterations, image_save_iterations-1, -1):\n        if j % image_save_iterations == 0:\n            write_one_row_html(html_file, j, '%s/gen_a2b_test_%08d.jpg' % (image_directory, j), all_size)\n            write_one_row_html(html_file, j, '%s/gen_b2a_test_%08d.jpg' % (image_directory, j), all_size)\n            write_one_row_html(html_file, j, '%s/gen_a2b_train_%08d.jpg' % (image_directory, j), all_size)\n            write_one_row_html(html_file, j, '%s/gen_b2a_train_%08d.jpg' % (image_directory, j), all_size)\n    html_file.write(\"</body></html>\")\n    html_file.close()\n\n\ndef write_loss(iterations, trainer, train_writer):\n    members = [attr for attr in dir(trainer) \\\n               if not callable(getattr(trainer, attr)) and not attr.startswith(\"__\") and ('loss' in attr or 'grad' in attr or 'nwd' in attr)]\n    for m in members:\n        train_writer.add_scalar(m, getattr(trainer, m), iterations + 1)\n\n\ndef slerp(val, low, high):\n    \"\"\"\n    original: Animating Rotation with Quaternion Curves, Ken Shoemake\n    https://arxiv.org/abs/1609.04468\n    Code: https://github.com/soumith/dcgan.torch/issues/14, Tom White\n    \"\"\"\n    omega = np.arccos(np.dot(low / np.linalg.norm(low), high / np.linalg.norm(high)))\n    so = np.sin(omega)\n    return np.sin((1.0 - val) * omega) / so * low + np.sin(val * omega) / so * high\n\n\ndef get_slerp_interp(nb_latents, nb_interp, z_dim):\n    \"\"\"\n    modified from: PyTorch inference for \"Progressive Growing of GANs\" with CelebA snapshot\n    https://github.com/ptrblck/prog_gans_pytorch_inference\n    \"\"\"\n\n    latent_interps = np.empty(shape=(0, z_dim), dtype=np.float32)\n    for _ in range(nb_latents):\n        low = np.random.randn(z_dim)\n        high = np.random.randn(z_dim)  # low + np.random.randn(512) * 0.7\n        interp_vals = np.linspace(0, 1, num=nb_interp)\n        latent_interp = np.array([slerp(v, low, high) for v in interp_vals],\n                                 dtype=np.float32)\n        latent_interps = np.vstack((latent_interps, latent_interp))\n\n    return latent_interps[:, :, np.newaxis, np.newaxis]\n\n\n# Get model list for resume\ndef get_model_list(dirname, key):\n    if os.path.exists(dirname) is False:\n        return None\n    gen_models = [os.path.join(dirname, f) for f in os.listdir(dirname) if\n                  os.path.isfile(os.path.join(dirname, f)) and key in f and \".pt\" in f]\n    if gen_models is None:\n        return None\n    gen_models.sort()\n    last_model_name = gen_models[-1]\n    return last_model_name\n\n\ndef load_vgg16(model_dir):\n    \"\"\" Use the model from https://github.com/abhiskk/fast-neural-style/blob/master/neural_style/utils.py \"\"\"\n    if not os.path.exists(model_dir):\n        os.mkdir(model_dir)\n    if not os.path.exists(os.path.join(model_dir, 'vgg16.weight')):\n        if not os.path.exists(os.path.join(model_dir, 'vgg16.t7')):\n            os.system('wget https://www.dropbox.com/s/76l3rt4kyi3s8x7/vgg16.t7?dl=1 -O ' + os.path.join(model_dir, 'vgg16.t7'))\n        vgglua = load_lua(os.path.join(model_dir, 'vgg16.t7'))\n        vgg = Vgg16()\n        for (src, dst) in zip(vgglua.parameters()[0], vgg.parameters()):\n            dst.data[:] = src\n        torch.save(vgg.state_dict(), os.path.join(model_dir, 'vgg16.weight'))\n    vgg = Vgg16()\n    vgg.load_state_dict(torch.load(os.path.join(model_dir, 'vgg16.weight')))\n    return vgg\n\n\ndef vgg_preprocess(batch):\n    tensortype = type(batch.data)\n    (r, g, b) = torch.chunk(batch, 3, dim = 1)\n    batch = torch.cat((b, g, r), dim = 1) # convert RGB to BGR\n    batch = (batch + 1) * 255 * 0.5 # [-1, 1] -> [0, 255]\n    mean = tensortype(batch.data.size()).cuda()\n    mean[:, 0, :, :] = 103.939\n    mean[:, 1, :, :] = 116.779\n    mean[:, 2, :, :] = 123.680\n    batch = batch.sub(Variable(mean)) # subtract mean\n    return batch\n\n\ndef get_scheduler(optimizer, hyperparameters, iterations=-1):\n    if 'lr_policy' not in hyperparameters or hyperparameters['lr_policy'] == 'constant':\n        scheduler = None # constant scheduler\n    elif hyperparameters['lr_policy'] == 'step':\n        scheduler = lr_scheduler.StepLR(optimizer, step_size=hyperparameters['step_size'],\n                                        gamma=hyperparameters['gamma'], last_epoch=iterations)\n    else:\n        return NotImplementedError('learning rate policy [%s] is not implemented', hyperparameters['lr_policy'])\n    return scheduler\n\n\ndef weights_init(init_type='gaussian'):\n    def init_fun(m):\n        classname = m.__class__.__name__\n        if (classname.find('Conv') == 0 or classname.find('Linear') == 0) and hasattr(m, 'weight'):\n            # print m.__class__.__name__\n            if init_type == 'gaussian':\n                init.normal_(m.weight.data, 0.0, 0.02)\n            elif init_type == 'xavier':\n                init.xavier_normal_(m.weight.data, gain=math.sqrt(2))\n            elif init_type == 'kaiming':\n                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n            elif init_type == 'orthogonal':\n                init.orthogonal_(m.weight.data, gain=math.sqrt(2))\n            elif init_type == 'default':\n                pass\n            else:\n                assert 0, \"Unsupported initialization: {}\".format(init_type)\n            if hasattr(m, 'bias') and m.bias is not None:\n                init.constant_(m.bias.data, 0.0)\n\n    return init_fun\n\n\nclass Timer:\n    def __init__(self, msg):\n        self.msg = msg\n        self.start_time = None\n\n    def __enter__(self):\n        self.start_time = time.time()\n\n    def __exit__(self, exc_type, exc_value, exc_tb):\n        print(self.msg % (time.time() - self.start_time))\n\n\ndef pytorch03_to_pytorch04(state_dict_base):\n    def __conversion_core(state_dict_base):\n        state_dict = state_dict_base.copy()\n        for key, value in state_dict_base.items():\n            if key.endswith(('enc.model.0.norm.running_mean',\n                             'enc.model.0.norm.running_var',\n                             'enc.model.1.norm.running_mean',\n                             'enc.model.1.norm.running_var',\n                             'enc.model.2.norm.running_mean',\n                             'enc.model.2.norm.running_var',\n                             'enc.model.3.model.0.model.1.norm.running_mean',\n                             'enc.model.3.model.0.model.1.norm.running_var',\n                             'enc.model.3.model.0.model.0.norm.running_mean',\n                             'enc.model.3.model.0.model.0.norm.running_var',\n                             'enc.model.3.model.1.model.1.norm.running_mean',\n                             'enc.model.3.model.1.model.1.norm.running_var',\n                             'enc.model.3.model.1.model.0.norm.running_mean',\n                             'enc.model.3.model.1.model.0.norm.running_var',\n                             'enc.model.3.model.2.model.1.norm.running_mean',\n                             'enc.model.3.model.2.model.1.norm.running_var',\n                             'enc.model.3.model.2.model.0.norm.running_mean',\n                             'enc.model.3.model.2.model.0.norm.running_var',\n                             'enc.model.3.model.3.model.1.norm.running_mean',\n                             'enc.model.3.model.3.model.1.norm.running_var',\n                             'enc.model.3.model.3.model.0.norm.running_mean',\n                             'enc.model.3.model.3.model.0.norm.running_var',\n                             'dec.model.0.model.0.model.1.norm.running_mean',\n                             'dec.model.0.model.0.model.1.norm.running_var',\n                             'dec.model.0.model.0.model.0.norm.running_mean',\n                             'dec.model.0.model.0.model.0.norm.running_var',\n                             'dec.model.0.model.1.model.1.norm.running_mean',\n                             'dec.model.0.model.1.model.1.norm.running_var',\n                             'dec.model.0.model.1.model.0.norm.running_mean',\n                             'dec.model.0.model.1.model.0.norm.running_var',\n                             'dec.model.0.model.2.model.1.norm.running_mean',\n                             'dec.model.0.model.2.model.1.norm.running_var',\n                             'dec.model.0.model.2.model.0.norm.running_mean',\n                             'dec.model.0.model.2.model.0.norm.running_var',\n                             'dec.model.0.model.3.model.1.norm.running_mean',\n                             'dec.model.0.model.3.model.1.norm.running_var',\n                             'dec.model.0.model.3.model.0.norm.running_mean',\n                             'dec.model.0.model.3.model.0.norm.running_var',\n                             )):\n                del state_dict[key]\n        return state_dict\n    state_dict = dict()\n    state_dict['a'] = __conversion_core(state_dict_base['a'])\n    state_dict['b'] = __conversion_core(state_dict_base['b'])\n    return state_dict","metadata":{"execution":{"iopub.status.busy":"2024-06-22T15:47:52.852481Z","iopub.execute_input":"2024-06-22T15:47:52.852958Z","iopub.status.idle":"2024-06-22T15:47:52.915733Z","shell.execute_reply.started":"2024-06-22T15:47:52.852931Z","shell.execute_reply":"2024-06-22T15:47:52.914670Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nCopyright (C) 2018 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n\"\"\"\nfrom torch import nn\nfrom torch.autograd import Variable\nimport torch\nimport torch.nn.functional as F\ntry:\n    from itertools import izip as zip\nexcept ImportError: # will be 3.x series\n    pass\n\n##################################################################################\n# Discriminator\n##################################################################################\n\nclass MsImageDis(nn.Module):\n    # Multi-scale discriminator architecture\n    def __init__(self, input_dim, params):\n        super(MsImageDis, self).__init__()\n        self.n_layer = params['n_layer']\n        self.gan_type = params['gan_type']\n        self.dim = params['dim']\n        self.norm = params['norm']\n        self.activ = params['activ']\n        self.num_scales = params['num_scales']\n        self.pad_type = params['pad_type']\n        self.input_dim = input_dim\n        self.downsample = nn.AvgPool2d(3, stride=2, padding=[1, 1], count_include_pad=False)\n        self.cnns = nn.ModuleList()\n        for _ in range(self.num_scales):\n            self.cnns.append(self._make_net())\n\n    def _make_net(self):\n        dim = self.dim\n        cnn_x = []\n        cnn_x += [Conv2dBlock(self.input_dim, dim, 4, 2, 1, norm='none', activation=self.activ, pad_type=self.pad_type)]\n        for i in range(self.n_layer - 1):\n            cnn_x += [Conv2dBlock(dim, dim * 2, 4, 2, 1, norm=self.norm, activation=self.activ, pad_type=self.pad_type)]\n            dim *= 2\n        cnn_x += [nn.Conv2d(dim, 1, 1, 1, 0)]\n        cnn_x = nn.Sequential(*cnn_x)\n        return cnn_x\n\n    def forward(self, x):\n        outputs = []\n        for model in self.cnns:\n            outputs.append(model(x))\n            x = self.downsample(x)\n        return outputs\n\n    def calc_dis_loss(self, input_fake, input_real):\n        # calculate the loss to train D\n        outs0 = self.forward(input_fake)\n        outs1 = self.forward(input_real)\n        loss = 0\n\n        for it, (out0, out1) in enumerate(zip(outs0, outs1)):\n            if self.gan_type == 'lsgan':\n                loss += torch.mean((out0 - 0)**2) + torch.mean((out1 - 1)**2)\n            elif self.gan_type == 'nsgan':\n                all0 = Variable(torch.zeros_like(out0.data).cuda(), requires_grad=False)\n                all1 = Variable(torch.ones_like(out1.data).cuda(), requires_grad=False)\n                loss += torch.mean(F.binary_cross_entropy(F.sigmoid(out0), all0) +\n                                   F.binary_cross_entropy(F.sigmoid(out1), all1))\n            else:\n                assert 0, \"Unsupported GAN type: {}\".format(self.gan_type)\n        return loss\n\n    def calc_gen_loss(self, input_fake):\n        # calculate the loss to train G\n        outs0 = self.forward(input_fake)\n        loss = 0\n        for it, (out0) in enumerate(outs0):\n            if self.gan_type == 'lsgan':\n                loss += torch.mean((out0 - 1)**2) # LSGAN\n            elif self.gan_type == 'nsgan':\n                all1 = Variable(torch.ones_like(out0.data).cuda(), requires_grad=False)\n                loss += torch.mean(F.binary_cross_entropy(F.sigmoid(out0), all1))\n            else:\n                assert 0, \"Unsupported GAN type: {}\".format(self.gan_type)\n        return loss\n\n##################################################################################\n# Generator\n##################################################################################\n\nclass AdaINGen(nn.Module):\n    # AdaIN auto-encoder architecture\n    def __init__(self, input_dim, params):\n        super(AdaINGen, self).__init__()\n        dim = params['dim']\n        style_dim = params['style_dim']\n        n_downsample = params['n_downsample']\n        n_res = params['n_res']\n        activ = params['activ']\n        pad_type = params['pad_type']\n        mlp_dim = params['mlp_dim']\n\n        # style encoder\n        self.enc_style = StyleEncoder(4, input_dim, dim, style_dim, norm='none', activ=activ, pad_type=pad_type)\n\n        # content encoder\n        self.enc_content = ContentEncoder(n_downsample, n_res, input_dim, dim, 'in', activ, pad_type=pad_type)\n        self.dec = Decoder(n_downsample, n_res, self.enc_content.output_dim, input_dim, res_norm='adain', activ=activ, pad_type=pad_type)\n\n        # MLP to generate AdaIN parameters\n        self.mlp = MLP(style_dim, self.get_num_adain_params(self.dec), mlp_dim, 3, norm='none', activ=activ)\n\n    def forward(self, images):\n        # reconstruct an image\n        content, style_fake = self.encode(images)\n        images_recon = self.decode(content, style_fake)\n        return images_recon\n\n    def encode(self, images):\n        # encode an image to its content and style codes\n        style_fake = self.enc_style(images)\n        content = self.enc_content(images)\n        return content, style_fake\n\n    def decode(self, content, style):\n        # decode content and style codes to an image\n        adain_params = self.mlp(style)\n        self.assign_adain_params(adain_params, self.dec)\n        images = self.dec(content)\n        return images\n\n    def assign_adain_params(self, adain_params, model):\n        # assign the adain_params to the AdaIN layers in model\n        for m in model.modules():\n            if m.__class__.__name__ == \"AdaptiveInstanceNorm2d\":\n                mean = adain_params[:, :m.num_features]\n                std = adain_params[:, m.num_features:2*m.num_features]\n                m.bias = mean.contiguous().view(-1)\n                m.weight = std.contiguous().view(-1)\n                if adain_params.size(1) > 2*m.num_features:\n                    adain_params = adain_params[:, 2*m.num_features:]\n\n    def get_num_adain_params(self, model):\n        # return the number of AdaIN parameters needed by the model\n        num_adain_params = 0\n        for m in model.modules():\n            if m.__class__.__name__ == \"AdaptiveInstanceNorm2d\":\n                num_adain_params += 2*m.num_features\n        return num_adain_params\n\n\nclass VAEGen(nn.Module):\n    # VAE architecture\n    def __init__(self, input_dim, params):\n        super(VAEGen, self).__init__()\n        dim = params['dim']\n        n_downsample = params['n_downsample']\n        n_res = params['n_res']\n        activ = params['activ']\n        pad_type = params['pad_type']\n\n        # content encoder\n        self.enc = ContentEncoder(n_downsample, n_res, input_dim, dim, 'in', activ, pad_type=pad_type)\n        self.dec = Decoder(n_downsample, n_res, self.enc.output_dim, input_dim, res_norm='in', activ=activ, pad_type=pad_type)\n\n    def forward(self, images):\n        # This is a reduced VAE implementation where we assume the outputs are multivariate Gaussian distribution with mean = hiddens and std_dev = all ones.\n        hiddens = self.encode(images)\n        if self.training == True:\n            noise = Variable(torch.randn(hiddens.size()).cuda(hiddens.data.get_device()))\n            images_recon = self.decode(hiddens + noise)\n        else:\n            images_recon = self.decode(hiddens)\n        return images_recon, hiddens\n\n    def encode(self, images):\n        hiddens = self.enc(images)\n        noise = Variable(torch.randn(hiddens.size()).cuda(hiddens.data.get_device()))\n        return hiddens, noise\n\n    def decode(self, hiddens):\n        images = self.dec(hiddens)\n        return images\n\n\n##################################################################################\n# Encoder and Decoders\n##################################################################################\n\nclass StyleEncoder(nn.Module):\n    def __init__(self, n_downsample, input_dim, dim, style_dim, norm, activ, pad_type):\n        super(StyleEncoder, self).__init__()\n        self.model = []\n        self.model += [Conv2dBlock(input_dim, dim, 7, 1, 3, norm=norm, activation=activ, pad_type=pad_type)]\n        for i in range(2):\n            self.model += [Conv2dBlock(dim, 2 * dim, 4, 2, 1, norm=norm, activation=activ, pad_type=pad_type)]\n            dim *= 2\n        for i in range(n_downsample - 2):\n            self.model += [Conv2dBlock(dim, dim, 4, 2, 1, norm=norm, activation=activ, pad_type=pad_type)]\n        self.model += [nn.AdaptiveAvgPool2d(1)] # global average pooling\n        self.model += [nn.Conv2d(dim, style_dim, 1, 1, 0)]\n        self.model = nn.Sequential(*self.model)\n        self.output_dim = dim\n\n    def forward(self, x):\n        return self.model(x)\n\nclass ContentEncoder(nn.Module):\n    def __init__(self, n_downsample, n_res, input_dim, dim, norm, activ, pad_type):\n        super(ContentEncoder, self).__init__()\n        self.model = []\n        self.model += [Conv2dBlock(input_dim, dim, 7, 1, 3, norm=norm, activation=activ, pad_type=pad_type)]\n        # downsampling blocks\n        for i in range(n_downsample):\n            self.model += [Conv2dBlock(dim, 2 * dim, 4, 2, 1, norm=norm, activation=activ, pad_type=pad_type)]\n            dim *= 2\n        # residual blocks\n        self.model += [ResBlocks(n_res, dim, norm=norm, activation=activ, pad_type=pad_type)]\n        self.model = nn.Sequential(*self.model)\n        self.output_dim = dim\n\n    def forward(self, x):\n        return self.model(x)\n\nclass Decoder(nn.Module):\n    def __init__(self, n_upsample, n_res, dim, output_dim, res_norm='adain', activ='relu', pad_type='zero'):\n        super(Decoder, self).__init__()\n\n        self.model = []\n        # AdaIN residual blocks\n        self.model += [ResBlocks(n_res, dim, res_norm, activ, pad_type=pad_type)]\n        # upsampling blocks\n        for i in range(n_upsample):\n            self.model += [nn.Upsample(scale_factor=2),\n                           Conv2dBlock(dim, dim // 2, 5, 1, 2, norm='ln', activation=activ, pad_type=pad_type)]\n            dim //= 2\n        # use reflection padding in the last conv layer\n        self.model += [Conv2dBlock(dim, output_dim, 7, 1, 3, norm='none', activation='tanh', pad_type=pad_type)]\n        self.model = nn.Sequential(*self.model)\n\n    def forward(self, x):\n        return self.model(x)\n\n##################################################################################\n# Sequential Models\n##################################################################################\nclass ResBlocks(nn.Module):\n    def __init__(self, num_blocks, dim, norm='in', activation='relu', pad_type='zero'):\n        super(ResBlocks, self).__init__()\n        self.model = []\n        for i in range(num_blocks):\n            self.model += [ResBlock(dim, norm=norm, activation=activation, pad_type=pad_type)]\n        self.model = nn.Sequential(*self.model)\n\n    def forward(self, x):\n        return self.model(x)\n\nclass MLP(nn.Module):\n    def __init__(self, input_dim, output_dim, dim, n_blk, norm='none', activ='relu'):\n\n        super(MLP, self).__init__()\n        self.model = []\n        self.model += [LinearBlock(input_dim, dim, norm=norm, activation=activ)]\n        for i in range(n_blk - 2):\n            self.model += [LinearBlock(dim, dim, norm=norm, activation=activ)]\n        self.model += [LinearBlock(dim, output_dim, norm='none', activation='none')] # no output activations\n        self.model = nn.Sequential(*self.model)\n\n    def forward(self, x):\n        return self.model(x.view(x.size(0), -1))\n\n##################################################################################\n# Basic Blocks\n##################################################################################\nclass ResBlock(nn.Module):\n    def __init__(self, dim, norm='in', activation='relu', pad_type='zero'):\n        super(ResBlock, self).__init__()\n\n        model = []\n        model += [Conv2dBlock(dim ,dim, 3, 1, 1, norm=norm, activation=activation, pad_type=pad_type)]\n        model += [Conv2dBlock(dim ,dim, 3, 1, 1, norm=norm, activation='none', pad_type=pad_type)]\n        self.model = nn.Sequential(*model)\n\n    def forward(self, x):\n        residual = x\n        out = self.model(x)\n        out += residual\n        return out\n\nclass Conv2dBlock(nn.Module):\n    def __init__(self, input_dim ,output_dim, kernel_size, stride,\n                 padding=0, norm='none', activation='relu', pad_type='zero'):\n        super(Conv2dBlock, self).__init__()\n        self.use_bias = True\n        # initialize padding\n        if pad_type == 'reflect':\n            self.pad = nn.ReflectionPad2d(padding)\n        elif pad_type == 'replicate':\n            self.pad = nn.ReplicationPad2d(padding)\n        elif pad_type == 'zero':\n            self.pad = nn.ZeroPad2d(padding)\n        else:\n            assert 0, \"Unsupported padding type: {}\".format(pad_type)\n\n        # initialize normalization\n        norm_dim = output_dim\n        if norm == 'bn':\n            self.norm = nn.BatchNorm2d(norm_dim)\n        elif norm == 'in':\n            #self.norm = nn.InstanceNorm2d(norm_dim, track_running_stats=True)\n            self.norm = nn.InstanceNorm2d(norm_dim)\n        elif norm == 'ln':\n            self.norm = LayerNorm(norm_dim)\n        elif norm == 'adain':\n            self.norm = AdaptiveInstanceNorm2d(norm_dim)\n        elif norm == 'none':\n            self.norm = None\n        else:\n            assert 0, \"Unsupported normalization: {}\".format(norm)\n\n        # initialize activation\n        if activation == 'relu':\n            self.activation = nn.ReLU(inplace=True)\n        elif activation == 'lrelu':\n            self.activation = nn.LeakyReLU(0.2, inplace=True)\n        elif activation == 'prelu':\n            self.activation = nn.PReLU()\n        elif activation == 'selu':\n            self.activation = nn.SELU(inplace=True)\n        elif activation == 'tanh':\n            self.activation = nn.Tanh()\n        elif activation == 'none':\n            self.activation = None\n        else:\n            assert 0, \"Unsupported activation: {}\".format(activation)\n\n        # initialize convolution\n        self.conv = nn.Conv2d(input_dim, output_dim, kernel_size, stride, bias=self.use_bias)\n\n    def forward(self, x):\n        x = self.conv(self.pad(x))\n        if self.norm:\n            x = self.norm(x)\n        if self.activation:\n            x = self.activation(x)\n        return x\n\nclass LinearBlock(nn.Module):\n    def __init__(self, input_dim, output_dim, norm='none', activation='relu'):\n        super(LinearBlock, self).__init__()\n        use_bias = True\n        # initialize fully connected layer\n        self.fc = nn.Linear(input_dim, output_dim, bias=use_bias)\n\n        # initialize normalization\n        norm_dim = output_dim\n        if norm == 'bn':\n            self.norm = nn.BatchNorm1d(norm_dim)\n        elif norm == 'in':\n            self.norm = nn.InstanceNorm1d(norm_dim)\n        elif norm == 'ln':\n            self.norm = LayerNorm(norm_dim)\n        elif norm == 'none':\n            self.norm = None\n        else:\n            assert 0, \"Unsupported normalization: {}\".format(norm)\n\n        # initialize activation\n        if activation == 'relu':\n            self.activation = nn.ReLU(inplace=True)\n        elif activation == 'lrelu':\n            self.activation = nn.LeakyReLU(0.2, inplace=True)\n        elif activation == 'prelu':\n            self.activation = nn.PReLU()\n        elif activation == 'selu':\n            self.activation = nn.SELU(inplace=True)\n        elif activation == 'tanh':\n            self.activation = nn.Tanh()\n        elif activation == 'none':\n            self.activation = None\n        else:\n            assert 0, \"Unsupported activation: {}\".format(activation)\n\n    def forward(self, x):\n        out = self.fc(x)\n        if self.norm:\n            out = self.norm(out)\n        if self.activation:\n            out = self.activation(out)\n        return out\n\n##################################################################################\n# VGG network definition\n##################################################################################\nclass Vgg16(nn.Module):\n    def __init__(self):\n        super(Vgg16, self).__init__()\n        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n\n        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n\n        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n\n        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n\n        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n\n    def forward(self, X):\n        h = F.relu(self.conv1_1(X), inplace=True)\n        h = F.relu(self.conv1_2(h), inplace=True)\n        # relu1_2 = h\n        h = F.max_pool2d(h, kernel_size=2, stride=2)\n\n        h = F.relu(self.conv2_1(h), inplace=True)\n        h = F.relu(self.conv2_2(h), inplace=True)\n        # relu2_2 = h\n        h = F.max_pool2d(h, kernel_size=2, stride=2)\n\n        h = F.relu(self.conv3_1(h), inplace=True)\n        h = F.relu(self.conv3_2(h), inplace=True)\n        h = F.relu(self.conv3_3(h), inplace=True)\n        # relu3_3 = h\n        h = F.max_pool2d(h, kernel_size=2, stride=2)\n\n        h = F.relu(self.conv4_1(h), inplace=True)\n        h = F.relu(self.conv4_2(h), inplace=True)\n        h = F.relu(self.conv4_3(h), inplace=True)\n        # relu4_3 = h\n\n        h = F.relu(self.conv5_1(h), inplace=True)\n        h = F.relu(self.conv5_2(h), inplace=True)\n        h = F.relu(self.conv5_3(h), inplace=True)\n        relu5_3 = h\n\n        return relu5_3\n        # return [relu1_2, relu2_2, relu3_3, relu4_3]\n\n##################################################################################\n# Normalization layers\n##################################################################################\nclass AdaptiveInstanceNorm2d(nn.Module):\n    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n        super(AdaptiveInstanceNorm2d, self).__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.momentum = momentum\n        # weight and bias are dynamically assigned\n        self.weight = None\n        self.bias = None\n        # just dummy buffers, not used\n        self.register_buffer('running_mean', torch.zeros(num_features))\n        self.register_buffer('running_var', torch.ones(num_features))\n\n    def forward(self, x):\n        assert self.weight is not None and self.bias is not None, \"Please assign weight and bias before calling AdaIN!\"\n        b, c = x.size(0), x.size(1)\n        running_mean = self.running_mean.repeat(b)\n        running_var = self.running_var.repeat(b)\n\n        # Apply instance norm\n        x_reshaped = x.contiguous().view(1, b * c, *x.size()[2:])\n\n        out = F.batch_norm(\n            x_reshaped, running_mean, running_var, self.weight, self.bias,\n            True, self.momentum, self.eps)\n\n        return out.view(b, c, *x.size()[2:])\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(' + str(self.num_features) + ')'\n\n\nclass LayerNorm(nn.Module):\n    def __init__(self, num_features, eps=1e-5, affine=True):\n        super(LayerNorm, self).__init__()\n        self.num_features = num_features\n        self.affine = affine\n        self.eps = eps\n\n        if self.affine:\n            self.gamma = nn.Parameter(torch.Tensor(num_features).uniform_())\n            self.beta = nn.Parameter(torch.zeros(num_features))\n\n    def forward(self, x):\n        shape = [-1] + [1] * (x.dim() - 1)\n        # print(x.size())\n        if x.size(0) == 1:\n            # These two lines run much faster in pytorch 0.4 than the two lines listed below.\n            mean = x.view(-1).mean().view(*shape)\n            std = x.view(-1).std().view(*shape)\n        else:\n            mean = x.view(x.size(0), -1).mean(1).view(*shape)\n            std = x.view(x.size(0), -1).std(1).view(*shape)\n\n        x = (x - mean) / (std + self.eps)\n\n        if self.affine:\n            shape = [1, -1] + [1] * (x.dim() - 2)\n            x = x * self.gamma.view(*shape) + self.beta.view(*shape)\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2024-06-22T15:47:52.917107Z","iopub.execute_input":"2024-06-22T15:47:52.917452Z","iopub.status.idle":"2024-06-22T15:47:53.014301Z","shell.execute_reply.started":"2024-06-22T15:47:52.917407Z","shell.execute_reply":"2024-06-22T15:47:53.013351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nCopyright (C) 2017 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n\"\"\"\n# from networks import AdaINGen, MsImageDis, VAEGen\n# from utils import weights_init, get_model_list, vgg_preprocess, load_vgg16, get_scheduler\nfrom torch.autograd import Variable\nimport torch\nimport torch.nn as nn\nimport os\n\nclass MUNIT_Trainer(nn.Module):\n    def __init__(self, hyperparameters):\n        super(MUNIT_Trainer, self).__init__()\n        lr = hyperparameters['lr']\n        # Initiate the networks\n        self.gen_a = AdaINGen(hyperparameters['input_dim_a'], hyperparameters['gen'])  # auto-encoder for domain a\n        self.gen_b = AdaINGen(hyperparameters['input_dim_b'], hyperparameters['gen'])  # auto-encoder for domain b\n        self.dis_a = MsImageDis(hyperparameters['input_dim_a'], hyperparameters['dis'])  # discriminator for domain a\n        self.dis_b = MsImageDis(hyperparameters['input_dim_b'], hyperparameters['dis'])  # discriminator for domain b\n        self.instancenorm = nn.InstanceNorm2d(512, affine=False)\n        self.style_dim = hyperparameters['gen']['style_dim']\n\n        # fix the noise used in sampling\n        display_size = int(hyperparameters['display_size'])\n        self.s_a = torch.randn(display_size, self.style_dim, 1, 1).cuda()\n        self.s_b = torch.randn(display_size, self.style_dim, 1, 1).cuda()\n\n        # Setup the optimizers\n        beta1 = hyperparameters['beta1']\n        beta2 = hyperparameters['beta2']\n        dis_params = list(self.dis_a.parameters()) + list(self.dis_b.parameters())\n        gen_params = list(self.gen_a.parameters()) + list(self.gen_b.parameters())\n        self.dis_opt = torch.optim.Adam([p for p in dis_params if p.requires_grad],\n                                        lr=lr, betas=(beta1, beta2), weight_decay=hyperparameters['weight_decay'])\n        self.gen_opt = torch.optim.Adam([p for p in gen_params if p.requires_grad],\n                                        lr=lr, betas=(beta1, beta2), weight_decay=hyperparameters['weight_decay'])\n        self.dis_scheduler = get_scheduler(self.dis_opt, hyperparameters)\n        self.gen_scheduler = get_scheduler(self.gen_opt, hyperparameters)\n\n        # Network weight initialization\n        self.apply(weights_init(hyperparameters['init']))\n        self.dis_a.apply(weights_init('gaussian'))\n        self.dis_b.apply(weights_init('gaussian'))\n\n        # Load VGG model if needed\n        if 'vgg_w' in hyperparameters.keys() and hyperparameters['vgg_w'] > 0:\n            self.vgg = load_vgg16(hyperparameters['vgg_model_path'] + '/models')\n            self.vgg.eval()\n            for param in self.vgg.parameters():\n                param.requires_grad = False\n\n    def recon_criterion(self, input, target):\n        return torch.mean(torch.abs(input - target))\n\n    def forward(self, x_a, x_b):\n        self.eval()\n        s_a = Variable(self.s_a)\n        s_b = Variable(self.s_b)\n        c_a, s_a_fake = self.gen_a.encode(x_a)\n        c_b, s_b_fake = self.gen_b.encode(x_b)\n        x_ba = self.gen_a.decode(c_b, s_a)\n        x_ab = self.gen_b.decode(c_a, s_b)\n        self.train()\n        return x_ab, x_ba\n\n    def gen_update(self, x_a, x_b, hyperparameters):\n        self.gen_opt.zero_grad()\n        s_a = Variable(torch.randn(x_a.size(0), self.style_dim, 1, 1).cuda())\n        s_b = Variable(torch.randn(x_b.size(0), self.style_dim, 1, 1).cuda())\n        # encode\n        c_a, s_a_prime = self.gen_a.encode(x_a)\n        c_b, s_b_prime = self.gen_b.encode(x_b)\n        # decode (within domain)\n        x_a_recon = self.gen_a.decode(c_a, s_a_prime)\n        x_b_recon = self.gen_b.decode(c_b, s_b_prime)\n        # decode (cross domain)\n        x_ba = self.gen_a.decode(c_b, s_a)\n        x_ab = self.gen_b.decode(c_a, s_b)\n        # encode again\n        c_b_recon, s_a_recon = self.gen_a.encode(x_ba)\n        c_a_recon, s_b_recon = self.gen_b.encode(x_ab)\n        # decode again (if needed)\n        x_aba = self.gen_a.decode(c_a_recon, s_a_prime) if hyperparameters['recon_x_cyc_w'] > 0 else None\n        x_bab = self.gen_b.decode(c_b_recon, s_b_prime) if hyperparameters['recon_x_cyc_w'] > 0 else None\n\n        # reconstruction loss\n        self.loss_gen_recon_x_a = self.recon_criterion(x_a_recon, x_a)\n        self.loss_gen_recon_x_b = self.recon_criterion(x_b_recon, x_b)\n        self.loss_gen_recon_s_a = self.recon_criterion(s_a_recon, s_a)\n        self.loss_gen_recon_s_b = self.recon_criterion(s_b_recon, s_b)\n        self.loss_gen_recon_c_a = self.recon_criterion(c_a_recon, c_a)\n        self.loss_gen_recon_c_b = self.recon_criterion(c_b_recon, c_b)\n        self.loss_gen_cycrecon_x_a = self.recon_criterion(x_aba, x_a) if hyperparameters['recon_x_cyc_w'] > 0 else 0\n        self.loss_gen_cycrecon_x_b = self.recon_criterion(x_bab, x_b) if hyperparameters['recon_x_cyc_w'] > 0 else 0\n        # GAN loss\n        self.loss_gen_adv_a = self.dis_a.calc_gen_loss(x_ba)\n        self.loss_gen_adv_b = self.dis_b.calc_gen_loss(x_ab)\n        # domain-invariant perceptual loss\n        self.loss_gen_vgg_a = self.compute_vgg_loss(self.vgg, x_ba, x_b) if hyperparameters['vgg_w'] > 0 else 0\n        self.loss_gen_vgg_b = self.compute_vgg_loss(self.vgg, x_ab, x_a) if hyperparameters['vgg_w'] > 0 else 0\n        # total loss\n        self.loss_gen_total = hyperparameters['gan_w'] * self.loss_gen_adv_a + \\\n                              hyperparameters['gan_w'] * self.loss_gen_adv_b + \\\n                              hyperparameters['recon_x_w'] * self.loss_gen_recon_x_a + \\\n                              hyperparameters['recon_s_w'] * self.loss_gen_recon_s_a + \\\n                              hyperparameters['recon_c_w'] * self.loss_gen_recon_c_a + \\\n                              hyperparameters['recon_x_w'] * self.loss_gen_recon_x_b + \\\n                              hyperparameters['recon_s_w'] * self.loss_gen_recon_s_b + \\\n                              hyperparameters['recon_c_w'] * self.loss_gen_recon_c_b + \\\n                              hyperparameters['recon_x_cyc_w'] * self.loss_gen_cycrecon_x_a + \\\n                              hyperparameters['recon_x_cyc_w'] * self.loss_gen_cycrecon_x_b + \\\n                              hyperparameters['vgg_w'] * self.loss_gen_vgg_a + \\\n                              hyperparameters['vgg_w'] * self.loss_gen_vgg_b\n        self.loss_gen_total.backward()\n        self.gen_opt.step()\n\n    def compute_vgg_loss(self, vgg, img, target):\n        img_vgg = vgg_preprocess(img)\n        target_vgg = vgg_preprocess(target)\n        img_fea = vgg(img_vgg)\n        target_fea = vgg(target_vgg)\n        return torch.mean((self.instancenorm(img_fea) - self.instancenorm(target_fea)) ** 2)\n\n    def sample(self, x_a, x_b):\n        self.eval()\n        s_a1 = Variable(self.s_a)\n        s_b1 = Variable(self.s_b)\n        s_a2 = Variable(torch.randn(x_a.size(0), self.style_dim, 1, 1).cuda())\n        s_b2 = Variable(torch.randn(x_b.size(0), self.style_dim, 1, 1).cuda())\n        x_a_recon, x_b_recon, x_ba1, x_ba2, x_ab1, x_ab2 = [], [], [], [], [], []\n        for i in range(x_a.size(0)):\n            c_a, s_a_fake = self.gen_a.encode(x_a[i].unsqueeze(0))\n            c_b, s_b_fake = self.gen_b.encode(x_b[i].unsqueeze(0))\n            x_a_recon.append(self.gen_a.decode(c_a, s_a_fake))\n            x_b_recon.append(self.gen_b.decode(c_b, s_b_fake))\n            x_ba1.append(self.gen_a.decode(c_b, s_a1[i].unsqueeze(0)))\n            x_ba2.append(self.gen_a.decode(c_b, s_a2[i].unsqueeze(0)))\n            x_ab1.append(self.gen_b.decode(c_a, s_b1[i].unsqueeze(0)))\n            x_ab2.append(self.gen_b.decode(c_a, s_b2[i].unsqueeze(0)))\n        x_a_recon, x_b_recon = torch.cat(x_a_recon), torch.cat(x_b_recon)\n        x_ba1, x_ba2 = torch.cat(x_ba1), torch.cat(x_ba2)\n        x_ab1, x_ab2 = torch.cat(x_ab1), torch.cat(x_ab2)\n        self.train()\n        return x_a, x_a_recon, x_ab1, x_ab2, x_b, x_b_recon, x_ba1, x_ba2\n\n    def dis_update(self, x_a, x_b, hyperparameters):\n        self.dis_opt.zero_grad()\n        s_a = Variable(torch.randn(x_a.size(0), self.style_dim, 1, 1).cuda())\n        s_b = Variable(torch.randn(x_b.size(0), self.style_dim, 1, 1).cuda())\n        # encode\n        c_a, _ = self.gen_a.encode(x_a)\n        c_b, _ = self.gen_b.encode(x_b)\n        # decode (cross domain)\n        x_ba = self.gen_a.decode(c_b, s_a)\n        x_ab = self.gen_b.decode(c_a, s_b)\n        # D loss\n        self.loss_dis_a = self.dis_a.calc_dis_loss(x_ba.detach(), x_a)\n        self.loss_dis_b = self.dis_b.calc_dis_loss(x_ab.detach(), x_b)\n        self.loss_dis_total = hyperparameters['gan_w'] * self.loss_dis_a + hyperparameters['gan_w'] * self.loss_dis_b\n        self.loss_dis_total.backward()\n        self.dis_opt.step()\n\n    def update_learning_rate(self):\n        if self.dis_scheduler is not None:\n            self.dis_scheduler.step()\n        if self.gen_scheduler is not None:\n            self.gen_scheduler.step()\n\n    def resume(self, checkpoint_dir, hyperparameters):\n        # Load generators\n        last_model_name = get_model_list(checkpoint_dir, \"gen\")\n        state_dict = torch.load(last_model_name)\n        self.gen_a.load_state_dict(state_dict['a'])\n        self.gen_b.load_state_dict(state_dict['b'])\n        iterations = int(last_model_name[-11:-3])*23000\n        # Load discriminators\n        last_model_name = get_model_list(checkpoint_dir, \"dis\")\n        state_dict = torch.load(last_model_name)\n        self.dis_a.load_state_dict(state_dict['a'])\n        self.dis_b.load_state_dict(state_dict['b'])\n        # Load optimizers\n        state_dict = torch.load(os.path.join(checkpoint_dir, 'optimizer.pt'))\n        self.dis_opt.load_state_dict(state_dict['dis'])\n        self.gen_opt.load_state_dict(state_dict['gen'])\n        # Reinitilize schedulers\n        self.dis_scheduler = get_scheduler(self.dis_opt, hyperparameters, iterations)\n        self.gen_scheduler = get_scheduler(self.gen_opt, hyperparameters, iterations)\n        print('Resume from iteration %d' % iterations)\n        return iterations\n\n    def save(self, snapshot_dir, iterations):\n        # Save generators, discriminators, and optimizers\n        gen_name = os.path.join(snapshot_dir, 'gen_%08d.pt' % (iterations + 1))\n        dis_name = os.path.join(snapshot_dir, 'dis_%08d.pt' % (iterations + 1))\n        opt_name = os.path.join(snapshot_dir, 'optimizer.pt')\n        torch.save({'a': self.gen_a.state_dict(), 'b': self.gen_b.state_dict()}, gen_name)\n        torch.save({'a': self.dis_a.state_dict(), 'b': self.dis_b.state_dict()}, dis_name)\n        torch.save({'gen': self.gen_opt.state_dict(), 'dis': self.dis_opt.state_dict()}, opt_name)\n\n\nclass UNIT_Trainer(nn.Module):\n    def __init__(self, hyperparameters):\n        super(UNIT_Trainer, self).__init__()\n        lr = hyperparameters['lr']\n        # Initiate the networks\n        self.gen_a = VAEGen(hyperparameters['input_dim_a'], hyperparameters['gen'])  # auto-encoder for domain a\n        self.gen_b = VAEGen(hyperparameters['input_dim_b'], hyperparameters['gen'])  # auto-encoder for domain b\n        self.dis_a = MsImageDis(hyperparameters['input_dim_a'], hyperparameters['dis'])  # discriminator for domain a\n        self.dis_b = MsImageDis(hyperparameters['input_dim_b'], hyperparameters['dis'])  # discriminator for domain b\n        self.instancenorm = nn.InstanceNorm2d(512, affine=False)\n\n        # Setup the optimizers\n        beta1 = hyperparameters['beta1']\n        beta2 = hyperparameters['beta2']\n        dis_params = list(self.dis_a.parameters()) + list(self.dis_b.parameters())\n        gen_params = list(self.gen_a.parameters()) + list(self.gen_b.parameters())\n        self.dis_opt = torch.optim.Adam([p for p in dis_params if p.requires_grad],\n                                        lr=lr, betas=(beta1, beta2), weight_decay=hyperparameters['weight_decay'])\n        self.gen_opt = torch.optim.Adam([p for p in gen_params if p.requires_grad],\n                                        lr=lr, betas=(beta1, beta2), weight_decay=hyperparameters['weight_decay'])\n        self.dis_scheduler = get_scheduler(self.dis_opt, hyperparameters)\n        self.gen_scheduler = get_scheduler(self.gen_opt, hyperparameters)\n\n        # Network weight initialization\n        self.apply(weights_init(hyperparameters['init']))\n        self.dis_a.apply(weights_init('gaussian'))\n        self.dis_b.apply(weights_init('gaussian'))\n\n        # Load VGG model if needed\n        if 'vgg_w' in hyperparameters.keys() and hyperparameters['vgg_w'] > 0:\n            self.vgg = load_vgg16(hyperparameters['vgg_model_path'] + '/models')\n            self.vgg.eval()\n            for param in self.vgg.parameters():\n                param.requires_grad = False\n\n    def recon_criterion(self, input, target):\n        return torch.mean(torch.abs(input - target))\n\n    def forward(self, x_a, x_b):\n        self.eval()\n        h_a, _ = self.gen_a.encode(x_a)\n        h_b, _ = self.gen_b.encode(x_b)\n        x_ba = self.gen_a.decode(h_b)\n        x_ab = self.gen_b.decode(h_a)\n        self.train()\n        return x_ab, x_ba\n\n    def __compute_kl(self, mu):\n        # def _compute_kl(self, mu, sd):\n        # mu_2 = torch.pow(mu, 2)\n        # sd_2 = torch.pow(sd, 2)\n        # encoding_loss = (mu_2 + sd_2 - torch.log(sd_2)).sum() / mu_2.size(0)\n        # return encoding_loss\n        mu_2 = torch.pow(mu, 2)\n        encoding_loss = torch.mean(mu_2)\n        return encoding_loss\n\n    def gen_update(self, x_a, x_b, hyperparameters):\n        self.gen_opt.zero_grad()\n        # encode\n        h_a, n_a = self.gen_a.encode(x_a)\n        h_b, n_b = self.gen_b.encode(x_b)\n        # decode (within domain)\n        x_a_recon = self.gen_a.decode(h_a + n_a)\n        x_b_recon = self.gen_b.decode(h_b + n_b)\n        # decode (cross domain)\n        x_ba = self.gen_a.decode(h_b + n_b)\n        x_ab = self.gen_b.decode(h_a + n_a)\n        # encode again\n        h_b_recon, n_b_recon = self.gen_a.encode(x_ba)\n        h_a_recon, n_a_recon = self.gen_b.encode(x_ab)\n        # decode again (if needed)\n        x_aba = self.gen_a.decode(h_a_recon + n_a_recon) if hyperparameters['recon_x_cyc_w'] > 0 else None\n        x_bab = self.gen_b.decode(h_b_recon + n_b_recon) if hyperparameters['recon_x_cyc_w'] > 0 else None\n\n        # reconstruction loss\n        self.loss_gen_recon_x_a = self.recon_criterion(x_a_recon, x_a)\n        self.loss_gen_recon_x_b = self.recon_criterion(x_b_recon, x_b)\n        self.loss_gen_recon_kl_a = self.__compute_kl(h_a)\n        self.loss_gen_recon_kl_b = self.__compute_kl(h_b)\n        self.loss_gen_cyc_x_a = self.recon_criterion(x_aba, x_a)\n        self.loss_gen_cyc_x_b = self.recon_criterion(x_bab, x_b)\n        self.loss_gen_recon_kl_cyc_aba = self.__compute_kl(h_a_recon)\n        self.loss_gen_recon_kl_cyc_bab = self.__compute_kl(h_b_recon)\n        # GAN loss\n        self.loss_gen_adv_a = self.dis_a.calc_gen_loss(x_ba)\n        self.loss_gen_adv_b = self.dis_b.calc_gen_loss(x_ab)\n        # domain-invariant perceptual loss\n        self.loss_gen_vgg_a = self.compute_vgg_loss(self.vgg, x_ba, x_b) if hyperparameters['vgg_w'] > 0 else 0\n        self.loss_gen_vgg_b = self.compute_vgg_loss(self.vgg, x_ab, x_a) if hyperparameters['vgg_w'] > 0 else 0\n        # total loss\n        self.loss_gen_total = hyperparameters['gan_w'] * self.loss_gen_adv_a + \\\n                              hyperparameters['gan_w'] * self.loss_gen_adv_b + \\\n                              hyperparameters['recon_x_w'] * self.loss_gen_recon_x_a + \\\n                              hyperparameters['recon_kl_w'] * self.loss_gen_recon_kl_a + \\\n                              hyperparameters['recon_x_w'] * self.loss_gen_recon_x_b + \\\n                              hyperparameters['recon_kl_w'] * self.loss_gen_recon_kl_b + \\\n                              hyperparameters['recon_x_cyc_w'] * self.loss_gen_cyc_x_a + \\\n                              hyperparameters['recon_kl_cyc_w'] * self.loss_gen_recon_kl_cyc_aba + \\\n                              hyperparameters['recon_x_cyc_w'] * self.loss_gen_cyc_x_b + \\\n                              hyperparameters['recon_kl_cyc_w'] * self.loss_gen_recon_kl_cyc_bab + \\\n                              hyperparameters['vgg_w'] * self.loss_gen_vgg_a + \\\n                              hyperparameters['vgg_w'] * self.loss_gen_vgg_b\n        self.loss_gen_total.backward()\n        self.gen_opt.step()\n\n    def compute_vgg_loss(self, vgg, img, target):\n        img_vgg = vgg_preprocess(img)\n        target_vgg = vgg_preprocess(target)\n        img_fea = vgg(img_vgg)\n        target_fea = vgg(target_vgg)\n        return torch.mean((self.instancenorm(img_fea) - self.instancenorm(target_fea)) ** 2)\n\n    def sample(self, x_a, x_b):\n        self.eval()\n        x_a_recon, x_b_recon, x_ba, x_ab = [], [], [], []\n        for i in range(x_a.size(0)):\n            h_a, _ = self.gen_a.encode(x_a[i].unsqueeze(0))\n            h_b, _ = self.gen_b.encode(x_b[i].unsqueeze(0))\n            x_a_recon.append(self.gen_a.decode(h_a))\n            x_b_recon.append(self.gen_b.decode(h_b))\n            x_ba.append(self.gen_a.decode(h_b))\n            x_ab.append(self.gen_b.decode(h_a))\n        x_a_recon, x_b_recon = torch.cat(x_a_recon), torch.cat(x_b_recon)\n        x_ba = torch.cat(x_ba)\n        x_ab = torch.cat(x_ab)\n        self.train()\n        return x_a, x_a_recon, x_ab, x_b, x_b_recon, x_ba\n\n    def dis_update(self, x_a, x_b, hyperparameters):\n        self.dis_opt.zero_grad()\n        # encode\n        h_a, n_a = self.gen_a.encode(x_a)\n        h_b, n_b = self.gen_b.encode(x_b)\n        # decode (cross domain)\n        x_ba = self.gen_a.decode(h_b + n_b)\n        x_ab = self.gen_b.decode(h_a + n_a)\n        # D loss\n        self.loss_dis_a = self.dis_a.calc_dis_loss(x_ba.detach(), x_a)\n        self.loss_dis_b = self.dis_b.calc_dis_loss(x_ab.detach(), x_b)\n        self.loss_dis_total = hyperparameters['gan_w'] * self.loss_dis_a + hyperparameters['gan_w'] * self.loss_dis_b\n        self.loss_dis_total.backward()\n        self.dis_opt.step()\n\n    def update_learning_rate(self):\n        if self.dis_scheduler is not None:\n            self.dis_scheduler.step()\n        if self.gen_scheduler is not None:\n            self.gen_scheduler.step()\n\n    def resume(self, checkpoint_dir, hyperparameters):\n        # Load generators\n        last_model_name = get_model_list(checkpoint_dir, \"gen\")\n        state_dict = torch.load(last_model_name)\n        self.gen_a.load_state_dict(state_dict['a'])\n        self.gen_b.load_state_dict(state_dict['b'])\n        epoch = int(last_model_name[-11:-3])\n        iterations = epoch*23000\n        # Load discriminators\n        last_model_name = get_model_list(checkpoint_dir, \"dis\")\n        state_dict = torch.load(last_model_name)\n        self.dis_a.load_state_dict(state_dict['a'])\n        self.dis_b.load_state_dict(state_dict['b'])\n        # Load optimizers\n        state_dict = torch.load(os.path.join(checkpoint_dir, 'optimizer.pt'))\n        self.dis_opt.load_state_dict(state_dict['dis'])\n        self.gen_opt.load_state_dict(state_dict['gen'])\n        # Reinitilize schedulers\n        self.dis_scheduler = get_scheduler(self.dis_opt, hyperparameters, iterations)\n        self.gen_scheduler = get_scheduler(self.gen_opt, hyperparameters, iterations)\n        print('Resume from iteration %d' % iterations)\n        return epoch,iterations\n\n    def save(self, snapshot_dir, iterations):\n        # Save generators, discriminators, and optimizers\n        gen_name = os.path.join(snapshot_dir, 'gen_%08d.pt' % (iterations + 1))\n        dis_name = os.path.join(snapshot_dir, 'dis_%08d.pt' % (iterations + 1))\n        opt_name = os.path.join(snapshot_dir, 'optimizer.pt')\n        torch.save({'a': self.gen_a.state_dict(), 'b': self.gen_b.state_dict()}, gen_name)\n        torch.save({'a': self.dis_a.state_dict(), 'b': self.dis_b.state_dict()}, dis_name)\n        torch.save({'gen': self.gen_opt.state_dict(), 'dis': self.dis_opt.state_dict()}, opt_name)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T15:47:53.015970Z","iopub.execute_input":"2024-06-22T15:47:53.016267Z","iopub.status.idle":"2024-06-22T15:47:53.102873Z","shell.execute_reply.started":"2024-06-22T15:47:53.016235Z","shell.execute_reply":"2024-06-22T15:47:53.101821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nCopyright (C) 2018 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n\"\"\"\n# from utils import get_all_data_loaders, prepare_sub_folder, write_html, write_loss, get_config, write_2images, Timer\nimport argparse\nfrom torch.autograd import Variable\n# from trainer import MUNIT_Trainer, UNIT_Trainer\nimport torch.backends.cudnn as cudnn\nimport torch\nfrom skimage.metrics import structural_similarity as ssim\ntry:\n    from itertools import izip as zip\nexcept ImportError: # will be 3.x series\n    pass\nimport os\nimport sys\nimport tensorboardX\nimport shutil\nimport csv\n# parser = argparse.ArgumentParser()\n# parser.add_argument('--config', type=str, default='configs/edges2handbags_folder.yaml', help='Path to the config file.')\n# parser.add_argument('--output_path', type=str, default='.', help=\"outputs path\")\n# parser.add_argument(\"--resume\", action=\"store_true\")\n# parser.add_argument('--trainer', type=str, default='MUNIT', help=\"MUNIT|UNIT\")\n# opts = parser.parse_args()\n\ncudnn.benchmark = True\n\n# Load experiment setting\n# config = get_config(opts.config)\nmax_iter = config['max_iter']\ndisplay_size = config['display_size']\n# config['vgg_model_path'] = opts.output_path\nmodel = config['model']\n# Setup model and data loader\nif model == 'MUNIT':\n    trainer = MUNIT_Trainer(config)\nelif model == 'UNIT':\n    trainer = UNIT_Trainer(config)\nelse:\n    sys.exit(\"Only support MUNIT|UNIT\")\ntrainer.cuda()\n# train_loader_a, train_loader_b, test_loader_a, test_loader_b = get_all_data_loaders(config)\ndisplay_size = 1\n\n# Get a batch of data from the data loader\nimages_ab = next(iter(train_dataloader))\n\n# Unpack images_ab into images_a and images_b\nimages_a, images_b = images_ab\n\n# Stack the required images from the batch\ntrain_display_images_a = torch.stack([images_a[i] for i in range(display_size)])\ntrain_display_images_b = torch.stack([images_b[i] for i in range(display_size)])\n\n# Move the stacked images to the GPU\ntrain_display_images_a = train_display_images_a.cuda().float()\ntrain_display_images_b = train_display_images_b.cuda().float()\n# train_display_images_a = torch.stack([train_loader_a.dataset[i] for i in range(display_size)]).cuda()\n# train_display_images_b = torch.stack([train_loader_b.dataset[i] for i in range(display_size)]).cuda()\n# test_display_images_a = torch.stack([test_loader_a.dataset[i] for i in range(display_size)]).cuda()\n# test_display_images_b = torch.stack([test_loader_b.dataset[i] for i in range(display_size)]).cuda()\n\n# Setup logger and output folders\n# model_name = os.path.splitext(os.path.basename(opts.config))[0]\ntrain_writer = tensorboardX.SummaryWriter(os.path.join('/kaggle/working/'+ \"/logs\", 'Hello Part1'))\n# output_directory = os.path.join(opts.output_path + \"/outputs\", model_name)\n# checkpoint_directory, image_directory = prepare_sub_folder(output_directory)\n# shutil.copy(opts.config, os.path.join(output_directory, 'config.yaml')) # copy config file to output folder\n\n# Start training\nstart_epoch,iterations = trainer.resume('/kaggle/input/modelunit', hyperparameters=config)\n# start_epoch = 0\n# iterations = 0\nfieldnames = ['Epoch', 'SSIM', 'MSE', 'PSNR','Val_SSIM', 'Val_MSE', 'Val_PSNR']\nflag = True\nfor epoch in range(start_epoch,config['epochs']+1):\n    SSIM = []\n    MSE = []\n    PSNR = []\n    Val_SSIM = []\n    Val_MSE = []\n    Val_PSNR = []\n    for it, (images_a, images_b) in enumerate(train_dataloader):\n        if images_a == None  :\n            continue\n        if images_a.shape[1] == 0:\n            continue\n        trainer.update_learning_rate()\n        images_a, images_b = images_a.cuda().detach().float(), images_b.cuda().detach().float()\n        x_a, x_a_recon, x_ab, x_b, x_b_recon, x_ba = trainer.sample(images_a, images_b) \n        x_a_np = x_a.cpu().detach().numpy().squeeze()\n        x_b_np = x_b.cpu().detach().numpy().squeeze()\n        x_ab_np = x_ab.cpu().detach().numpy().squeeze()\n        ssim_score = ssim(x_ab_np,x_b_np, win_size=5 ,multichannel=False,data_range=2.0)\n        \n        SSIM.append(ssim_score)\n\n        mse_loss = F.mse_loss(x_ab, x_b)\n\n\n        mse = mse_loss.item()\n        MSE.append(mse)\n        \n        psnr = 10 * torch.log10(torch.tensor(4 / mse))\n        PSNR.append(psnr.item())\n#         with Timer(\"Elapsed time in update: %f\"):\n            # Main training code\n        trainer.dis_update(images_a, images_b, config)\n        trainer.gen_update(images_a, images_b, config)\n        torch.cuda.synchronize()\n\n        # Dump training stats in log file\n        if (iterations + 1) % config['log_iter'] == 0:\n            print(\"Iteration: %08d/%08d\" % (iterations + 1, max_iter))\n            write_loss(iterations, trainer, train_writer)\n\n        # Write images\n#         if (iterations + 1) % config['image_save_iter'] == 0:\n#             with torch.no_grad():\n#                 test_image_outputs = trainer.sample(test_display_images_a, test_display_images_b)\n#                 train_image_outputs = trainer.sample(train_display_images_a, train_display_images_b)\n#                   x_a, x_a_recon, x_ab, x_b, x_b_recon, x_ba = trainer.sample(train_display_images_a, train_display_images_b)\n#             write_2images(test_image_outputs, display_size, image_directory, 'test_%08d' % (iterations + 1))\n#             write_2images(train_image_outputs, display_size, '/kaggle/working/', 'train_%08d' % (iterations + 1))\n            \n            # HTML\n#             write_html(output_directory + \"/index.html\", iterations + 1, config['image_save_iter'], 'images')\n\n        if (iterations + 1) % config['image_display_iter'] == 0:\n#             with torch.no_grad():\n#                 x_a, x_a_recon, x_ab, x_b, x_b_recon, x_ba = trainer.sample(images_a, images_b) \n#                 image_outputs = trainer.sample(train_display_images_a, train_display_images_b)\n#             write_2images(image_outputs, display_size, '/kaggle/working/', 'train_current')\n#             x_a_np = x_a.cpu().numpy().squeeze()\n#             x_b_np = x_b.cpu().numpy().squeeze()\n#             x_ab_np = x_ab.cpu().numpy().squeeze()\n#             fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n# #              # Plot images on each subplot\n#             axs[0].imshow(x_a_np.reshape(256,256,1),cmap='gray')\n#             axs[0].axis('off')  # Hide axis\n#             axs[0].set_title('Motion Image')\n#             axs[1].imshow(x_b_np.reshape(256,256,1),cmap='gray')\n#             axs[1].axis('off')  # Hide axis\n#             axs[1].set_title('Free Image')\n\n#             axs[2].imshow(x_ab_np.reshape(256,256,1),cmap='gray')\n#             axs[2].axis('off')  # Hide axis\n#             axs[2].set_title('Model Image')\n            avgssim = sum(SSIM)/len(SSIM)\n            print(\"SSIM score:\", avgssim)\n\n            avgmse = sum(MSE)/len(MSE)\n            print(\"MSE Loss:\",avgmse )\n\n            avgpsnr = sum(PSNR)/len(PSNR)\n            print(\"PSNR:\", avgpsnr)   \n            with open('/kaggle/working/metrics1000sample.csv', 'a', newline='') as csvfile:\n                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n                writer.writerow({'Epoch': epoch, 'SSIM': avgssim, 'MSE': avgmse, 'PSNR':avgpsnr})\n            # Adjust layout\n#             plt.tight_layout()\n\n#             plt.show()\n        iterations += 1\n\n        # Save network weights\n#         if (iterations + 1) % config['snapshot_save_iter'] == 0:\n    trainer.save('/kaggle/working/', epoch)\n    print(f\"=================== Saving epoch number {epoch+1} ===================\")\n    avgssim = sum(SSIM)/len(SSIM)\n    print(f\"Epoch {epoch} SSIM score:\", avgssim)\n\n    avgmse = sum(MSE)/len(MSE)\n    print(f\"Epoch {epoch} MSE Loss:\",avgmse )\n\n    avgpsnr = sum(PSNR)/len(PSNR)\n    print(f\"Epoch {epoch} PSNR:\", avgpsnr) \n    for it, (images_a, images_b) in enumerate(validation_dataloader):\n        if images_a == None  :\n            continue\n        if images_a.shape[1] == 0:\n            continue\n        images_a, images_b = images_a.cuda().detach().float(), images_b.cuda().detach().float()\n        x_a, x_a_recon, x_ab, x_b, x_b_recon, x_ba = trainer.sample(images_a, images_b) \n        x_a_np = x_a.cpu().detach().numpy().squeeze()\n        x_b_np = x_b.cpu().detach().numpy().squeeze()\n        x_ab_np = x_ab.cpu().detach().numpy().squeeze()\n        ssim_score = ssim(x_ab_np,x_b_np, win_size=5 ,multichannel=False,data_range=2.0)\n        \n        Val_SSIM.append(ssim_score)\n\n        mse_loss = F.mse_loss(x_ab, x_b)\n\n\n        mse = mse_loss.item()\n        Val_MSE.append(mse)\n        \n        psnr = 10 * torch.log10(torch.tensor(4 / mse))\n        Val_PSNR.append(psnr.item())\n    print(f\"=================== Valdtion epoch number {epoch+1} ===================\")\n    Val_avgssim = sum(Val_SSIM)/len(Val_SSIM)\n    print(f\"Epoch {epoch} Val SSIM score:\", Val_avgssim)\n\n    Val_avgmse = sum(Val_MSE)/len(Val_MSE)\n    print(f\"Epoch {epoch} Val MSE Loss:\",Val_avgmse )\n\n    Val_avgpsnr = sum(Val_PSNR)/len(Val_PSNR)\n    print(f\"Epoch {epoch}  Val PSNR:\", Val_avgpsnr) \n    with open('/kaggle/working/metricssample.csv', 'a', newline='') as csvfile:\n        \n#                 writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n                epoch_writer = csv.writer(csvfile)\n                if flag:\n                    epoch_writer.writerow(['Epoch','SSIM','MSE', 'PSNR','Val_SSIM','Val_MSE','Val_PSNR'])\n                    flag = False\n                epoch_writer.writerow([epoch+1, avgssim, avgmse, avgpsnr,Val_avgssim, Val_avgmse, Val_avgpsnr])","metadata":{"execution":{"iopub.status.busy":"2024-06-22T15:47:53.104216Z","iopub.execute_input":"2024-06-22T15:47:53.104552Z","iopub.status.idle":"2024-06-22T15:48:29.851565Z","shell.execute_reply.started":"2024-06-22T15:47:53.104527Z","shell.execute_reply":"2024-06-22T15:48:29.850116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}