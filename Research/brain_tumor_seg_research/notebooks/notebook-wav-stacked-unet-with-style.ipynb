{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8456774,"sourceType":"datasetVersion","datasetId":5040362},{"sourceId":8744287,"sourceType":"datasetVersion","datasetId":5248196}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":41687.332372,"end_time":"2024-05-22T01:04:29.290076","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-05-21T13:29:41.957704","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U tensorflow==2.15.0\nimport tensorflow as tf\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n\n\nprint(\"TensorFlow version:\", tf.__version__)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":28.705818,"end_time":"2024-05-21T13:30:13.464971","exception":false,"start_time":"2024-05-21T13:29:44.759153","status":"completed"},"tags":[],"scrolled":true,"execution":{"iopub.status.busy":"2024-06-21T21:36:56.617680Z","iopub.execute_input":"2024-06-21T21:36:56.618097Z","iopub.status.idle":"2024-06-21T21:37:09.453973Z","shell.execute_reply.started":"2024-06-21T21:36:56.618067Z","shell.execute_reply":"2024-06-21T21:37:09.452687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport subprocess\nimport nibabel as nib\nimport tensorflow as tf\nfrom skimage import exposure\nfrom math import ceil\nimport matplotlib.pyplot as plt\nimport random\n\nclass DataLoader:\n    def __init__(self, data_path, split_ratio, batch_size=None):\n        self.data_path = data_path\n        self.split_ratio = split_ratio\n        self.all_subjects = None\n        self.subjects_lists = []\n        self.labels = {'train': 0, 'test': 1, 'validation': 2}\n        self.size = [0, 0, 0]\n        self.batch_size = batch_size\n        self.slices_number = None\n\n    def list_subjects(self):\n        subjects = os.listdir(self.data_path)\n        subjects = [item for item in subjects if item.startswith('sub')]\n        self.all_subjects = subjects\n\n    def get_nifti_path(self, subject, number_of_motion='1'):\n        ref_path_stand = f'{self.data_path}/{subject}/anat/{subject}_acq-standard_T1w.nii'\n        ref_path_motion = f'{self.data_path}/{subject}/anat/{subject}_acq-headmotion{number_of_motion}_T1w.nii'\n\n        return [ref_path_stand, ref_path_motion]\n\n    def get_paired_volumes(self, path):\n        if os.path.exists(path[0]) and os.path.exists(path[1]):\n            free_data = nib.load(path[0]).get_fdata()\n#             free_data = exposure.rescale_intensity(free_data, out_range=(0.0, 1.0))\n            free_data = exposure.rescale_intensity(free_data[37:-37], out_range=(-1.0, 1.0))\n\n            motion_data = nib.load(path[1]).get_fdata()\n#             motion_data = exposure.rescale_intensity(motion_data, out_range=(0.0, 1.0))\n            motion_data = exposure.rescale_intensity(motion_data[37:-37], out_range=(-1.0, 1.0))\n            return tf.convert_to_tensor(free_data), tf.convert_to_tensor(motion_data)\n        else:\n            return None, None\n\n    def split_data(self):\n        self.list_subjects()\n        if ceil(sum(self.split_ratio)) == 1 and len(self.split_ratio) <= 3:\n            self.split_ratio.insert(0, 0)\n            cumulative_sum = [sum(self.split_ratio[:i + 1]) for i in range(len(self.split_ratio))]\n            number_of_subjects = len(self.all_subjects)\n\n            for i in range(1, len(self.split_ratio)):\n                self.subjects_lists.append(\n                    self.all_subjects[int(round(cumulative_sum[i - 1] * number_of_subjects)):int(\n                        round(cumulative_sum[i] * number_of_subjects))])\n\n                self.size[i - 1] = len(self.subjects_lists[i - 1])  * 2 * 190\n\n                if i - 1 == 0:\n                    self.size[i - 1] -= 8  * 2 * 190\n        else:\n            print(\"The Summation of ratios is not equal to 1\")\n       \n    def generator(self, mode):\n        subjects = self.subjects_lists[self.labels[mode]]\n\n        def data_gen():\n            for subject in subjects:\n                for i in range(2):\n                    pathes = self.get_nifti_path(subject, str(i + 1))\n                    free, motion = self.get_paired_volumes(pathes)\n                    if (free is not None) and (motion is not None):\n                        self.slices_number = motion.shape[0]\n\n                        for slice_id in range(0, self.slices_number):\n                            start_idx = slice_id + 1\n                            end_idx = (slice_id + 1) + 1\n                            if (end_idx < self.slices_number-1):\n                                free_slice = free[start_idx:end_idx]\n                                free_slice = tf.transpose(free_slice, perm=[1, 2, 0])\n                                \n                                motion_slice = motion[start_idx:end_idx]\n                                motion_slice = tf.transpose(motion_slice, perm=[1, 2, 0])\n                                \n                                motion_before_slice = motion[start_idx-1:end_idx-1]\n                                motion_before_slice = tf.transpose(motion_before_slice, perm=[1, 2, 0])\n                                \n                                motion_after_slice = motion[start_idx+1:end_idx+1]\n                                motion_after_slice = tf.transpose(motion_after_slice, perm=[1, 2, 0])\n\n                                yield (\n                                (motion_before_slice, motion_slice, motion_after_slice),\n                                free_slice\n                                )\n\n        input_signature = (\n            (tf.TensorSpec(shape=(256, 256, 1), dtype=tf.float32),\n             tf.TensorSpec(shape=(256, 256, 1), dtype=tf.float32),\n             tf.TensorSpec(shape=(256, 256, 1), dtype=tf.float32)),\n            tf.TensorSpec(shape=(256, 256, 1), dtype=tf.float32)\n        )\n\n        dataset = tf.data.Dataset.from_generator(data_gen, output_signature=input_signature)\n        dataset = dataset.batch(self.batch_size)\n\n        return dataset","metadata":{"papermill":{"duration":0.285759,"end_time":"2024-05-21T13:30:13.757272","exception":false,"start_time":"2024-05-21T13:30:13.471513","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-21T21:37:09.456835Z","iopub.execute_input":"2024-06-21T21:37:09.457293Z","iopub.status.idle":"2024-06-21T21:37:09.485745Z","shell.execute_reply.started":"2024-06-21T21:37:09.457241Z","shell.execute_reply":"2024-06-21T21:37:09.484816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import keras\nimport tensorflow as tf\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras import layers\nimport tensorflow.keras.backend as K\nimport numpy as np\nimport pywt\n\nIMAGE_ORDERING_CHANNELS_LAST = \"channels_last\"\nIMAGE_ORDERING_CHANNELS_FIRST = \"channels_first\"\n\n# Default IMAGE_ORDERING = channels_last\nIMAGE_ORDERING = IMAGE_ORDERING_CHANNELS_LAST\n\nif IMAGE_ORDERING == 'channels_first':\n\tMERGE_AXIS = 1\nelif IMAGE_ORDERING == 'channels_last':\n\tMERGE_AXIS = -1\n#Define Adaptive normalization layer\ndef expand_moments_dim(moment):\n    return tf.reshape(moment, [-1, 1, 1, tf.shape(moment)[-1]])\nclass AdaptiveInstanceNorm(Layer):\n    def __init__(self, epsilon=1e-5, **kwargs):\n        super(AdaptiveInstanceNorm, self).__init__(**kwargs)\n        self.epsilon = epsilon\n\n    def call(self, content, gamma, beta):\n        c_mean, c_var = tf.nn.moments(content, axes=[1, 2], keepdims=True)\n        c_std = tf.sqrt(c_var + self.epsilon)\n        normalized = (content - c_mean) / c_std\n        gamma = expand_moments_dim(gamma)\n        beta = expand_moments_dim(beta)\n        return multiply([gamma, normalized]) + beta\n\n    def get_config(self):\n        config = super(AdaptiveInstanceNorm, self).get_config()\n        config.update({\"epsilon\": self.epsilon})\n\n# Define the Haar filters\ndef haar_filter():\n    low_pass = tf.constant([1, 1], dtype=tf.float32) / tf.math.sqrt(2.0)\n    high_pass = tf.constant([1, -1], dtype=tf.float32) / tf.math.sqrt(2.0)\n    return low_pass, high_pass\n\n# Function to apply 1D filter across the width dimension\ndef apply_filter_1d(inputs, filter_kernel, stride):\n    \"\"\"\n    Applies a 1D filter across the width dimension.\n    \"\"\"\n    in_channels = inputs.shape[-1]\n    out_channels = in_channels  # Ensure output channels match input channels\n    \n    # Adjust the reshape operation to match filter_kernel's shape\n    filter_kernel = tf.reshape(filter_kernel, [2, 1, 1, 1])  # [filter_height, filter_width, in_channels, out_channels]\n    filter_kernel = tf.tile(filter_kernel, [1, 1, in_channels, out_channels])  # Tile to match input channels\n    \n    return tf.nn.conv2d(inputs, filter_kernel, strides=[1, 1, stride, 1], padding='SAME', data_format='NHWC')\n\n# Function to perform 1D wavelet transform\ndef wavelet_transform_1d(inputs):\n    low_pass, high_pass = haar_filter()\n    \n    # Apply filters manually\n    low = apply_filter_1d(inputs, low_pass, stride=2)\n    high = apply_filter_1d(inputs, high_pass, stride=2)\n    \n    return low, high\n\n# Function to perform 2D wavelet transform\ndef wavelet_transform_2d(inputs):\n    low, high = wavelet_transform_1d(inputs)\n    low_low, low_high = wavelet_transform_1d(tf.transpose(low, [0, 2, 1, 3]))\n    high_low, high_high = wavelet_transform_1d(tf.transpose(high, [0, 2, 1, 3]))\n    return tf.transpose(low_low, [0, 2, 1, 3]), tf.transpose(low_high, [0, 2, 1, 3]), tf.transpose(high_low, [0, 2, 1, 3]), tf.transpose(high_high, [0, 2, 1, 3])\n\n# Function to perform multi-level wavelet transform\ndef multi_level_wavelet_transform(inputs, levels=3):\n    wavelet_coeffs_1 = []\n    wavelet_coeffs_2 = []\n    wavelet_coeffs_3 = []\n    \n    for level in range(levels):\n        low_low, low_high, high_low, high_high = wavelet_transform_2d(inputs)\n        if level == 0:\n            wavelet_coeffs_1.append([low_high, high_low, high_high])\n        elif level == 1:\n            wavelet_coeffs_2.append([low_high, high_low, high_high])\n        elif level == 2:\n            wavelet_coeffs_3.append([low_low, low_high, high_low, high_high])\n\n        inputs = low_low  # Proceed to the next level with the low-frequency component\n    \n    wat1 = tf.convert_to_tensor(wavelet_coeffs_1)\n    wat2 = tf.convert_to_tensor(wavelet_coeffs_2)\n    wat3 = tf.convert_to_tensor(wavelet_coeffs_3)\n    \n#     print(\"wat1.shape: \", wat1.shape)\n#     print(\"wat2.shape: \", wat2.shape)\n#     print(\"wat3.shape: \", wat3.shape)\n\n    \n    wat1 = tf.squeeze(wat1, axis=0)\n#     print(\"wat1.shape after squeeze: \", wat1.shape)\n    wat1 = tf.transpose(wat1, perm=[1, 2, 3, 4, 0])\n#     print(\"wat1.shape after trans: \", wat1.shape)\n\n    wat2 = tf.squeeze(wat2, axis=0)\n    wat2 = tf.transpose(wat2, perm=[1, 2, 3, 4, 0])\n\n    wat3 = tf.squeeze(wat3, axis=0)\n    wat3 = tf.transpose(wat3, perm=[1, 2, 3, 4, 0])\n    \n    # Get the shape of the input tensor\n    wat1_shape = tf.shape(wat1)\n    # Calculate the new shape for reshaping\n    wat1_shape = tf.concat([wat1_shape[:-2], [wat1_shape[-2] * wat1_shape[-1]]], axis=0)\n    # Reshape the tensor\n    wat1 = tf.reshape(wat1, wat1_shape)\n    \n    # Get the shape of the input tensor\n    wat2_shape = tf.shape(wat2)\n    # Calculate the new shape for reshaping\n    wat2_shape = tf.concat([wat2_shape[:-2], [wat2_shape[-2] * wat2_shape[-1]]], axis=0)\n    # Reshape the tensor\n    wat2 = tf.reshape(wat2, wat2_shape)\n    \n    \n    # Get the shape of the input tensor\n    wat3_shape = tf.shape(wat3)\n    # Calculate the new shape for reshaping\n    wat3_shape = tf.concat([wat3_shape[:-2], [wat3_shape[-2] * wat3_shape[-1]]], axis=0)\n    # Reshape the tensor\n    wat3 = tf.reshape(wat3, wat3_shape)\n    \n#     print(\"wat1.shape: \", wat1.shape)\n#     print(\"wat2.shape: \", wat2.shape)\n#     print(\"wat3.shape: \", wat3.shape)\n    \n    return wat1, wat2, wat3\n\n    \ndef wat_3(inputs):\n#     print(\"inputs shape: \", inputs.shape)\n    wat1, wat2, wat3 = multi_level_wavelet_transform(inputs) \n    \n    # Explicitly set the shapes\n    batch_size = inputs.shape[0]\n    height, width = inputs.shape[1], inputs.shape[2]\n    channels = inputs.shape[3]\n\n    wat1.set_shape((batch_size, height // 2, width // 2, channels * 3))\n    wat2.set_shape((batch_size, height // 4, width // 4, channels * 3))\n    wat3.set_shape((batch_size, height // 8, width // 8, channels * 4))\n        \n#     print(\"wat1.shape: \", wat1.shape)\n#     print(\"wat2.shape: \", wat2.shape)\n#     print(\"wat3.shape: \", wat3.shape)\n        \n    return wat1, wat2, wat3\n\n\ndef wat_layer_1(x, wat):\n    watp_prod = Conv2D(16*(2**1), (3, 3), data_format=IMAGE_ORDERING, padding='same')(wat)\n    watp_prod = Activation('relu')(watp_prod)\n    watp_prod = Conv2D(32*(2**1), (3, 3), data_format=IMAGE_ORDERING, padding='same')(watp_prod)\n    \n    watp_sum = Conv2D(16*(2**1), (3, 3), data_format=IMAGE_ORDERING, padding='same')(wat)\n    watp_sum = Activation('relu')(watp_sum)\n    watp_sum = Conv2D(32*(2**1), (3, 3), data_format=IMAGE_ORDERING, padding='same')(watp_sum)\n    \n    x = multiply([x, watp_prod])\n    x = Add()([x, watp_sum])    \n    x = Activation('relu')(x)\n    return x\n    \n\ndef wat_layer_2(x, wat):\n    watp_prod = Conv2D(16*(2**2), (3, 3), data_format=IMAGE_ORDERING, padding='same')(wat)\n    watp_prod = Activation('relu')(watp_prod)\n    watp_prod = Conv2D(32*(2**2), (3, 3), data_format=IMAGE_ORDERING, padding='same')(watp_prod)\n    \n    watp_sum = Conv2D(16*(2**2), (3, 3), data_format=IMAGE_ORDERING, padding='same')(wat)\n    watp_sum = Activation('relu')(watp_sum)\n    watp_sum = Conv2D(32*(2**2), (3, 3), data_format=IMAGE_ORDERING, padding='same')(watp_sum)\n    \n    x = multiply([x, watp_prod])\n    x = Add()([x, watp_sum])    \n    x = Activation('relu')(x)\n    return x\n\n\ndef wat_layer_3(x, wat):\n    watp_prod = Conv2D(16*(2**3), (3, 3), data_format=IMAGE_ORDERING, padding='same')(wat)\n    watp_prod = Activation('relu')(watp_prod)\n    watp_prod = Conv2D(32*(2**3), (3, 3), data_format=IMAGE_ORDERING, padding='same')(watp_prod)\n    \n    watp_sum = Conv2D(16*(2**3), (3, 3), data_format=IMAGE_ORDERING, padding='same')(wat)\n    watp_sum = Activation('relu')(watp_sum)\n    watp_sum = Conv2D(32*(2**3), (3, 3), data_format=IMAGE_ORDERING, padding='same')(watp_sum)\n    \n    x = multiply([x, watp_prod])\n    x = Add()([x, watp_sum])    \n    x = Activation('relu')(x)\n    return x\n\n\n# CBAM --------------------------------------------\n# Convolutional Block Attention Module(CBAM) block\ndef cbam_block(cbam_feature, ratio=8):\n\tcbam_feature = channel_attention(cbam_feature, ratio)\n\tcbam_feature = spatial_attention(cbam_feature)\n\treturn cbam_feature\n\ndef channel_attention(input_feature, ratio=8):\n\n\tchannel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n\tchannel = input_feature.shape[channel_axis]  # input_feature._keras_shape[channel_axis]\n\n\tshared_layer_one = Dense(channel//ratio,\n\t\t\t\t\t\t\t activation='relu',\n\t\t\t\t\t\t\t kernel_initializer='he_normal',\n\t\t\t\t\t\t\t use_bias=True,\n\t\t\t\t\t\t\t bias_initializer='zeros')\n\tshared_layer_two = Dense(channel,\n\t\t\t\t\t\t\t kernel_initializer='he_normal',\n\t\t\t\t\t\t\t use_bias=True,\n\t\t\t\t\t\t\t bias_initializer='zeros')\n\n\tavg_pool = GlobalAveragePooling2D()(input_feature)\n\tavg_pool = Reshape((1,1,channel))(avg_pool)\n\tassert avg_pool.shape[1:] == (1,1,channel)\n\tavg_pool = shared_layer_one(avg_pool)\n\tassert avg_pool.shape[1:] == (1,1,channel//ratio)\n\tavg_pool = shared_layer_two(avg_pool)\n\tassert avg_pool.shape[1:] == (1,1,channel)\n\n\tmax_pool = GlobalMaxPooling2D()(input_feature)\n\tmax_pool = Reshape((1,1,channel))(max_pool)\n\tassert max_pool.shape[1:] == (1,1,channel)\n\tmax_pool = shared_layer_one(max_pool)\n\tassert max_pool.shape[1:] == (1,1,channel//ratio)\n\tmax_pool = shared_layer_two(max_pool)\n\tassert max_pool.shape[1:] == (1,1,channel)\n\n\tcbam_feature = Add()([avg_pool,max_pool])\n\tcbam_feature = Activation('sigmoid')(cbam_feature)\n\n\tif K.image_data_format() == \"channels_first\":\n\t\tcbam_feature = Permute((3, 1, 2))(cbam_feature)\n\n\treturn multiply([input_feature, cbam_feature])\n\ndef spatial_attention(input_feature):\n\tkernel_size = 7\n\n\tif K.image_data_format() == \"channels_first\":\n\t\tchannel = input_feature._keras_shape[1]\n\t\tcbam_feature = Permute((2,3,1))(input_feature)\n\telse:\n\t\tchannel = input_feature.shape[-1]\n\t\tcbam_feature = input_feature\n\n\tavg_pool = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(cbam_feature)\n\tassert avg_pool.shape[-1] == 1\n\tmax_pool = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(cbam_feature)\n\tassert max_pool.shape[-1] == 1\n\tconcat = Concatenate(axis=3)([avg_pool, max_pool])\n\tassert concat.shape[-1] == 2\n\tcbam_feature = Conv2D(filters = 1,\n\t\t\t\t\tkernel_size=kernel_size,\n\t\t\t\t\tstrides=1,\n\t\t\t\t\tpadding='same',\n\t\t\t\t\tactivation='sigmoid',\n\t\t\t\t\tkernel_initializer='he_normal',\n\t\t\t\t\tuse_bias=False)(concat)\n\tassert cbam_feature.shape[-1] == 1\n\n\tif K.image_data_format() == \"channels_first\":\n\t\tcbam_feature = Permute((3, 1, 2))(cbam_feature)\n\n\treturn multiply([input_feature, cbam_feature])\n    \ndef UNet(img_input,norm_list):\n    k1 = 32\n    k2 = 64\n    k3 = 128\n    k4 = 256\n    \n    watp1, watp2, watp3 = wat_3(img_input)\n    \n    # Block 1 in Contracting Path\n    conv1 = Conv2D(k1, (3, 3), data_format=IMAGE_ORDERING,padding='same', dilation_rate=1)(img_input)\n    conv1 = BatchNormalization()(conv1)\n    conv1 = Activation(tf.nn.leaky_relu)(conv1)\n    #conv1 = Dropout(0.2)(conv1)\n    conv1 = Conv2D(k1, (3, 3), data_format=IMAGE_ORDERING, padding='same', dilation_rate=1)(conv1)\n    conv1 = BatchNormalization()(conv1)\n    conv1 = Activation(tf.nn.leaky_relu)(conv1)\n\n    conv1 = cbam_block(conv1)    # Convolutional Block Attention Module(CBAM) block\n\n    o = AveragePooling2D((2, 2), strides=(2, 2))(conv1)\n\n    # Block 2 in Contracting Path\n    conv2 = Conv2D(k2, (3, 3), data_format=IMAGE_ORDERING, padding='same', dilation_rate=1)(o)\n    conv2 = BatchNormalization()(conv2)\n    conv2 = Activation(tf.nn.leaky_relu)(conv2)\n#     conv2 = Dropout(0.2)(conv2)\n    conv2 = Conv2D(k2, (3, 3), data_format=IMAGE_ORDERING, padding='same', dilation_rate=1)(conv2)\n    conv2 = BatchNormalization()(conv2)\n    conv2 = Activation(tf.nn.leaky_relu)(conv2)\n    \n    conv2 = wat_layer_1(conv2, watp1)\n\n    conv2 = cbam_block(conv2)    # Convolutional Block Attention Module(CBAM) block\n\n    o = AveragePooling2D((2, 2), strides=(2, 2))(conv2)\n\n    # Block 3 in Contracting Path\n    conv3 = Conv2D(k3, (3, 3), data_format=IMAGE_ORDERING, padding='same', dilation_rate=1)(o)\n    conv3 = BatchNormalization()(conv3)\n    conv3 = Activation(tf.nn.leaky_relu)(conv3)\n    #conv3 = Dropout(0.2)(conv3)\n    conv3 = Conv2D(k3, (3, 3), data_format=IMAGE_ORDERING, padding='same', dilation_rate=1)(conv3)\n    conv3 = BatchNormalization()(conv3)\n    conv3 = Activation(tf.nn.leaky_relu)(conv3)\n\n    conv3 = wat_layer_2(conv3, watp2)\n\n    conv3 = cbam_block(conv3)    # Convolutional Block Attention Module(CBAM) block\n\n    o = AveragePooling2D((2, 2), strides=(2, 2))(conv3)\n\n     # Transition layer between contracting and expansive paths:\n    conv4 = Conv2D(k4, (3, 3), data_format=IMAGE_ORDERING, padding='same', dilation_rate=1)(o)\n    conv4 = BatchNormalization()(conv4)\n    conv4 = Activation(tf.nn.leaky_relu)(conv4)\n    #conv4 = Dropout(0.2)(conv4)\n    conv4 = Conv2D(k4, (3, 3), data_format=IMAGE_ORDERING, padding='same', dilation_rate=1)(conv4)\n    conv4 = BatchNormalization()(conv4)\n    conv4 =Activation(tf.nn.leaky_relu)(conv4)\n\n    conv4 = wat_layer_3(conv4, watp3)\n\n    conv4 = cbam_block(conv4)    # Convolutional Block Attention Module(CBAM) block\n    res1 = Conv2D(k4, (3, 3),padding='same',activation='relu')(conv4)\n    AdaptiveInstanceNorm()(res1,norm_list[:,:,:,:k4], norm_list[:,:,:,k4:2*k4])\n    res1 = Conv2D(k4, (3, 3),padding='same')(res1)\n    res1 += conv4\n    \n    res2 = Conv2D(k4, (3, 3),padding='same',activation='relu')(res1)\n    AdaptiveInstanceNorm()(res2,norm_list[:,:,:,2*k4:3*k4], norm_list[:,:,:,3*k4:4*k4])\n    res2 = Conv2D(k4, (3, 3),padding='same')(res2)\n    res2 += res1\n    \n    res3 = Conv2D(k4, (3, 3),padding='same',activation='relu')(res2)\n    AdaptiveInstanceNorm()(res3,norm_list[:,:,:,4*k4:5*k4], norm_list[:,:,:,4*k4:5*k4])\n    res3 = Conv2D(k4, (3, 3),padding='same')(res3)\n    res3 += res2\n    \n    res4 = Conv2D(k4, (3, 3),padding='same',activation='relu')(res3)\n    AdaptiveInstanceNorm()(res4,norm_list[:,:,:,5*k4:6*k4], norm_list[:,:,:,5*k4:6*k4])\n    res4 = Conv2D(k4, (3, 3),padding='same')(res4)\n    res4 += res3\n    conv4 = res4\n\n    # Block 1 in Expansive Path\n    up1 = UpSampling2D((2, 2), data_format=IMAGE_ORDERING)(conv4)\n    up1 = concatenate([up1, conv3], axis=MERGE_AXIS)\n    deconv1 =  Conv2D(k3, (3, 3), data_format=IMAGE_ORDERING, padding='same', dilation_rate=1)(up1)\n    deconv1 = BatchNormalization()(deconv1)\n    deconv1 = Activation(tf.nn.leaky_relu)(deconv1)\n    #deconv1 = Dropout(0.2)(deconv1)\n    deconv1 =  Conv2D(k3, (3, 3), data_format=IMAGE_ORDERING, padding='same', dilation_rate=1)(deconv1)\n    deconv1 = BatchNormalization()(deconv1)\n    deconv1 = Activation(tf.nn.leaky_relu)(deconv1)\n\n    deconv1 = cbam_block(deconv1)    # Convolutional Block Attention Module(CBAM) block\n\n    # Block 2 in Expansive Path\n    up2 = UpSampling2D((2, 2), data_format=IMAGE_ORDERING)(deconv1)\n    up2 = concatenate([up2, conv2], axis=MERGE_AXIS)\n    deconv2 = Conv2D(k2, (3, 3), data_format=IMAGE_ORDERING, padding='same', dilation_rate=1)(up2)\n    deconv2 = BatchNormalization()(deconv2)\n    deconv2 = Activation(tf.nn.leaky_relu)(deconv2)\n    #deconv2 = Dropout(0.2)(deconv2)\n    deconv2 = Conv2D(k2, (3, 3), data_format=IMAGE_ORDERING, padding='same', dilation_rate=1)(deconv2)\n    deconv2 = BatchNormalization()(deconv2)\n    deconv2 = Activation(tf.nn.leaky_relu)(deconv2)\n\n    deconv2 = cbam_block(deconv2)    # Convolutional Block Attention Module(CBAM) block\n\n    # Block 3 in Expansive Path\n    up3 = UpSampling2D((2, 2), data_format=IMAGE_ORDERING)(deconv2)\n    up3 = concatenate([up3, conv1], axis=MERGE_AXIS)\n    deconv3 = Conv2D(k1, (3, 3), data_format=IMAGE_ORDERING, padding='same', dilation_rate=1)(up3)\n    deconv3 = BatchNormalization()(deconv3)\n    deconv3 = Activation(tf.nn.leaky_relu)(deconv3)\n    #deconv3 = Dropout(0.2)(deconv3)\n    deconv3 = Conv2D(k1, (3, 3), data_format=IMAGE_ORDERING, padding='same', dilation_rate=1)(deconv3)\n    deconv3 = BatchNormalization()(deconv3)\n    deconv3 = Activation(tf.nn.leaky_relu)(deconv3)\n\n    deconv3 = cbam_block(deconv3)    # Convolutional Block Attention Module(CBAM) block\n\n    output = Conv2D(1, (3, 3), data_format=IMAGE_ORDERING, padding='same')(deconv3)\n    # \toutput = Activation('sigmoid')(output)\n    output = Activation('tanh')(output)\n    return output\n\n\ndef Correction_Multi_input(input_height, input_width):\n\tassert input_height % 32 == 0\n\tassert input_width % 32 == 0\n\n#   UNET\n\timg_input_1 = Input(shape=(input_height, input_width, 1))\n\timg_input_2 = Input(shape=(input_height, input_width, 1))\n\timg_input_3 = Input(shape=(input_height, input_width, 1))\n\tnorm_list = Input(shape=(1, 1, 1536))    \n# \tkk = 32\n\tkk = 64\n\tconv1 = Conv2D(kk, (3, 3), data_format=IMAGE_ORDERING,padding='same', dilation_rate=1)(img_input_1) # dilation_rate=6\n\tconv1 = BatchNormalization()(conv1)\n\tconv1 = Activation('relu')(conv1)\n\tconv2 = Conv2D(kk, (3, 3), data_format=IMAGE_ORDERING,padding='same', dilation_rate=1)(img_input_2) # dilation_rate=6\n\tconv2 = BatchNormalization()(conv2)\n\tconv2 = Activation('relu')(conv2)\n\tconv3 = Conv2D(kk, (3, 3), data_format=IMAGE_ORDERING,padding='same', dilation_rate=1)(img_input_3) # dilation_rate=6\n\tconv3 = BatchNormalization()(conv3)\n\tconv3 = Activation('relu')(conv3)\n\n\tinput_concat = concatenate([conv1, conv2, conv3], axis=MERGE_AXIS)  #conv4\n\t# dataset = tf.data.Dataset.from_tensor_slices((img_input_1, img_input_2, img_input_3)\n\n\t## Two Stacked Nets:\n\tpred_1  = UNet(input_concat,norm_list)\n\tinput_2 = concatenate([input_concat, pred_1], axis=MERGE_AXIS)\n\tpred_2  = UNet(input_2,norm_list) #\n\n\tmodel = Model(inputs=[img_input_1,img_input_2,img_input_3,norm_list], outputs=pred_2)\n\n\treturn model\ndef Style_Encoder(style_dim, img_input):\n    initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n    k1, k2, k3 = 32, 64, 128\n    conv_1 = Conv2D(k1, (5, 5), padding=\"same\", strides=1, activation='relu', kernel_initializer=initializer)(img_input)\n    conv_2 = Conv2D(k2, (2, 2), padding=\"same\", strides=2, activation='relu', kernel_initializer=initializer)(conv_1)\n    conv_3 = Conv2D(k3, (2, 2), padding=\"same\", strides=2, activation='relu', kernel_initializer=initializer)(conv_2)\n    conv_4 = Conv2D(k3, (2, 2), padding=\"same\", strides=2, activation='relu', kernel_initializer=initializer)(conv_3)\n    conv_5 = Conv2D(k3, (2, 2), padding=\"same\", strides=2, activation='relu', kernel_initializer=initializer)(conv_4)\n    avg = GlobalAveragePooling2D(keepdims=True)(conv_5)\n    conv_6 = Conv2D(style_dim, (1, 1), padding=\"valid\", strides=1, kernel_initializer=initializer)(avg)\n    return conv_6\n\ndef MLP(input_block, output_dim, dim):\n    layer1 = Dense(dim, activation='relu', use_bias=True)(input_block)\n    layer2 = Dense(dim, activation='relu', use_bias=True)(layer1)\n    layer3 = Dense(output_dim, activation='relu', use_bias=True)(layer2)\n    return layer3","metadata":{"papermill":{"duration":0.068477,"end_time":"2024-05-21T13:30:13.832005","exception":false,"start_time":"2024-05-21T13:30:13.763528","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-21T21:37:09.487297Z","iopub.execute_input":"2024-06-21T21:37:09.487620Z","iopub.status.idle":"2024-06-21T21:37:09.583139Z","shell.execute_reply.started":"2024-06-21T21:37:09.487595Z","shell.execute_reply":"2024-06-21T21:37:09.582183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.models import Model\n\ndef ssim_score(y_true, y_pred):\n    score = tf.image.ssim(\n    y_true,\n    y_pred,\n    max_val=2.0,\n    filter_size=5,\n    filter_sigma=1.5,\n    k1=0.01,\n    k2=0.03,\n    )\n    return score\n\ndef ssim_loss(y_true, y_pred):\n    score = tf.image.ssim(\n    y_true,\n    y_pred,\n    max_val=2.0,\n    filter_size=5,\n    filter_sigma=1.5,\n    k1=0.01,\n    k2=0.03,\n    )\n    \n    loss = (1-score)/2\n    return loss\n\ndef create_center_rectangle_mask(mask_shape, rect_height, rect_width):\n    mask_shape = mask_shape[1:]\n    # Create a mask with a central rectangle of zeros\n    mask = np.ones(mask_shape, dtype=np.float32)\n\n    # Calculate the position of the top-left corner of the rectangle\n    rect_top = (mask_shape[0] - rect_height) // 2\n    rect_left = (mask_shape[1] - rect_width) // 2\n\n    # Update the mask with the rectangle at the center\n    mask[rect_top:rect_top+rect_height, rect_left:rect_left+rect_width] = 0\n\n    # Convert the NumPy array to a TensorFlow tensor\n    mask_tensor = tf.convert_to_tensor(mask, dtype=tf.float32)\n\n    return mask_tensor\n\ndef crop_center_rectangle_mask(tensor, rect_height=128, rect_width=128):\n    mask = create_center_rectangle_mask(tensor.shape, rect_height, rect_height)\n    return tf.multiply(tensor, mask)\n\ndef fft_loss(y_true, y_pred, crop=False):\n    \"\"\"\n    Custom loss function for spatial loss with FFT features.\n\n    Args:\n        y_true: Ground truth image(s).\n        y_pred: Predicted image(s).\n\n    Returns:\n        Normalized reduced spatial loss.\n\n    This function defines a custom loss for training neural networks. It applies a Fourier Transform\n    to the true and predicted images, extracts the real and imaginary parts of the transformed\n    features, and calculates the mean squared error between them. The loss is then normalized and\n    reduced to a single scalar value.\n\n    \"\"\"\n    # Apply Fourier Transform to the true and predicted images\n    true_fft = tf.signal.fft2d(tf.cast(y_true, dtype=tf.complex64))\n    pred_fft = tf.signal.fft2d(tf.cast(y_pred, dtype=tf.complex64))\n\n    # Extract Real & Imaginary parts\n    true_fft_real = tf.math.real(true_fft)\n    true_fft_imag = tf.math.imag(true_fft)\n    pred_fft_real = tf.math.real(pred_fft)\n    pred_fft_imag = tf.math.imag(pred_fft)\n    \n    if crop:\n        # Crop center rectangles for real and imag\n        true_fft_real_cropped = crop_center_rectangle_mask(true_fft_real)\n        true_fft_imag_cropped = crop_center_rectangle_mask(true_fft_imag)\n        pred_fft_real_cropped = crop_center_rectangle_mask(pred_fft_real)\n        pred_fft_imag_cropped = crop_center_rectangle_mask(pred_fft_imag)\n\n        # Calculate Total loss\n        loss_real = tf.reduce_mean(tf.keras.losses.mse(true_fft_real_cropped, pred_fft_real_cropped))\n        loss_imag = tf.reduce_mean(tf.keras.losses.mse(true_fft_imag_cropped, pred_fft_imag_cropped))\n        total_loss = 0.5*(loss_real+loss_imag)\n    else:\n        # Calculate Total loss\n        loss_real = tf.reduce_mean(tf.keras.losses.mse(true_fft_real, pred_fft_real))\n        loss_imag = tf.reduce_mean(tf.keras.losses.mse(true_fft_imag, pred_fft_imag))\n        total_loss = 0.5*(loss_real+loss_imag)\n    \n    return total_loss\n\ndef init_vgg16_model():\n    \"\"\"\n    Initialize a pre-trained VGG16 model for feature extraction.\n\n    Args:\n        perceptual_layer_name: Name of the layer to extract features from.\n\n    Returns:\n        Pre-trained VGG16 model with specified layer for feature extraction.\n\n    This function loads a pre-trained VGG16 model with ImageNet weights and removes the top\n    classification layers. It then extracts the specified layer for feature extraction and\n    freezes the model's layers to prevent further training.\n\n    \"\"\"\n    # Load pre-trained VGG16 model without the top classification layers\n    vgg_model = VGG16(include_top=False, weights='imagenet', input_shape=(256, 256, 3))\n\n    # Extract the specified layer from the VGG16 model\n    perceptual_model_conv1 = Model(inputs=vgg_model.input, outputs=vgg_model.get_layer('block1_conv1').output)\n    perceptual_model_conv2 = Model(inputs=vgg_model.input, outputs=vgg_model.get_layer('block2_conv1').output)\n    perceptual_model_conv3 = Model(inputs=vgg_model.input, outputs=vgg_model.get_layer('block3_conv1').output)\n\n    # Freeze the layers in the perceptual model so they are not trained further\n    for perceptual_model in [perceptual_model_conv1,perceptual_model_conv2,perceptual_model_conv3]:\n        for layer in perceptual_model.layers:\n            layer.trainable = False\n        \n    print(\"VGG16 Model Initialized\")\n    return perceptual_model_conv1, perceptual_model_conv2, perceptual_model_conv3\n\n# Initialize VGG16 model for feature extraction\nperceptual_models = init_vgg16_model()\n\n\ndef perceptual_loss(y_true, y_pred):\n    \"\"\"\n    Custom loss function for perceptual loss.\n\n    Args:\n        y_true: Ground truth image(s).\n        y_pred: Predicted image(s).\n\n    Returns:\n        Normalized reduced perceptual loss.\n\n    This function defines a custom loss for training neural networks. It converts single-channel\n    images to RGB, preprocesses them for VGG16, and extracts features from a specified layer\n    using a pre-trained VGG16 model. It then calculates the mean squared error between the features\n    of the true and predicted images. The loss is normalized and reduced to a single scalar value.\n\n    \"\"\"\n    # Extract perceptual models\n    perceptual_model_conv1, perceptual_model_conv2, perceptual_model_conv3 = perceptual_models\n\n    # Convert single-channel images to RGB\n    y_true_rgb = tf.repeat(y_true, 3, axis=-1)\n    y_pred_rgb = tf.repeat(y_pred, 3, axis=-1)\n\n    # Preprocess images for VGG16\n    y_true_processed = tf.keras.applications.vgg16.preprocess_input(y_true_rgb)\n    y_pred_processed = tf.keras.applications.vgg16.preprocess_input(y_pred_rgb)\n\n    # Extract features from specified layer for true and predicted images\n    features_true_conv1 = perceptual_model_conv1(y_true_processed)\n    features_pred_conv1 = perceptual_model_conv1(y_pred_processed)\n   \n    # Extract features from specified layer for true and predicted images\n    features_true_conv2 = perceptual_model_conv2(y_true_processed)\n    features_pred_conv2 = perceptual_model_conv2(y_pred_processed)\n  \n    # Extract features from specified layer for true and predicted images\n    features_true_conv3 = perceptual_model_conv3(y_true_processed)\n    features_pred_conv3 = perceptual_model_conv3(y_pred_processed)\n    \n    # Calculate L2 loss\n    mse_conv1 = tf.reduce_mean(tf.keras.losses.mse(features_true_conv1, features_pred_conv1))\n    mse_conv2 = tf.reduce_mean(tf.keras.losses.mse(features_true_conv2, features_pred_conv2))\n    mse_conv3 = tf.reduce_mean(tf.keras.losses.mse(features_true_conv3, features_pred_conv3))\n    \n    total_loss = 0.65*mse_conv1 + 0.3*mse_conv2 + 0.05*mse_conv3\n\n    return total_loss\n\ndef psnr(y_true, y_pred):\n    return tf.reduce_mean(tf.image.psnr(y_true, y_pred, max_val=2.0))  # Adjust max_val for data normalized between -1 and 1\ndef l1_loss(y_true, y_pred):\n    return tf.reduce_mean(tf.abs(y_true - y_pred))\ndef l2_loss(y_true, y_pred):\n    \"\"\"\n    Computes the L2 loss between the ground truth and predicted tensors.\n\n    Parameters:\n        y_true (tf.Tensor): Ground truth tensor.\n        y_pred (tf.Tensor): Predicted tensor.\n\n    Returns:\n        tf.Tensor: Normalized L2 loss.\n\n    This function calculates the mean squared error (MSE) between the ground truth\n    and predicted tensors. It then reduces the MSE along the spatial dimensions,\n    typically representing the height and width of the tensors, resulting in a\n    tensor of shape (batch_size,), where each element represents the mean MSE\n    for a single sample in the batch.\n\n    The loss is then normalized using L2 normalization to ensure that it falls\n    within the range of 0 to 1. Finally, the mean of the normalized loss across\n    the batch is computed and returned.\n    \"\"\"\n    mse = tf.keras.losses.mean_squared_error(y_true, y_pred)\n\n    # Reduce on spatial information\n    batch_mse = tf.reduce_mean(mse, axis=(1, 2))\n\n    # Normalize the loss function to be between 0 and 1\n    normalized_loss = tf.nn.l2_normalize(batch_mse, axis=-1)\n    \n    # Compute the mean of the normalized loss across the batch\n    normalized_reduced_loss = tf.reduce_mean(batch_mse)\n\n    return normalized_reduced_loss\ndef total_loss(y_true, y_pred):\n    perceptual = perceptual_loss(y_true, y_pred)\n    ssim = ssim_loss(y_true, y_pred)\n    \n    scaled_perceptual = (perceptual*0.05807468295097351)\n    adjusted_perceptual = (scaled_perceptual+0.009354699403047562)\n    \n    total = (ssim+adjusted_perceptual)/2\n    return total","metadata":{"papermill":{"duration":0.029537,"end_time":"2024-05-21T13:30:13.867773","exception":false,"start_time":"2024-05-21T13:30:13.838236","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-21T21:37:09.585548Z","iopub.execute_input":"2024-06-21T21:37:09.585917Z","iopub.status.idle":"2024-06-21T21:37:10.026500Z","shell.execute_reply.started":"2024-06-21T21:37:09.585885Z","shell.execute_reply":"2024-06-21T21:37:10.025564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, LearningRateScheduler\nimport math\nimport pandas as pd\nfrom tensorflow.keras.models import model_from_json\n\n# Constants\nTRAIN = 1  # True False\nTEST = 0  # True False\nNB_EPOCH = 100\nLEARNING_RATE = 0.001  # 0.001 (default)\nHEIGHT, WIDTH = 256, 256\nPREDICTION_PATH = '/kaggle/working/Prediction'\nWEIGHTS_PATH = '/kaggle/working/Weights'\n\nprint('Reading Data .... ')\ndata_path = \"/kaggle/input/mmmai-regist-data/MR-ART-Regist\"\nsplit_ratio = [0.7, 0.2, 0.1]\n#split_ratio = [0.03, 0.92, 0.03]\nbatch_size = 5\n\ndata_loader = DataLoader(data_path, split_ratio, batch_size)\ndata_loader.split_data()\n\ntrain_dataset = data_loader.generator('train')\ntest_dataset = data_loader.generator('test')\nvalidation_dataset = data_loader.generator('validation')","metadata":{"papermill":{"duration":0.773064,"end_time":"2024-05-21T13:30:14.646616","exception":false,"start_time":"2024-05-21T13:30:13.873552","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-21T21:37:10.027754Z","iopub.execute_input":"2024-06-21T21:37:10.028134Z","iopub.status.idle":"2024-06-21T21:37:10.123401Z","shell.execute_reply.started":"2024-06-21T21:37:10.028101Z","shell.execute_reply":"2024-06-21T21:37:10.122509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resume = False","metadata":{"execution":{"iopub.status.busy":"2024-06-21T21:37:10.124535Z","iopub.execute_input":"2024-06-21T21:37:10.124788Z","iopub.status.idle":"2024-06-21T21:37:10.129125Z","shell.execute_reply.started":"2024-06-21T21:37:10.124766Z","shell.execute_reply":"2024-06-21T21:37:10.128151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import csv\nimport os\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport h5py\nfrom tensorflow.keras.models import load_model\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# epochs = 10\nepoch_results_path = \"/kaggle/working/epoch_results_file.csv\"\nstep_results_path = \"/kaggle/working/step_results_file.csv\"\nmodel_checkpoint_path = \"/kaggle/working/model_checkpoint_{epoch:02d}.h5\"\nresume = False  # Set this according to your needs\nLEARNING_RATE = 0.001  # Replace with your learning rate\nNB_EPOCH = 10  # Replace with the number of epochs\n\n# Define callbacks to save the model after each epoch\ncheckpoint_callback = ModelCheckpoint(\n    filepath=model_checkpoint_path,\n    monitor='val_loss',  # You can change the monitor value based on your requirement\n    save_best_only=True,\n    save_weights_only=False,\n    mode='min',  # You can change the mode value based on your requirement\n    verbose=1\n)\n\nif resume:\n    data_of_model = os.listdir('/kaggle/input/modelstacked-unet-with-style')\n    data_of_model.sort()\n    curr_epoch = int((len(data_of_model) - 2) / 2)\n    full_unet = load_model(f'/kaggle/input/modelstacked-unet-with-style/Wav_Stacked_UNet_epoch_{curr_epoch}.h5')\n    full_style = load_model(f'/kaggle/input/modelstacked-unet-with-style/full_style_epoch_{curr_epoch}.h5')\nelse:\n    full_unet = Correction_Multi_input(HEIGHT, WIDTH)  # Replace with your model initialization\n    img_free = tf.keras.Input(shape=(256, 256, 1))\n    Style_output = Style_Encoder(256, img_free)  # Replace with your model initialization\n    Style_model = tf.keras.Model(inputs=img_free, outputs=Style_output)\n\n    Mlp_output = MLP(Style_output, 1536, 2048)  # Replace with your model initialization\n    Mlp_model = tf.keras.Model(inputs=Style_output, outputs=Mlp_output)\n\n    full_style = tf.keras.Model(inputs=img_free, outputs=Mlp_output)\n    curr_epoch = 0\n\ndef scheduler(epoch):\n    ep = 10\n    if epoch < ep:\n        return LEARNING_RATE\n    else:\n        return LEARNING_RATE * np.exp(0.1 * (ep - epoch))\n\nUnet_optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\nStyle_optimizer = tf.keras.optimizers.Adam(0.0001)\n\ntry:\n    with open(epoch_results_path, 'w', newline='') as epoch_file, open(step_results_path, 'w', newline='') as step_file:\n        epoch_writer = csv.writer(epoch_file)\n        step_writer = csv.writer(step_file)\n        epoch_writer.writerow(['Epoch', 'Loss', 'L1_loss', 'SSIM_Score', 'PSNR', 'MSE', 'SSIM_Loss_val', 'L1_loss_val', 'SSIM_Score_val', 'PSNR_val', 'MSE_val'])\n        step_writer.writerow(['Epoch', 'Step', 'Avg_Loss', 'Avg_L1_loss', 'Avg_SSIM_Score', 'Avg_PSNR'])\n\n        for epoch in range(curr_epoch, NB_EPOCH):\n            print(f'Starting epoch {epoch + 1}')\n            ssim_scores, losses, l1_losses, psnr_values, l2_losses = [], [], [], [], []\n            \n            new_learning_rate = scheduler(epoch)\n            Unet_optimizer.learning_rate.assign(new_learning_rate)\n            \n            for step, ([Slice1, Slice2, Slice3], FreeImage) in enumerate(train_dataset):\n                with tf.GradientTape() as tape:\n                    norm_list_output = full_style(Slice2, training=True)\n                    output1 = full_unet([Slice1, Slice2, Slice3, norm_list_output], training=True)\n                    ssim_scores.append(ssim_score(FreeImage, output1))\n                    loss_value = total_loss(FreeImage, output1)\n                    psnr_values.append(psnr(FreeImage, output1))\n                    losses.append(loss_value)\n                    l2_losses.append(l2_loss(FreeImage, output1))\n\n                Unet_gradients = tape.gradient(loss_value, full_unet.trainable_variables)\n                Unet_optimizer.apply_gradients(zip(Unet_gradients, full_unet.trainable_variables))\n\n                with tf.GradientTape() as tape:\n                    recon_norm_list = full_style(output1)\n                    free_norm_list = full_style(FreeImage)\n                    L1_loss_value = l1_loss(free_norm_list, recon_norm_list)\n                    l1_losses.append(L1_loss_value)\n\n                Style_gradients = tape.gradient(L1_loss_value, full_style.trainable_variables)\n                Style_optimizer.apply_gradients(zip(Style_gradients, full_style.trainable_variables))\n\n                if step % 100 == 0:\n                    avg_loss = float(np.mean(losses))\n                    avg_l1_loss = float(np.mean(l1_losses))\n                    avg_ssim_score = float(np.mean(ssim_scores))\n                    avg_psnr = float(np.mean(psnr_values))\n                    step_writer.writerow([epoch + 1, step, \"{:.4f}\".format(avg_loss), avg_l1_loss, \"{:.4f}\".format(avg_ssim_score), \"{:.4f}\".format(avg_psnr)])\n                    step_file.flush()  # Ensure data is written to file\n                    #plt.subplot(1, 3, 1)\n                    #plt.imshow(FreeImage[2], cmap='gray')\n                    #plt.subplot(1, 3, 2)\n                    #plt.imshow(Slice2[2], cmap='gray')\n                    #plt.subplot(1, 3, 3)\n                    #plt.imshow(output1[2], cmap='gray')\n                    #plt.show()\n                    print(f'Epoch {epoch + 1}, Step {step}, Avg_Loss: {\"{:.4f}\".format(avg_loss)}, Avg_L1_loss: {avg_l1_loss}, Avg_SSIM_Score: {\"{:.4f}\".format(avg_ssim_score)}, Avg_PSNR: {\"{:.4f}\".format(avg_psnr)}, Avg_L2_Loss: {\"{:.4f}\".format(float(np.mean(l2_losses)))}')\n\n            print(f\"================================Epoch {epoch + 1} Validation ===========================================\")\n            ssim_scores_val, losses_val, l1_losses_val, psnr_values_val, l2_losses_val = [], [], [], [], []\n\n            for step, ([Slice1, Slice2, Slice3], FreeImage) in enumerate(validation_dataset):\n                norm_list_val = full_style(Slice2, training=False)\n                output1_val = full_unet([Slice1, Slice2, Slice3, norm_list_val], training=False)\n                ssim_scores_val.append(ssim_score(FreeImage, output1_val))\n                loss_value = total_loss(FreeImage, output1_val)\n                psnr_values_val.append(psnr(FreeImage, output1_val))\n                losses_val.append(loss_value)\n                l2_losses_val.append(l2_loss(FreeImage, output1_val))\n                recon_norm_list_val = full_style(output1_val)\n                L1_loss_value = l1_loss(norm_list_val, recon_norm_list_val)\n                l1_losses_val.append(L1_loss_value)\n\n            avg_loss = float(np.mean(losses))\n            avg_l1_loss = float(np.mean(l1_losses))\n            avg_ssim_score = float(np.mean(ssim_scores))\n            avg_psnr = float(np.mean(psnr_values))\n            avg_loss_val = float(np.mean(losses_val))\n            avg_l1_loss_val = float(np.mean(l1_losses_val))\n            avg_ssim_score_val = float(np.mean(ssim_scores_val))\n            avg_psnr_val = float(np.mean(psnr_values_val))\n            epoch_writer.writerow([epoch + 1, \"{:.4f}\".format(avg_loss), avg_l1_loss, \"{:.4f}\".format(avg_ssim_score), \"{:.4f}\".format(avg_psnr), \"{:.4f}\".format(float(np.mean(l2_losses))), \"{:.4f}\".format(avg_loss_val), avg_l1_loss_val, \"{:.4f}\".format(avg_ssim_score_val), \"{:.4f}\".format(avg_psnr_val), \"{:.4f}\".format(float(np.mean(l2_losses_val)))])\n            epoch_file.flush()  # Ensure data is written to file\n            full_unet.save(f\"/kaggle/working/Wav_Stacked_UNet_epoch_{epoch + 1}.h5\")\n            full_style.save(f\"/kaggle/working/full_style_epoch_{epoch + 1}.h5\")\n\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-21T21:41:33.809430Z","iopub.execute_input":"2024-06-21T21:41:33.809801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import math\n# import pandas as pd\n# import tensorflow as tf\n# from tensorflow.keras.callbacks import CSVLogger, LearningRateScheduler, ModelCheckpoint\n# from tensorflow.keras.optimizers import Adam\n# from tensorflow.keras.models import model_from_json, load_model\n# from tensorflow.keras.utils import plot_model\n\n# def exponential_lr(epoch, LEARNING_RATE):\n#     if epoch < 10:\n#         return LEARNING_RATE\n#     else:\n#         return LEARNING_RATE * math.exp(0.1 * (10 - epoch)) # lr decreases exponentially by a factor of 10\n    \n\n\n# def main():\n#         print('---------------------------------')\n#         print('Model Training ...')\n#         print('---------------------------------')\n        \n#         model = Correction_Multi_input(HEIGHT, WIDTH)\n#         model.summary()\n# #         model = load_model(\"/kaggle/input/stackedunet-regist-final-wavtf-dataset/stacked_model_20_val_loss_0.0661.h5\",\n# #                            custom_objects={'total_loss':total_loss, 'ssim_score': ssim_score, 'psnr':psnr, 'K':K})\n        \n#         csv_logger = CSVLogger(f'{WEIGHTS_PATH}_Loss_Acc.csv', append=True, separator=',')\n#         reduce_lr = LearningRateScheduler(exponential_lr)\n        \n#         model.compile(loss=total_loss, optimizer=Adam(learning_rate=LEARNING_RATE),\n#                       metrics=[ssim_score, 'mse', psnr])\n        \n#         checkpoint_path = '/kaggle/working/stacked_model_{epoch:02d}_val_loss_{val_loss:.4f}.h5'\n#         model_checkpoint = ModelCheckpoint(checkpoint_path,\n#                                    monitor='val_loss',\n#                                    save_best_only=False,\n#                                    save_weights_only=False,\n#                                    mode='min',\n#                                    verbose=1)\n        \n#         hist = model.fit(train_dataset,\n#                          epochs=NB_EPOCH,\n#                          verbose=1,\n#                          validation_data=validation_dataset,\n#                          initial_epoch=20,\n#                          callbacks=[csv_logger, reduce_lr, model_checkpoint])\n\n\n# if __name__ == \"__main__\":\n#     main()","metadata":{"execution":{"iopub.status.busy":"2024-06-21T21:41:25.891535Z","iopub.status.idle":"2024-06-21T21:41:25.891889Z","shell.execute_reply.started":"2024-06-21T21:41:25.891711Z","shell.execute_reply":"2024-06-21T21:41:25.891724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}