{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8456774,"sourceType":"datasetVersion","datasetId":5040362},{"sourceId":8730088,"sourceType":"datasetVersion","datasetId":5239925}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","scrolled":true,"execution":{"iopub.status.busy":"2024-06-19T19:27:47.923701Z","iopub.execute_input":"2024-06-19T19:27:47.924318Z","iopub.status.idle":"2024-06-19T19:27:49.948703Z","shell.execute_reply.started":"2024-06-19T19:27:47.924276Z","shell.execute_reply":"2024-06-19T19:27:49.947767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tensorflow==2.15.0","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-06-19T19:27:49.950568Z","iopub.execute_input":"2024-06-19T19:27:49.950956Z","iopub.status.idle":"2024-06-19T19:28:05.890533Z","shell.execute_reply.started":"2024-06-19T19:27:49.950928Z","shell.execute_reply":"2024-06-19T19:28:05.889646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport subprocess\nimport nibabel as nib\nimport tensorflow as tf\nfrom skimage import exposure\nfrom math import ceil\nimport matplotlib.pyplot as plt\nimport random\nfrom torch import nn\nfrom torch.autograd import Variable\nimport torch\nimport torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2024-06-19T19:28:05.891956Z","iopub.execute_input":"2024-06-19T19:28:05.892269Z","iopub.status.idle":"2024-06-19T19:28:20.645268Z","shell.execute_reply.started":"2024-06-19T19:28:05.892242Z","shell.execute_reply":"2024-06-19T19:28:20.644329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport subprocess\nimport nibabel as nib\nimport tensorflow as tf\nfrom skimage import exposure\nfrom math import ceil\nimport matplotlib.pyplot as plt\nimport random\n\nclass DataLoader:\n    def __init__(self, data_path, split_ratio, batch_size=None):\n        self.data_path = data_path\n        self.split_ratio = split_ratio\n        self.all_subjects = None\n        self.subjects_lists = []\n        self.labels = {'train': 0, 'test': 1, 'validation': 2}\n        self.size = [0, 0, 0]\n        self.batch_size = batch_size\n        self.slices_number = None\n\n    def list_subjects(self):\n        subjects = os.listdir(self.data_path)\n        subjects = [item for item in subjects if item.startswith('sub')]\n        subjects.reverse()\n        self.all_subjects = subjects\n\n    def get_nifti_path(self, subject, number_of_motion='1'):\n        ref_path_stand = f'{self.data_path}/{subject}/anat/{subject}_acq-standard_T1w.nii'\n#         select_path_stand = subprocess.run(['ls', ref_path_stand], capture_output=True, text=True).stdout.replace(\"\\n\", \"\")\n\n        ref_path_motion = f'{self.data_path}/{subject}/anat/{subject}_acq-headmotion{number_of_motion}_T1w.nii'\n#         select_path_motion = subprocess.run(['ls', ref_path_motion], capture_output=True, text=True).stdout.replace(\"\\n\", \"\")\n\n        return [ref_path_stand , ref_path_motion ]\n\n    def get_paired_volumes(self, path,remove_black=True):\n        if os.path.exists(path[0]) and os.path.exists(path[1]):\n            free_data = nib.load(path[0]).get_fdata()\n            \n            motion_data = nib.load(path[1]).get_fdata()\n            if remove_black:\n                free_data = exposure.rescale_intensity(free_data[37:-37], out_range=(-1.0, 1.0))\n                motion_data = exposure.rescale_intensity(motion_data[37:-37], out_range=(-1.0, 1.0))\n            else:\n                free_data = exposure.rescale_intensity(free_data, out_range=(-1.0, 1.0))\n                motion_data = exposure.rescale_intensity(motion_data, out_range=(-1.0, 1.0))\n            return tf.convert_to_tensor(free_data), tf.convert_to_tensor(motion_data)\n        else:\n            return None, None\n\n    def split_data(self):\n        self.list_subjects()\n        if ceil(sum(self.split_ratio)) == 1 and len(self.split_ratio) <= 3:\n            self.split_ratio.insert(0, 0)\n            cumulative_sum = [sum(self.split_ratio[:i + 1]) for i in range(len(self.split_ratio))]\n            number_of_subjects = len(self.all_subjects)\n\n            for i in range(1, len(self.split_ratio)):\n                self.subjects_lists.append(\n                    self.all_subjects[int(round(cumulative_sum[i - 1] * number_of_subjects)):int(\n                        round(cumulative_sum[i] * number_of_subjects))])\n\n                self.size[i - 1] = len(self.subjects_lists[i - 1])  * 2 * 190\n\n                if i - 1 == 0:\n                    self.size[i - 1] -= 8  * 2 * 190\n        else:\n            print(\"The Summation of ratios is not equal to 1\")\n       \n    def generator(self, mode,remove_black=True):\n        subjects = self.subjects_lists[self.labels[mode]]\n\n        def data_gen():\n            for subject in subjects:\n                for i in range(2):\n                    pathes = self.get_nifti_path(subject, str(i + 1))\n                    free, motion = self.get_paired_volumes(pathes,remove_black)\n                    if motion is not None:\n                        self.slices_number = motion.shape[0]\n\n                        for slice_id in range(0, self.slices_number):\n                            start_idx = slice_id + 1\n                            end_idx = (slice_id + 1) + 1\n                            if (end_idx < self.slices_number-1):\n                                free_slice = free[start_idx:end_idx]\n                                free_slice = tf.transpose(free_slice, perm=[1, 2, 0])\n                                \n                                motion_slice = motion[start_idx:end_idx]\n                                motion_slice = tf.transpose(motion_slice, perm=[1, 2, 0])\n                                \n                                motion_before_slice = motion[start_idx-1:end_idx-1]\n                                motion_before_slice = tf.transpose(motion_before_slice, perm=[1, 2, 0])\n                                \n                                motion_after_slice = motion[start_idx+1:end_idx+1]\n                                motion_after_slice = tf.transpose(motion_after_slice, perm=[1, 2, 0])\n\n                                yield (\n                                (motion_before_slice, motion_slice, motion_after_slice),\n                                free_slice\n                                )\n\n        input_signature = (\n            (tf.TensorSpec(shape=(256, 256, 1), dtype=tf.float32),\n             tf.TensorSpec(shape=(256, 256, 1), dtype=tf.float32),\n             tf.TensorSpec(shape=(256, 256, 1), dtype=tf.float32)),\n            tf.TensorSpec(shape=(256, 256, 1), dtype=tf.float32)\n        )\n\n        dataset = tf.data.Dataset.from_generator(data_gen, output_signature=input_signature)\n        dataset = dataset.batch(self.batch_size)\n\n        return dataset","metadata":{"execution":{"iopub.status.busy":"2024-06-19T19:28:20.647432Z","iopub.execute_input":"2024-06-19T19:28:20.647991Z","iopub.status.idle":"2024-06-19T19:28:20.673249Z","shell.execute_reply.started":"2024-06-19T19:28:20.647964Z","shell.execute_reply":"2024-06-19T19:28:20.672411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, LearningRateScheduler\nimport math\nimport pandas as pd\nfrom tensorflow.keras.models import model_from_json\n\n# Constants\nTRAIN = 1  # True False\nTEST = 0  # True False\nNB_EPOCH = 50\nLEARNING_RATE = 0.001  # 0.001 (default)\nHEIGHT, WIDTH = 256, 256\nPREDICTION_PATH = '/kaggle/working/Prediction'\nWEIGHTS_PATH = '/kaggle/working/Weights'\n\nprint('Reading Data ... ')\ndata_path = \"/kaggle/input/mmmai-regist-data/MR-ART-Regist\"\nsplit_ratio = [0.7, 0.2, 0.1]\n# split_ratio = [0.01, 0.98, 0.01]\nbatch_size = 5\n\ndata_loader = DataLoader(data_path, split_ratio, batch_size)\ndata_loader.split_data()\n\ntrain_dataset = data_loader.generator('train')\ntest_dataset = data_loader.generator('test',False)\nvalidation_dataset = data_loader.generator('validation')","metadata":{"execution":{"iopub.status.busy":"2024-06-19T19:28:20.674362Z","iopub.execute_input":"2024-06-19T19:28:20.674625Z","iopub.status.idle":"2024-06-19T19:28:21.295641Z","shell.execute_reply.started":"2024-06-19T19:28:20.674602Z","shell.execute_reply":"2024-06-19T19:28:21.294890Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.losses import MeanSquaredError\nimport numpy as np\n\nIMAGE_ORDERING = 'channels_last'\nMERGE_AXIS = -1\n\ndef cbam_block(cbam_feature, ratio=8):\n    cbam_feature = channel_attention(cbam_feature, ratio)\n    cbam_feature = spatial_attention(cbam_feature)\n    return cbam_feature\n\ndef channel_attention(input_feature, ratio=8):\n    channel_axis = -1\n    channel = input_feature.shape[channel_axis]\n\n    shared_layer_one = Dense(channel//ratio, activation='relu', kernel_initializer='he_normal', use_bias=True, bias_initializer='zeros')\n    shared_layer_two = Dense(channel, kernel_initializer='he_normal', use_bias=True, bias_initializer='zeros')\n\n    avg_pool = GlobalAveragePooling2D()(input_feature)\n    avg_pool = Reshape((1,1,channel))(avg_pool)\n    avg_pool = shared_layer_one(avg_pool)\n    avg_pool = shared_layer_two(avg_pool)\n\n    max_pool = GlobalMaxPooling2D()(input_feature)\n    max_pool = Reshape((1,1,channel))(max_pool)\n    max_pool = shared_layer_one(max_pool)\n    max_pool = shared_layer_two(max_pool)\n\n    cbam_feature = Add()([avg_pool,max_pool])\n    cbam_feature = Activation('sigmoid')(cbam_feature)\n    \n    cbam_feature = 1-cbam_feature\n    \n    return multiply([input_feature, cbam_feature])\ndef spatial_attention(input_feature):\n    kernel_size = 7\n\n    avg_pool = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(input_feature)\n    max_pool = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(input_feature)\n    concat = Concatenate(axis=3)([avg_pool, max_pool])\n    cbam_feature = Conv2D(filters = 1, kernel_size=kernel_size, strides=1, padding='same', activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(concat)\n\n    return multiply([input_feature, cbam_feature])\ndef expand_moments_dim(moment):\n    return tf.reshape(moment, [-1, 1, 1, tf.shape(moment)[-1]])","metadata":{"execution":{"iopub.status.busy":"2024-06-19T19:28:21.296848Z","iopub.execute_input":"2024-06-19T19:28:21.297150Z","iopub.status.idle":"2024-06-19T19:28:21.310852Z","shell.execute_reply.started":"2024-06-19T19:28:21.297124Z","shell.execute_reply":"2024-06-19T19:28:21.310005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.models import Model\ndef ms_ssim_score(y_true, y_pred):\n    score = tf.reduce_mean(tf.image.ssim_multiscale(y_true, y_pred, 2.0))\n    return score\ndef ms_ssim_loss(y_true, y_pred):\n    loss_ssim = 1.0 - tf.reduce_mean(tf.image.ssim_multiscale(y_true, y_pred, 2.0))\n    return loss_ssim\ndef ssim_score(y_true, y_pred):\n    score = tf.reduce_mean(tf.image.ssim(y_true, y_pred, 2.0,filter_size=5,\n                          filter_sigma=1.5))\n    return score\n\ndef ssim_loss(y_true, y_pred):\n    loss_ssim = 1.0 - tf.reduce_mean(tf.image.ssim(y_true, y_pred, 2.0,filter_size=5,\n                          filter_sigma=1.5))\n    return loss_ssim\ndef l1_loss(y_true, y_pred):\n    return tf.reduce_mean(tf.abs(y_true - y_pred))\ndef l2_loss(y_true, y_pred):\n    \"\"\"\n    Computes the L2 loss between the ground truth and predicted tensors.\n\n    Parameters:\n        y_true (tf.Tensor): Ground truth tensor.\n        y_pred (tf.Tensor): Predicted tensor.\n\n    Returns:\n        tf.Tensor: Normalized L2 loss.\n\n    This function calculates the mean squared error (MSE) between the ground truth\n    and predicted tensors. It then reduces the MSE along the spatial dimensions,\n    typically representing the height and width of the tensors, resulting in a\n    tensor of shape (batch_size,), where each element represents the mean MSE\n    for a single sample in the batch.\n\n    The loss is then normalized using L2 normalization to ensure that it falls\n    within the range of 0 to 1. Finally, the mean of the normalized loss across\n    the batch is computed and returned.\n    \"\"\"\n    mse = tf.keras.losses.mean_squared_error(y_true, y_pred)\n\n    # Reduce on spatial information\n    batch_mse = tf.reduce_mean(mse, axis=(1, 2))\n\n    # Normalize the loss function to be between 0 and 1\n    normalized_loss = tf.nn.l2_normalize(batch_mse, axis=-1)\n    \n    # Compute the mean of the normalized loss across the batch\n    normalized_reduced_loss = tf.reduce_mean(batch_mse)\n\n    return normalized_reduced_loss\n\ndef spatial_fft_loss(y_true, y_pred):\n    \"\"\"\n    Custom loss function for spatial loss with FFT features.\n\n    Args:\n        y_true: Ground truth image(s).\n        y_pred: Predicted image(s).\n\n    Returns:\n        Normalized reduced spatial loss.\n\n    This function defines a custom loss for training neural networks. It applies a Fourier Transform\n    to the true and predicted images, extracts the real and imaginary parts of the transformed\n    features, and calculates the mean squared error between them. The loss is then normalized and\n    reduced to a single scalar value.\n\n    \"\"\"\n    # Apply Fourier Transform to the true and predicted images\n    true_fft = tf.signal.fft2d(tf.cast(y_true, dtype=tf.complex64))\n    pred_fft = tf.signal.fft2d(tf.cast(y_pred, dtype=tf.complex64))\n\n    # Extract Real & Imaginary parts\n    true_fft_real = tf.math.real(true_fft)\n    true_fft_imag = tf.math.imag(true_fft)\n    pred_fft_real = tf.math.real(pred_fft)\n    pred_fft_imag = tf.math.imag(pred_fft)\n\n    # Crop center rectangles for real and imag\n    true_fft_real_cropped = crop_center_rectangle_mask(true_fft_real)\n    true_fft_imag_cropped = crop_center_rectangle_mask(true_fft_imag)\n    pred_fft_real_cropped = crop_center_rectangle_mask(pred_fft_real)\n    pred_fft_imag_cropped = crop_center_rectangle_mask(pred_fft_imag)\n\n    # Calculate L2 loss\n    mse_real = tf.keras.losses.mean_squared_error(true_fft_real_cropped, pred_fft_real_cropped)\n    mse_imag = tf.keras.losses.mean_squared_error(true_fft_imag_cropped, pred_fft_imag_cropped)\n\n    # Total L2 loss\n    total_loss = 0.5 * (mse_real + mse_imag)\n    \n    # Reduce on spatial information\n    batch_loss = tf.reduce_mean(total_loss, axis=(1, 2))\n    \n    # Normalize the loss function to be between 0 and 1\n    normalized_loss = tf.nn.l2_normalize(batch_loss, axis=-1)\n\n    normalized_reduced_loss = tf.reduce_mean(normalized_loss)\n\n    return normalized_reduced_loss\n\nimport tensorflow as tf\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.models import Model\n\ndef init_vgg16_model():\n    \"\"\"\n    Initialize a pre-trained VGG16 model for feature extraction.\n\n    Args:\n        perceptual_layer_name: Name of the layer to extract features from.\n\n    Returns:\n        Pre-trained VGG16 model with specified layer for feature extraction.\n\n    This function loads a pre-trained VGG16 model with ImageNet weights and removes the top\n    classification layers. It then extracts the specified layer for feature extraction and\n    freezes the model's layers to prevent further training.\n\n    \"\"\"\n    # Load pre-trained VGG16 model without the top classification layers\n    vgg_model = VGG16(include_top=False, weights='imagenet', input_shape=(256, 256, 3))\n\n    # Extract the specified layer from the VGG16 model\n    perceptual_model_conv1 = Model(inputs=vgg_model.input, outputs=vgg_model.get_layer('block1_conv1').output)\n    perceptual_model_conv2 = Model(inputs=vgg_model.input, outputs=vgg_model.get_layer('block2_conv1').output)\n    perceptual_model_conv3 = Model(inputs=vgg_model.input, outputs=vgg_model.get_layer('block3_conv1').output)\n\n    # Freeze the layers in the perceptual model so they are not trained further\n    for perceptual_model in [perceptual_model_conv1,perceptual_model_conv2,perceptual_model_conv3]:\n        for layer in perceptual_model.layers:\n            layer.trainable = False\n        \n    print(\"VGG16 Model Initialized\")\n    return perceptual_model_conv1, perceptual_model_conv2, perceptual_model_conv3\n\n# Initialize VGG16 model for feature extraction\nperceptual_models = init_vgg16_model()\n\ndef perceptual_loss(y_true, y_pred):\n    \"\"\"\n    Custom loss function for perceptual loss.\n\n    Args:\n        y_true: Ground truth image(s).\n        y_pred: Predicted image(s).\n\n    Returns:\n        Normalized reduced perceptual loss.\n\n    This function defines a custom loss for training neural networks. It converts single-channel\n    images to RGB, preprocesses them for VGG16, and extracts features from a specified layer\n    using a pre-trained VGG16 model. It then calculates the mean squared error between the features\n    of the true and predicted images. The loss is normalized and reduced to a single scalar value.\n\n    \"\"\"\n    # Extract perceptual models\n    perceptual_model_conv1, perceptual_model_conv2, perceptual_model_conv3 = perceptual_models\n\n    # Convert single-channel images to RGB\n    y_true_rgb = tf.repeat(y_true, 3, axis=-1)\n    y_pred_rgb = tf.repeat(y_pred, 3, axis=-1)\n\n    # Preprocess images for VGG16\n    y_true_processed = tf.keras.applications.vgg16.preprocess_input(y_true_rgb)\n    y_pred_processed = tf.keras.applications.vgg16.preprocess_input(y_pred_rgb)\n\n    # Extract features from specified layer for true and predicted images\n    features_true_conv1 = perceptual_model_conv1(y_true_processed)\n    features_pred_conv1 = perceptual_model_conv1(y_pred_processed)\n   \n    # Extract features from specified layer for true and predicted images\n    features_true_conv2 = perceptual_model_conv2(y_true_processed)\n    features_pred_conv2 = perceptual_model_conv2(y_pred_processed)\n  \n    # Extract features from specified layer for true and predicted images\n    features_true_conv3 = perceptual_model_conv3(y_true_processed)\n    features_pred_conv3 = perceptual_model_conv3(y_pred_processed)\n    \n    # Calculate L2 loss\n    mse_conv1 = tf.reduce_mean(tf.keras.losses.mse(features_true_conv1, features_pred_conv1))\n    mse_conv2 = tf.reduce_mean(tf.keras.losses.mse(features_true_conv2, features_pred_conv2))\n    mse_conv3 = tf.reduce_mean(tf.keras.losses.mse(features_true_conv3, features_pred_conv3))\n    \n    total_loss = 0.65*mse_conv1 + 0.3*mse_conv2 + 0.05*mse_conv3\n\n    return total_loss\n\n\ndef psnr(y_true, y_pred):\n    return tf.reduce_mean(tf.image.psnr(y_true, y_pred, max_val=2.0))  # Adjust max_val for data normalized between -1 and 1\n\ndef total_loss(y_true, y_pred):\n    perceptual = perceptual_loss(y_true, y_pred)\n    ssim = ssim_loss(y_true, y_pred)\n    \n    scaled_perceptual = (perceptual*0.05807468295097351)\n    adjusted_perceptual = (scaled_perceptual+0.009354699403047562)\n    \n    total = (ssim+adjusted_perceptual)/2\n    return total\n","metadata":{"execution":{"iopub.status.busy":"2024-06-19T19:28:21.312082Z","iopub.execute_input":"2024-06-19T19:28:21.312400Z","iopub.status.idle":"2024-06-19T19:28:24.929464Z","shell.execute_reply.started":"2024-06-19T19:28:21.312370Z","shell.execute_reply":"2024-06-19T19:28:24.928540Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import *\nimport math\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import CSVLogger, LearningRateScheduler, ModelCheckpoint\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import model_from_json\nLEARNING_RATE = 0.001\ndef UNet(img_input,norm_list):\n    k1 = 32\n    k2 = 64\n    k3 = 128\n    k4 = 256\n    # Block 1 in Contracting Path\n    conv1 = Conv2D(k1, (3, 3), data_format=IMAGE_ORDERING,padding='same', dilation_rate=1)(img_input)\n    conv1 = BatchNormalization()(conv1)\n    conv1 = Activation(tf.nn.leaky_relu)(conv1)\n    #conv1 = Dropout(0.2)(conv1)\n    conv1 = Conv2D(k1, (3, 3), data_format=IMAGE_ORDERING, padding='same', dilation_rate=1)(conv1)\n    conv1 = BatchNormalization()(conv1)\n    conv1 = Activation(tf.nn.leaky_relu)(conv1)\n\n    conv1 = cbam_block(conv1)    # Convolutional Block Attention Module(CBAM) block\n\n    o = AveragePooling2D((2, 2), strides=(2, 2))(conv1)\n\n    # Block 2 in Contracting Path\n    conv2 = Conv2D(k2, (3, 3), data_format=IMAGE_ORDERING, padding='same', dilation_rate=1)(o)\n    conv2 = BatchNormalization()(conv2)\n    conv2 = Activation(tf.nn.leaky_relu)(conv2)\n    conv2 = Dropout(0.2)(conv2)\n    conv2 = Conv2D(k2, (3, 3), data_format=IMAGE_ORDERING, padding='same', dilation_rate=1)(conv2)\n    conv2 = BatchNormalization()(conv2)\n    conv2 = Activation(tf.nn.leaky_relu)(conv2)\n\n    conv2 = cbam_block(conv2)    # Convolutional Block Attention Module(CBAM) block\n\n    o = AveragePooling2D((2, 2), strides=(2, 2))(conv2)\n\n    # Block 3 in Contracting Path\n    conv3 = Conv2D(k3, (3, 3), data_format=IMAGE_ORDERING, padding='same', dilation_rate=1)(o)\n    conv3 = BatchNormalization()(conv3)\n    conv3 = Activation(tf.nn.leaky_relu)(conv3)\n    #conv3 = Dropout(0.2)(conv3)\n    conv3 = Conv2D(k3, (3, 3), data_format=IMAGE_ORDERING, padding='same', dilation_rate=1)(conv3)\n    conv3 = BatchNormalization()(conv3)\n    conv3 = Activation(tf.nn.leaky_relu)(conv3)\n\n    conv3 = cbam_block(conv3)    # Convolutional Block Attention Module(CBAM) block\n\n    o = AveragePooling2D((2, 2), strides=(2, 2))(conv3)\n\n     # Transition layer between contracting and expansive paths:\n    conv4 = Conv2D(k4, (3, 3), data_format=IMAGE_ORDERING, padding='same', dilation_rate=1)(o)\n    conv4 = BatchNormalization()(conv4)\n    conv4 = Activation(tf.nn.leaky_relu)(conv4)\n    #conv4 = Dropout(0.2)(conv4)\n    conv4 = Conv2D(k4, (3, 3), data_format=IMAGE_ORDERING, padding='same', dilation_rate=1)(conv4)\n    conv4 = BatchNormalization()(conv4)\n    conv4 =Activation(tf.nn.leaky_relu)(conv4)\n\n    conv4 = cbam_block(conv4)    # Convolutional Block Attention Module(CBAM) block\n    res1 = Conv2D(k4, (3, 3),padding='same',activation='relu')(conv4)\n    AdaptiveInstanceNorm()(res1,norm_list[:,:,:,:k4], norm_list[:,:,:,k4:2*k4])\n    res1 = Conv2D(k4, (3, 3),padding='same')(res1)\n# AdaptiveInstanceNorm()(res1,norm_list[:,:,:,:k4], norm_list[:,:,:,k4:2*k4])\n    res1 += conv4\n    \n    res2 = Conv2D(k4, (3, 3),padding='same',activation='relu')(res1)\n    AdaptiveInstanceNorm()(res2,norm_list[:,:,:,2*k4:3*k4], norm_list[:,:,:,3*k4:4*k4])\n    res2 = Conv2D(k4, (3, 3),padding='same')(res2)\n# \tAdaptiveInstanceNorm()(res2,norm_list[:,:,:,2*k4:3*k4], norm_list[:,:,:,3*k4:4*k4])\n    res2 += res1\n    \n    res3 = Conv2D(k4, (3, 3),padding='same',activation='relu')(res2)\n    AdaptiveInstanceNorm()(res3,norm_list[:,:,:,4*k4:5*k4], norm_list[:,:,:,4*k4:5*k4])\n    res3 = Conv2D(k4, (3, 3),padding='same')(res3)\n# \tAdaptiveInstanceNorm()(res3,norm_list[:,:,:,4*k4:5*k4], norm_list[:,:,:,4*k4:5*k4])\n    res3 += res2\n    \n    res4 = Conv2D(k4, (3, 3),padding='same',activation='relu')(res3)\n    AdaptiveInstanceNorm()(res4,norm_list[:,:,:,5*k4:6*k4], norm_list[:,:,:,5*k4:6*k4])\n    res4 = Conv2D(k4, (3, 3),padding='same')(res4)\n# \tAdaptiveInstanceNorm()(res4,norm_list[:,:,:,5*k4:6*k4], norm_list[:,:,:,5*k4:6*k4])\n    res4 += res3\n    conv4 = res4\n    \n    \n\t# Block 1 in Expansive Path\n    up1 = UpSampling2D((2, 2), data_format=IMAGE_ORDERING)(conv4)\n    up1 = concatenate([up1, conv3], axis=MERGE_AXIS)\n    deconv1 =  Conv2D(k3, (3, 3), data_format=IMAGE_ORDERING, padding='same', dilation_rate=1)(up1)\n    deconv1 = BatchNormalization()(deconv1)\n# \tdeconv1 = AdaptiveInstanceNorm()(deconv1,norm_list[:,:,:,:128], norm_list[:,:,:,128:256])\n    deconv1 = Activation(tf.nn.leaky_relu)(deconv1)\n\t#deconv1 = Dropout(0.2)(deconv1)\n    deconv1 =  Conv2D(k3, (3, 3), data_format=IMAGE_ORDERING, padding='same', dilation_rate=1)(deconv1)\n    deconv1 = BatchNormalization()(deconv1)\n# \tdeconv1 = AdaptiveInstanceNorm()(deconv1,norm_list[:,:,:,:128], norm_list[:,:,:,128:256])\n    deconv1 = Activation(tf.nn.leaky_relu)(deconv1)\n\n    deconv1 = cbam_block(deconv1)    # Convolutional Block Attention Module(CBAM) block\n\n\t# Block 2 in Expansive Path\n    up2 = UpSampling2D((2, 2), data_format=IMAGE_ORDERING)(deconv1)\n    up2 = concatenate([up2, conv2], axis=MERGE_AXIS)\n    deconv2 = Conv2D(k2, (3, 3), data_format=IMAGE_ORDERING, padding='same', dilation_rate=1)(up2)\n    deconv2 = BatchNormalization()(deconv2)\n# \tdeconv2 = AdaptiveInstanceNorm()(deconv2,norm_list[:,:,:,256:320], norm_list[:,:,:,320:384])\n    deconv2 = Activation(tf.nn.leaky_relu)(deconv2)\n\t#deconv2 = Dropout(0.2)(deconv2)\n    deconv2 = Conv2D(k2, (3, 3), data_format=IMAGE_ORDERING, padding='same', dilation_rate=1)(deconv2)\n    deconv2 = BatchNormalization()(deconv2)\n# \tdeconv2 = AdaptiveInstanceNorm()(deconv2,norm_list[:,:,:,256:320], norm_list[:,:,:,320:384])\n    deconv2 = Activation(tf.nn.leaky_relu)(deconv2)\n\n    deconv2 = cbam_block(deconv2)    # Convolutional Block Attention Module(CBAM) block\n\n\t# Block 3 in Expansive Path\n    up3 = UpSampling2D((2, 2), data_format=IMAGE_ORDERING)(deconv2)\n    up3 = concatenate([up3, conv1], axis=MERGE_AXIS)\n    deconv3 = Conv2D(k1, (3, 3), data_format=IMAGE_ORDERING, padding='same', dilation_rate=1)(up3)\n    deconv3 = BatchNormalization()(deconv3)\n# deconv3 = AdaptiveInstanceNorm()(deconv3,norm_list[:,:,:,384:416], norm_list[:,:,:,416:448])\n    deconv3 = Activation(tf.nn.leaky_relu)(deconv3)\n    #deconv3 = Dropout(0.2)(deconv3)\n    deconv3 = Conv2D(k1, (3, 3), data_format=IMAGE_ORDERING, padding='same', dilation_rate=1)(deconv3)\n    deconv3 = BatchNormalization()(deconv3)\n#  deconv3 = AdaptiveInstanceNorm()(deconv3,norm_list[:,:,:,384:416], norm_list[:,:,:,416:448])\n    deconv3 = Activation(tf.nn.leaky_relu)(deconv3)\n\n    deconv3 = cbam_block(deconv3)    # Convolutional Block Attention Module(CBAM) block\n\n    output = Conv2D(1, (3, 3), data_format=IMAGE_ORDERING, padding='same')(deconv3)\n# \toutput = Activation('sigmoid')(output)\n    output = Activation('tanh')(output)\n    return output\nclass AdaptiveInstanceNorm(Layer):\n    def __init__(self, epsilon=1e-5, **kwargs):\n        super(AdaptiveInstanceNorm, self).__init__(**kwargs)\n        self.epsilon = epsilon\n\n    def call(self, content, gamma, beta):\n        c_mean, c_var = tf.nn.moments(content, axes=[1, 2], keepdims=True)\n        c_std = tf.sqrt(c_var + self.epsilon)\n        normalized = (content - c_mean) / c_std\n        gamma = expand_moments_dim(gamma)\n        beta = expand_moments_dim(beta)\n        return multiply([gamma, normalized]) + beta\n\n    def get_config(self):\n        config = super(AdaptiveInstanceNorm, self).get_config()\n        config.update({\"epsilon\": self.epsilon})\n    \ndef Style_Encoder(style_dim, img_input):\n    initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n    k1, k2, k3 = 32, 64, 128\n    conv_1 = Conv2D(k1, (7, 7), padding=\"same\", strides=1, activation='relu', kernel_initializer=initializer)(img_input)\n    conv_2 = Conv2D(k2, (4, 4), padding=\"same\", strides=2, activation='relu', kernel_initializer=initializer)(conv_1)\n    conv_3 = Conv2D(k3, (4, 4), padding=\"same\", strides=2, activation='relu', kernel_initializer=initializer)(conv_2)\n    conv_4 = Conv2D(k3, (4, 4), padding=\"same\", strides=2, activation='relu', kernel_initializer=initializer)(conv_3)\n    conv_5 = Conv2D(k3, (4, 4), padding=\"same\", strides=2, activation='relu', kernel_initializer=initializer)(conv_4)\n    avg = GlobalAveragePooling2D(keepdims=True)(conv_5)\n    conv_6 = Conv2D(style_dim, (1, 1), padding=\"valid\", strides=1, kernel_initializer=initializer)(avg)\n    return conv_6\n\ndef MLP(input_block, output_dim, dim):\n    layer1 = Dense(dim, activation='relu', use_bias=True)(input_block)\n    layer2 = Dense(dim, activation='relu', use_bias=True)(layer1)\n    layer3 = Dense(output_dim, activation='relu', use_bias=True)(layer2)\n    return layer3\ndef Feature_extractor(img_input_1, img_input_2,img_input_3):\n#     assert input_height % 32 == 0\n#     assert input_width % 32 == 0\n\n# #   UNET\n#     img_input_1 = Input(shape=(input_height, input_width, 1))\n#     img_input_2 = Input(shape=(input_height, input_width, 1))\n#     img_input_3 = Input(shape=(input_height, input_width, 1))\n# \tkk = 32\n    kk = 64\n    conv1 = Conv2D(kk, (3, 3), data_format=IMAGE_ORDERING,padding='same', dilation_rate=1)(img_input_1) # dilation_rate=6\n    conv1 = BatchNormalization()(conv1)\n    conv1 = Activation('relu')(conv1)\n    conv2 = Conv2D(kk, (3, 3), data_format=IMAGE_ORDERING,padding='same', dilation_rate=1)(img_input_2) # dilation_rate=6\n    conv2 = BatchNormalization()(conv2)\n    conv2 = Activation('relu')(conv2)\n    conv3 = Conv2D(kk, (3, 3), data_format=IMAGE_ORDERING,padding='same', dilation_rate=1)(img_input_3) # dilation_rate=6\n    conv3 = BatchNormalization()(conv3)\n    conv3 = Activation('relu')(conv3)\n\n    input_concat = concatenate([conv1, conv2, conv3], axis=MERGE_AXIS)  #conv4\n    return input_concat\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-19T19:28:24.930779Z","iopub.execute_input":"2024-06-19T19:28:24.931054Z","iopub.status.idle":"2024-06-19T19:28:24.973959Z","shell.execute_reply.started":"2024-06-19T19:28:24.931029Z","shell.execute_reply":"2024-06-19T19:28:24.973052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resume = True","metadata":{"execution":{"iopub.status.busy":"2024-06-19T19:28:24.975348Z","iopub.execute_input":"2024-06-19T19:28:24.975708Z","iopub.status.idle":"2024-06-19T19:28:24.997244Z","shell.execute_reply.started":"2024-06-19T19:28:24.975684Z","shell.execute_reply":"2024-06-19T19:28:24.996413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import csv\nimport os\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport h5py\nfrom tensorflow.keras.models import load_model\n# epochs = 10\nepoch_results_path = \"/kaggle/working/epoch_results_file.csv\"\nstep_results_path = \"/kaggle/working/step_results_file.csv\"\nmodel_checkpoint_path = \"/kaggle/working/model_checkpoint_{epoch:02d}.h5\"\n# Define callbacks to save the model after each epoch\ncheckpoint_callback = ModelCheckpoint(\n    filepath=model_checkpoint_path,\n    monitor='val_loss',  # You can change the monitor value based on your requirement\n    save_best_only=True,\n    save_weights_only=False,\n    mode='min',  # You can change the mode value based on your requirement\n    verbose=1\n)\nif resume:\n    data_of_model = os.listdir('/kaggle/input/modelstacked-unet-with-style')\n    data_of_model.sort()\n    curr_epoch = int((len(data_of_model)-2)/2)\n    full_unet = load_model(f'/kaggle/input/modelstacked-unet-with-style/full_unet_epoch_{curr_epoch}.h5')\n    full_style = load_model(f'/kaggle/input/modelstacked-unet-with-style/full_style_epoch_{curr_epoch}.h5')\n# feature_input = Input(shape=(256, 256, 192))\nelse:\n    norm_list = Input(shape=(1, 1, 1536))  # Adjust the shape as necessary\n    img_free = Input(shape=(256, 256, 1))\n    img_input1 = Input(shape=(256, 256, 1))\n    img_input2 = Input(shape=(256, 256, 1))\n    img_input3 = Input(shape=(256, 256, 1))\n# Create the U-Net model\n    Feature_output = Feature_extractor(img_input1,img_input2,img_input3)\n    Feature_model = Model(inputs=[img_input1,img_input2,img_input3], outputs=Feature_output)\n\n    Unet_output = UNet(Feature_output, norm_list)\n# Unet_model = Model(inputs=[Feature_output, norm_list], outputs=Unet_output)\n\n    input_2 = concatenate([Feature_output, Unet_output], axis=MERGE_AXIS)\n\n    Unet_output2 = UNet(input_2, norm_list)\n# Unet_model2 = Model(inputs=[input_2, norm_list], outputs=Unet_output2)\n\n    full_unet = Model(inputs=[img_input1,img_input2,img_input3,norm_list], outputs=Unet_output2)\n\n# full_unet.summary()\n\n    Style_output = Style_Encoder(256,img_free)\n    Style_model = Model(inputs=img_free, outputs=Style_output)\n\n    Mlp_output = MLP(Style_output,1536,2048)\n    Mlp_model = Model(inputs=Style_output, outputs=Mlp_output)\n\n    full_style = Model(inputs=img_free, outputs=Mlp_output)\n    curr_epoch = 0\ndef scheduler(epoch):\n    ep = 10\n    if epoch < ep:\n        return LEARNING_RATE\n    else:\n        return LEARNING_RATE * math.exp(0.1 * (ep - epoch))\n\nUnet_optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\nStyle_optimizer = tf.keras.optimizers.Adam(0.00001)\n\n\ntry:\n    with open(epoch_results_path, 'w', newline='') as epoch_file, open(step_results_path, 'w', newline='') as step_file:\n        epoch_writer = csv.writer(epoch_file)\n        step_writer = csv.writer(step_file)\n        epoch_writer.writerow(['Epoch', 'Loss', 'L1_loss', 'SSIM_Score', 'PSNR', 'MSE', 'SSIM_Loss_val', 'L1_loss_val', 'SSIM_Score_val', 'PSNR_val', 'MSE_val'])\n        step_writer.writerow(['Epoch', 'Step', 'Avg_Loss', 'Avg_L1_loss', 'Avg_SSIM_Score', 'Avg_PSNR'])\n\n        for epoch in range(curr_epoch,NB_EPOCH):\n            print(f'Starting epoch {epoch + 1}')\n            ssim_scores, losses, l1_losses, pnsr_values, l2_losses = [], [], [], [], []\n            \n            new_learning_rate = scheduler(epoch)\n            Unet_optimizer.learning_rate.assign(new_learning_rate)\n            \n            for step, ([Slice1, Slice2, Slice3], FreeImage) in enumerate(train_dataset):\n                with tf.GradientTape() as tape:\n                    norm_list_output = full_style(Slice2, training=True)\n                    output1 = full_unet([Slice1, Slice2, Slice3, norm_list_output], training=True)\n                    ssim_scores.append(ssim_score(FreeImage, output1))\n                    loss_value = total_loss(FreeImage, output1)\n                    pnsr_values.append(psnr(FreeImage, output1))\n                    losses.append(loss_value)\n                    l2_losses.append(l2_loss(FreeImage, output1))\n\n                Unet_gradients = tape.gradient(loss_value, full_unet.trainable_variables)\n                Unet_optimizer.apply_gradients(zip(Unet_gradients, full_unet.trainable_variables))\n\n                with tf.GradientTape() as tape:\n                    recon_norm_list = full_style(output1)\n                    L1_loss_value = l1_loss(norm_list_output, recon_norm_list)\n                    l1_losses.append(L1_loss_value)\n\n                Style_gradients = tape.gradient(L1_loss_value, full_style.trainable_variables)\n                Style_optimizer.apply_gradients(zip(Style_gradients, full_style.trainable_variables))\n\n                if step % 100 == 0:\n                    step_writer.writerow([epoch + 1, step, \"{:.4f}\".format(sum(losses)/len(losses)), sum(l1_losses)/len(l1_losses), \"{:.4f}\".format(sum(ssim_scores)/len(ssim_scores)), \"{:.4f}\".format(sum(pnsr_values)/len(pnsr_values))])\n                    step_file.flush()  # Ensure data is written to file\n#                     plt.imshow(output1[2],cmap='gray')\n#                     plt.show()\n                    print(f'Epoch {epoch + 1}, Step {step}, Avg_Loss: {\"{:.4f}\".format(sum(losses)/len(losses))}, Avg_L1_loss: {sum(l1_losses)/len(l1_losses)}, Avg_SSIM_Score: {\"{:.4f}\".format(sum(ssim_scores)/len(ssim_scores))}, Avg_PSNR: {\"{:.4f}\".format(sum(pnsr_values)/len(pnsr_values))}, Avg_L2_Loss: {\"{:.4f}\".format(sum(l2_losses)/len(l2_losses))}')\n\n            print(f\"================================Epoch {epoch+1} Validation ===========================================\")\n            ssim_scores_val, losses_val, l1_losses_val, pnsr_values_val, l2_losses_val = [], [], [], [], []\n\n            for step, ([Slice1, Slice2, Slice3], FreeImage) in enumerate(validation_dataset):\n                norm_list_val = full_style(Slice2, training=True)\n                output1_val = full_unet([Slice1, Slice2, Slice3, norm_list_val], training=True)\n                ssim_scores_val.append(ssim_score(FreeImage, output1_val))\n                loss_value = total_loss(FreeImage, output1_val)\n                pnsr_values_val.append(psnr(FreeImage, output1_val))\n                losses_val.append(loss_value)\n                l2_losses_val.append(l2_loss(FreeImage, output1_val))\n                recon_norm_list_val = full_style(output1_val)\n                L1_loss_value = l1_loss(norm_list_val, recon_norm_list_val)\n                l1_losses_val.append(L1_loss_value)\n\n            full_unet.save(f\"/kaggle/working/full_unet_epoch_{epoch + 1}.h5\")\n            full_style.save(f\"/kaggle/working/full_style_epoch_{epoch + 1}.h5\")\n            epoch_writer.writerow([epoch + 1, \"{:.4f}\".format(sum(losses)/len(losses)), float(sum(l1_losses)/len(l1_losses)), \"{:.4f}\".format(sum(ssim_scores)/len(ssim_scores)), \"{:.4f}\".format(sum(pnsr_values)/len(pnsr_values)), \"{:.4f}\".format(sum(l2_losses)/len(l2_losses)), \"{:.4f}\".format(sum(losses_val)/len(losses_val)), float(sum(l1_losses_val)/len(l1_losses_val)), \"{:.4f}\".format(sum(ssim_scores_val)/len(ssim_scores_val)), \"{:.4f}\".format(sum(pnsr_values_val)/len(pnsr_values_val)), \"{:.4f}\".format(sum(l2_losses_val)/len(l2_losses_val))])\n            epoch_file.flush()  # Ensure data is written to file\n            \n            \nexcept Exception as e:\n    print(f\"An error occurred: {e}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-19T19:28:24.999649Z","iopub.execute_input":"2024-06-19T19:28:24.999939Z","iopub.status.idle":"2024-06-19T19:35:21.383665Z","shell.execute_reply.started":"2024-06-19T19:28:24.999914Z","shell.execute_reply":"2024-06-19T19:35:21.382259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}